{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from model.ppo_parallel import PPO\n",
    "from model.network import ActorCritic\n",
    "from model.environments import LunarLanderWithUnknownWind\n",
    "from logger import WandbSummaryWritter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     'timesteps_per_batch': 1024 ,                # Number of timesteps to run per batch\n",
    "#     'max_timesteps_per_episode': 1200,           # Max number of timesteps per episode\n",
    "#     'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "#     'lr': 2.5e-4 ,                                # Learning rate of actor optimizer\n",
    "#     'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "#     'clip': 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "# }\n",
    "hyperparameters = {'gamma': 0.999, 'lr_gamma': 0.995,\n",
    "                   'max_timesteps_per_episode': 1600,\n",
    "\t\t\t\t\t\t\t'clip_range': 0.2, 'lr': 0.005 }\n",
    "\n",
    "misc_hyperparameters = {\n",
    "    'num_workers': 2  ,\n",
    "    'seed': None \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = None\n",
    "if LOG:\n",
    "    logger = WandbSummaryWritter(project='lunar', config =hyperparameters)\n",
    "else:\n",
    "    logger=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'ppo_parallel_checkpoints/charmed-armadillo-108/ppo_policy_960.pth'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "ppo = PPO(logger, LunarLanderWithUnknownWind, **hyperparameters, **misc_hyperparameters)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    pass\n",
    "    # env = LunarContinuous().make_environment()\n",
    "    # model = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    # model.load_state_dict(torch.load(checkpoint))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model\n",
    "\n",
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Length: 93.25\n",
      "Average Episodic Return: -320.26\n",
      "Average Loss: 0.0137\n",
      "Timesteps So Far: 4849\n",
      "Iteration took: 8.01 secs\n",
      "Current learning rate: 0.004876243765609375\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Length: 92.83\n",
      "Average Episodic Return: -368.0\n",
      "Average Loss: 0.00674\n",
      "Timesteps So Far: 9676\n",
      "Iteration took: 7.92 secs\n",
      "Current learning rate: 0.004755550652328859\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Length: 103.66\n",
      "Average Episodic Return: -304.84\n",
      "Average Loss: 0.00415\n",
      "Timesteps So Far: 14548\n",
      "Iteration took: 8.07 secs\n",
      "Current learning rate: 0.004637844844091639\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Length: 103.45\n",
      "Average Episodic Return: -352.49\n",
      "Average Loss: 0.0029\n",
      "Timesteps So Far: 19410\n",
      "Iteration took: 7.79 secs\n",
      "Current learning rate: 0.004523052401373088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Length: 99.1\n",
      "Average Episodic Return: -351.72\n",
      "Average Loss: 0.0023\n",
      "Timesteps So Far: 24266\n",
      "Iteration took: 8.2 secs\n",
      "Current learning rate: 0.004411101214744006\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Length: 100.73\n",
      "Average Episodic Return: -306.93\n",
      "Average Loss: 0.00174\n",
      "Timesteps So Far: 29101\n",
      "Iteration took: 8.51 secs\n",
      "Current learning rate: 0.0043019209595734804\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Length: 99.49\n",
      "Average Episodic Return: -322.96\n",
      "Average Loss: 0.00133\n",
      "Timesteps So Far: 33976\n",
      "Iteration took: 7.83 secs\n",
      "Current learning rate: 0.004195443051852896\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Length: 134.03\n",
      "Average Episodic Return: -277.16\n",
      "Average Loss: 0.00108\n",
      "Timesteps So Far: 38801\n",
      "Iteration took: 11.35 secs\n",
      "Current learning rate: 0.004091600605113372\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Length: 101.67\n",
      "Average Episodic Return: -266.51\n",
      "Average Loss: 0.0008\n",
      "Timesteps So Far: 43681\n",
      "Iteration took: 7.78 secs\n",
      "Current learning rate: 0.003990328388409525\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Length: 105.57\n",
      "Average Episodic Return: -231.91\n",
      "Average Loss: 0.00053\n",
      "Timesteps So Far: 48537\n",
      "Iteration took: 7.89 secs\n",
      "Current learning rate: 0.0038915627853432096\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_10.pth\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Length: 99.35\n",
      "Average Episodic Return: -234.76\n",
      "Average Loss: 0.00015\n",
      "Timesteps So Far: 53405\n",
      "Iteration took: 8.2 secs\n",
      "Current learning rate: 0.003795241754101456\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Length: 146.67\n",
      "Average Episodic Return: -184.2\n",
      "Average Loss: 3e-05\n",
      "Timesteps So Far: 58245\n",
      "Iteration took: 10.23 secs\n",
      "Current learning rate: 0.0037013047884835227\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Length: 103.02\n",
      "Average Episodic Return: -174.7\n",
      "Average Loss: -0.0002\n",
      "Timesteps So Far: 63087\n",
      "Iteration took: 7.85 secs\n",
      "Current learning rate: 0.0036096928798925805\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Length: 101.15\n",
      "Average Episodic Return: -128.87\n",
      "Average Loss: -0.00014\n",
      "Timesteps So Far: 67942\n",
      "Iteration took: 7.62 secs\n",
      "Current learning rate: 0.003520348480268149\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Length: 93.85\n",
      "Average Episodic Return: -120.15\n",
      "Average Loss: -0.00023\n",
      "Timesteps So Far: 72822\n",
      "Iteration took: 7.68 secs\n",
      "Current learning rate: 0.0034332154659359997\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Length: 156.71\n",
      "Average Episodic Return: -116.02\n",
      "Average Loss: -0.00034\n",
      "Timesteps So Far: 77680\n",
      "Iteration took: 11.42 secs\n",
      "Current learning rate: 0.003348239102352821\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Length: 98.73\n",
      "Average Episodic Return: -99.95\n",
      "Average Loss: -0.00041\n",
      "Timesteps So Far: 82518\n",
      "Iteration took: 7.97 secs\n",
      "Current learning rate: 0.0032653660097234946\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Length: 103.13\n",
      "Average Episodic Return: -101.72\n",
      "Average Loss: -0.00045\n",
      "Timesteps So Far: 87365\n",
      "Iteration took: 7.51 secs\n",
      "Current learning rate: 0.0031845441294693906\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Length: 100.33\n",
      "Average Episodic Return: -93.77\n",
      "Average Loss: -0.00045\n",
      "Timesteps So Far: 92281\n",
      "Iteration took: 7.56 secs\n",
      "Current learning rate: 0.0031057226915266094\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Length: 100.65\n",
      "Average Episodic Return: -92.22\n",
      "Average Loss: -0.00028\n",
      "Timesteps So Far: 97213\n",
      "Iteration took: 8.46 secs\n",
      "Current learning rate: 0.0030288521824536397\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_20.pth\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Length: 96.3\n",
      "Average Episodic Return: -79.54\n",
      "Average Loss: -0.00013\n",
      "Timesteps So Far: 102028\n",
      "Iteration took: 7.94 secs\n",
      "Current learning rate: 0.0029538843143283823\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Length: 106.67\n",
      "Average Episodic Return: -76.43\n",
      "Average Loss: -0.00025\n",
      "Timesteps So Far: 106828\n",
      "Iteration took: 7.82 secs\n",
      "Current learning rate: 0.002880771994415019\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Length: 114.88\n",
      "Average Episodic Return: -85.6\n",
      "Average Loss: -0.00033\n",
      "Timesteps So Far: 111653\n",
      "Iteration took: 7.67 secs\n",
      "Current learning rate: 0.0028094692955816644\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Length: 167.91\n",
      "Average Episodic Return: -54.59\n",
      "Average Loss: -0.00039\n",
      "Timesteps So Far: 117026\n",
      "Iteration took: 13.1 secs\n",
      "Current learning rate: 0.0027399314274502108\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Length: 166.32\n",
      "Average Episodic Return: -40.96\n",
      "Average Loss: -0.00045\n",
      "Timesteps So Far: 123346\n",
      "Iteration took: 14.99 secs\n",
      "Current learning rate: 0.002672114708260257\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Length: 127.39\n",
      "Average Episodic Return: -37.95\n",
      "Average Loss: -0.00052\n",
      "Timesteps So Far: 128187\n",
      "Iteration took: 8.15 secs\n",
      "Current learning rate: 0.002605976537429438\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Length: 344.14\n",
      "Average Episodic Return: -19.45\n",
      "Average Loss: -0.00052\n",
      "Timesteps So Far: 133005\n",
      "Iteration took: 19.47 secs\n",
      "Current learning rate: 0.0025414753687929208\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Length: 200.46\n",
      "Average Episodic Return: -28.64\n",
      "Average Loss: -0.00053\n",
      "Timesteps So Far: 137816\n",
      "Iteration took: 15.16 secs\n",
      "Current learning rate: 0.0024785706845052535\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Length: 307.0\n",
      "Average Episodic Return: -26.55\n",
      "Average Loss: -0.00057\n",
      "Timesteps So Far: 143649\n",
      "Iteration took: 21.25 secs\n",
      "Current learning rate: 0.0024172229695881807\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Length: 447.36\n",
      "Average Episodic Return: -21.69\n",
      "Average Loss: -0.0006\n",
      "Timesteps So Far: 148570\n",
      "Iteration took: 20.2 secs\n",
      "Current learning rate: 0.002357393687108429\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_30.pth\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Length: 745.75\n",
      "Average Episodic Return: -67.54\n",
      "Average Loss: -0.00065\n",
      "Timesteps So Far: 154536\n",
      "Iteration took: 23.77 secs\n",
      "Current learning rate: 0.002299045253969875\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Length: 1032.2\n",
      "Average Episodic Return: 1.57\n",
      "Average Loss: -0.00067\n",
      "Timesteps So Far: 159697\n",
      "Iteration took: 28.28 secs\n",
      "Current learning rate: 0.002242141017304885\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Length: 1047.0\n",
      "Average Episodic Return: -39.99\n",
      "Average Loss: -0.00067\n",
      "Timesteps So Far: 164932\n",
      "Iteration took: 23.29 secs\n",
      "Current learning rate: 0.002186645231450001\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Length: 402.83\n",
      "Average Episodic Return: -1.77\n",
      "Average Loss: -0.00067\n",
      "Timesteps So Far: 169766\n",
      "Iteration took: 21.54 secs\n",
      "Current learning rate: 0.0021325230354915076\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Length: 897.0\n",
      "Average Episodic Return: -32.31\n",
      "Average Loss: -0.00069\n",
      "Timesteps So Far: 175148\n",
      "Iteration took: 23.2 secs\n",
      "Current learning rate: 0.0020797404313667688\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #36 --------------------\n",
      "Average Episodic Length: 774.71\n",
      "Average Episodic Return: -14.5\n",
      "Average Loss: -0.00071\n",
      "Timesteps So Far: 180571\n",
      "Iteration took: 26.13 secs\n",
      "Current learning rate: 0.0020282642625075913\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #37 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -28.06\n",
      "Average Loss: -0.00073\n",
      "Timesteps So Far: 185371\n",
      "Iteration took: 18.02 secs\n",
      "Current learning rate: 0.001978062193012188\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #38 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -4.63\n",
      "Average Loss: -0.00076\n",
      "Timesteps So Far: 190171\n",
      "Iteration took: 20.8 secs\n",
      "Current learning rate: 0.0019291026873326578\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #39 --------------------\n",
      "Average Episodic Length: 1151.0\n",
      "Average Episodic Return: -99.46\n",
      "Average Loss: -0.00076\n",
      "Timesteps So Far: 195926\n",
      "Iteration took: 24.79 secs\n",
      "Current learning rate: 0.0018813549904652326\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #40 --------------------\n",
      "Average Episodic Length: 752.12\n",
      "Average Episodic Return: -20.88\n",
      "Average Loss: -0.00077\n",
      "Timesteps So Far: 201943\n",
      "Iteration took: 23.87 secs\n",
      "Current learning rate: 0.001834789108630835\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_40.pth\n",
      "\n",
      "-------------------- Iteration #41 --------------------\n",
      "Average Episodic Length: 1245.75\n",
      "Average Episodic Return: -30.7\n",
      "Average Loss: -0.00076\n",
      "Timesteps So Far: 206926\n",
      "Iteration took: 24.63 secs\n",
      "Current learning rate: 0.0017893757904338186\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #42 --------------------\n",
      "Average Episodic Length: 1328.75\n",
      "Average Episodic Return: -43.94\n",
      "Average Loss: -0.00077\n",
      "Timesteps So Far: 212241\n",
      "Iteration took: 22.48 secs\n",
      "Current learning rate: 0.001745086508487051\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #43 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -66.25\n",
      "Average Loss: -0.00078\n",
      "Timesteps So Far: 217041\n",
      "Iteration took: 21.57 secs\n",
      "Current learning rate: 0.0017018934414918026\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #44 --------------------\n",
      "Average Episodic Length: 1266.25\n",
      "Average Episodic Return: -18.2\n",
      "Average Loss: -0.00081\n",
      "Timesteps So Far: 222106\n",
      "Iteration took: 22.39 secs\n",
      "Current learning rate: 0.0016597694567611774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #45 --------------------\n",
      "Average Episodic Length: 1034.6\n",
      "Average Episodic Return: 27.25\n",
      "Average Loss: -0.0008\n",
      "Timesteps So Far: 227279\n",
      "Iteration took: 24.14 secs\n",
      "Current learning rate: 0.00161868809317611\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #46 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 16.91\n",
      "Average Loss: -0.0008\n",
      "Timesteps So Far: 232079\n",
      "Iteration took: 19.53 secs\n",
      "Current learning rate: 0.0015786235445632267\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #47 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -8.35\n",
      "Average Loss: -0.00081\n",
      "Timesteps So Far: 236879\n",
      "Iteration took: 19.25 secs\n",
      "Current learning rate: 0.0015395506434841214\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #48 --------------------\n",
      "Average Episodic Length: 640.88\n",
      "Average Episodic Return: -34.52\n",
      "Average Loss: -0.00083\n",
      "Timesteps So Far: 242006\n",
      "Iteration took: 19.33 secs\n",
      "Current learning rate: 0.0015014448454258697\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #49 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 3.21\n",
      "Average Loss: -0.00083\n",
      "Timesteps So Far: 246806\n",
      "Iteration took: 19.84 secs\n",
      "Current learning rate: 0.0014642822133828456\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #50 --------------------\n",
      "Average Episodic Length: 837.86\n",
      "Average Episodic Return: -12.27\n",
      "Average Loss: -0.00083\n",
      "Timesteps So Far: 252671\n",
      "Iteration took: 23.3 secs\n",
      "Current learning rate: 0.0014280394028201597\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_50.pth\n",
      "\n",
      "-------------------- Iteration #51 --------------------\n",
      "Average Episodic Length: 1043.4\n",
      "Average Episodic Return: 32.45\n",
      "Average Loss: -0.00083\n",
      "Timesteps So Far: 257888\n",
      "Iteration took: 20.4 secs\n",
      "Current learning rate: 0.0013926936470092677\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #52 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -1.91\n",
      "Average Loss: -0.00082\n",
      "Timesteps So Far: 262688\n",
      "Iteration took: 21.28 secs\n",
      "Current learning rate: 0.001358222742726545\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #53 --------------------\n",
      "Average Episodic Length: 1022.17\n",
      "Average Episodic Return: -35.56\n",
      "Average Loss: -0.00083\n",
      "Timesteps So Far: 268821\n",
      "Iteration took: 20.47 secs\n",
      "Current learning rate: 0.0013246050363058361\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #54 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 6.25\n",
      "Average Loss: -0.00082\n",
      "Timesteps So Far: 273621\n",
      "Iteration took: 18.3 secs\n",
      "Current learning rate: 0.0012918194100362227\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #55 --------------------\n",
      "Average Episodic Length: 1230.75\n",
      "Average Episodic Return: 15.84\n",
      "Average Loss: -0.00081\n",
      "Timesteps So Far: 278544\n",
      "Iteration took: 19.85 secs\n",
      "Current learning rate: 0.0012598452688964623\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #56 --------------------\n",
      "Average Episodic Length: 985.8\n",
      "Average Episodic Return: -26.87\n",
      "Average Loss: -0.00081\n",
      "Timesteps So Far: 283473\n",
      "Iteration took: 16.69 secs\n",
      "Current learning rate: 0.001228662527617768\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #57 --------------------\n",
      "Average Episodic Length: 1038.0\n",
      "Average Episodic Return: 47.48\n",
      "Average Loss: -0.0008\n",
      "Timesteps So Far: 288663\n",
      "Iteration took: 21.33 secs\n",
      "Current learning rate: 0.0011982515980667996\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #58 --------------------\n",
      "Average Episodic Length: 1036.4\n",
      "Average Episodic Return: 25.52\n",
      "Average Loss: -0.0008\n",
      "Timesteps So Far: 293845\n",
      "Iteration took: 16.77 secs\n",
      "Current learning rate: 0.0011685933769409404\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #59 --------------------\n",
      "Average Episodic Length: 898.0\n",
      "Average Episodic Return: 51.65\n",
      "Average Loss: -0.00081\n",
      "Timesteps So Far: 299233\n",
      "Iteration took: 19.72 secs\n",
      "Current learning rate: 0.0011396692337681334\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #60 --------------------\n",
      "Average Episodic Length: 1251.75\n",
      "Average Episodic Return: 76.19\n",
      "Average Loss: -0.00081\n",
      "Timesteps So Far: 304240\n",
      "Iteration took: 18.54 secs\n",
      "Current learning rate: 0.0011114609992037348\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_60.pth\n",
      "\n",
      "-------------------- Iteration #61 --------------------\n",
      "Average Episodic Length: 668.22\n",
      "Average Episodic Return: 44.79\n",
      "Average Loss: -0.0008\n",
      "Timesteps So Far: 310254\n",
      "Iteration took: 18.67 secs\n",
      "Current learning rate: 0.0010839509536170356\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #62 --------------------\n",
      "Average Episodic Length: 1246.0\n",
      "Average Episodic Return: 76.91\n",
      "Average Loss: -0.00079\n",
      "Timesteps So Far: 315238\n",
      "Iteration took: 18.37 secs\n",
      "Current learning rate: 0.0010571218159602814\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #63 --------------------\n",
      "Average Episodic Length: 1246.75\n",
      "Average Episodic Return: 50.49\n",
      "Average Loss: -0.00079\n",
      "Timesteps So Far: 320225\n",
      "Iteration took: 21.36 secs\n",
      "Current learning rate: 0.0010309567329131965\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m total_timesteps_to_train \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps_to_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:65\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, total_timesteps)\u001b[0m\n\u001b[0;32m     62\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_so_far\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t_sim \u001b[38;5;241m<\u001b[39m total_timesteps:   \n\u001b[1;32m---> 65\u001b[0m     obs, acts, log_probs, ep_lens, advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     t_sim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(ep_lens)\n\u001b[0;32m     68\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:166\u001b[0m, in \u001b[0;36mPPO.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m batch_obs\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    164\u001b[0m ep_dones\u001b[38;5;241m.\u001b[39mappend(done)\n\u001b[1;32m--> 166\u001b[0m action, log_prob, val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m obs, rew, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    169\u001b[0m ep_rews\u001b[38;5;241m.\u001b[39mappend(rew)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:197\u001b[0m, in \u001b[0;36mPPO.get_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        Samples an action from the actor/critic network.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     mean, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     dist \u001b[38;5;241m=\u001b[39m MultivariateNormal(mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_mat)\n\u001b[0;32m    201\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\network.py:96\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m        Build a network that maps environment observation -> action probabilities + value estimate.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     critic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(obs)\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actions, critic\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\network.py:53\u001b[0m, in \u001b[0;36mFNN.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m---> 53\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     activation1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(obs))\n\u001b[0;32m     56\u001b[0m     activation2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(activation1))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_timesteps_to_train =  1_000_000\n",
    "\n",
    "ppo.train(total_timesteps_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "Run multiple episodes from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Query deterministic action from policy and run it\u001b[39;00m\n\u001b[0;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39mactor(obs)\n\u001b[1;32m---> 17\u001b[0m obs, rew, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;241m|\u001b[39m truncated\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Sum all episodic rewards as we go along\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:599\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    596\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:710\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    709\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
