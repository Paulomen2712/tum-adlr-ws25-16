{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from model.ppo_parallel import PPO\n",
    "from model.network import ActorCritic\n",
    "from model.environments import LunarContinuous\n",
    "from logger import WandbSummaryWritter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     'timesteps_per_batch': 1024 ,                # Number of timesteps to run per batch\n",
    "#     'max_timesteps_per_episode': 1200,           # Max number of timesteps per episode\n",
    "#     'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "#     'lr': 2.5e-4 ,                                # Learning rate of actor optimizer\n",
    "#     'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "#     'clip': 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "# }\n",
    "hyperparameters = {'gamma': 0.999, 'lr_gamma': 0.995,\n",
    "                   'max_timesteps_per_episode': 1600,\n",
    "\t\t\t\t\t\t\t'clip_range': 0.2, 'lr': 0.005 }\n",
    "\n",
    "misc_hyperparameters = {\n",
    "    'num_workers': 2  ,\n",
    "    'seed': None \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpmsaraiva2712\u001b[0m (\u001b[33mpmsaraiva2712-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\wandb\\run-20241205_001134-8cw2jipr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8cw2jipr' target=\"_blank\">cool-dew-296</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8cw2jipr' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8cw2jipr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = WandbSummaryWritter(project='lunar', config =hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'ppo_parallel_checkpoints/charmed-armadillo-108/ppo_policy_960.pth'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "ppo = PPO(logger, **hyperparameters, **misc_hyperparameters)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    \n",
    "    env = LunarContinuous().make_environment()\n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model\n",
    "\n",
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Length: 110.8\n",
      "Average Episodic Return: -266.33\n",
      "Average Loss: -1e-04\n",
      "Timesteps So Far: 4875\n",
      "Iteration took: 6.91 secs\n",
      "Current learning rate: 0.004876243765609375\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Length: 109.55\n",
      "Average Episodic Return: -295.6\n",
      "Average Loss: -0.00114\n",
      "Timesteps So Far: 9695\n",
      "Iteration took: 6.37 secs\n",
      "Current learning rate: 0.004755550652328859\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Length: 113.63\n",
      "Average Episodic Return: -220.16\n",
      "Average Loss: -0.0016\n",
      "Timesteps So Far: 14581\n",
      "Iteration took: 6.41 secs\n",
      "Current learning rate: 0.00463784484409164\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Length: 103.19\n",
      "Average Episodic Return: -162.9\n",
      "Average Loss: -0.00218\n",
      "Timesteps So Far: 19431\n",
      "Iteration took: 6.28 secs\n",
      "Current learning rate: 0.004523052401373088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Length: 118.95\n",
      "Average Episodic Return: -159.17\n",
      "Average Loss: -0.00254\n",
      "Timesteps So Far: 24308\n",
      "Iteration took: 6.47 secs\n",
      "Current learning rate: 0.004411101214744006\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Length: 104.74\n",
      "Average Episodic Return: -122.79\n",
      "Average Loss: -0.0026\n",
      "Timesteps So Far: 29126\n",
      "Iteration took: 6.14 secs\n",
      "Current learning rate: 0.0043019209595734804\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Length: 136.39\n",
      "Average Episodic Return: -109.38\n",
      "Average Loss: -0.00259\n",
      "Timesteps So Far: 34036\n",
      "Iteration took: 6.76 secs\n",
      "Current learning rate: 0.004195443051852896\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Length: 140.97\n",
      "Average Episodic Return: -106.81\n",
      "Average Loss: -0.00273\n",
      "Timesteps So Far: 38970\n",
      "Iteration took: 6.91 secs\n",
      "Current learning rate: 0.004091600605113371\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Length: 123.44\n",
      "Average Episodic Return: -67.56\n",
      "Average Loss: -0.00252\n",
      "Timesteps So Far: 43784\n",
      "Iteration took: 6.55 secs\n",
      "Current learning rate: 0.003990328388409524\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Length: 117.51\n",
      "Average Episodic Return: -55.16\n",
      "Average Loss: -0.00233\n",
      "Timesteps So Far: 48602\n",
      "Iteration took: 6.41 secs\n",
      "Current learning rate: 0.0038915627853432075\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Length: 144.79\n",
      "Average Episodic Return: -44.06\n",
      "Average Loss: -0.00234\n",
      "Timesteps So Far: 53525\n",
      "Iteration took: 7.41 secs\n",
      "Current learning rate: 0.0037952417541014536\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Length: 131.3\n",
      "Average Episodic Return: -60.3\n",
      "Average Loss: -0.00241\n",
      "Timesteps So Far: 58383\n",
      "Iteration took: 6.38 secs\n",
      "Current learning rate: 0.00370130478848352\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Length: 136.19\n",
      "Average Episodic Return: -34.42\n",
      "Average Loss: -0.00239\n",
      "Timesteps So Far: 63286\n",
      "Iteration took: 7.26 secs\n",
      "Current learning rate: 0.0036096928798925784\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Length: 136.47\n",
      "Average Episodic Return: -39.28\n",
      "Average Loss: -0.00226\n",
      "Timesteps So Far: 68199\n",
      "Iteration took: 7.92 secs\n",
      "Current learning rate: 0.003520348480268147\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Length: 148.0\n",
      "Average Episodic Return: -25.61\n",
      "Average Loss: -0.00229\n",
      "Timesteps So Far: 73083\n",
      "Iteration took: 7.64 secs\n",
      "Current learning rate: 0.003433215465935998\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Length: 187.31\n",
      "Average Episodic Return: -21.58\n",
      "Average Loss: -0.00236\n",
      "Timesteps So Far: 77953\n",
      "Iteration took: 10.22 secs\n",
      "Current learning rate: 0.003348239102352819\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Length: 217.78\n",
      "Average Episodic Return: 0.2\n",
      "Average Loss: -0.00237\n",
      "Timesteps So Far: 82962\n",
      "Iteration took: 10.53 secs\n",
      "Current learning rate: 0.003265366009723493\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Length: 625.5\n",
      "Average Episodic Return: -87.05\n",
      "Average Loss: -0.00238\n",
      "Timesteps So Far: 87966\n",
      "Iteration took: 14.17 secs\n",
      "Current learning rate: 0.003184544129469388\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Length: 299.5\n",
      "Average Episodic Return: -48.99\n",
      "Average Loss: -0.00236\n",
      "Timesteps So Far: 93357\n",
      "Iteration took: 11.58 secs\n",
      "Current learning rate: 0.0031057226915266077\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Length: 461.0\n",
      "Average Episodic Return: -35.84\n",
      "Average Loss: -0.00239\n",
      "Timesteps So Far: 98889\n",
      "Iteration took: 13.35 secs\n",
      "Current learning rate: 0.003028852182453637\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Length: 505.8\n",
      "Average Episodic Return: -53.91\n",
      "Average Loss: -0.00239\n",
      "Timesteps So Far: 103947\n",
      "Iteration took: 13.89 secs\n",
      "Current learning rate: 0.0029538843143283792\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Length: 881.83\n",
      "Average Episodic Return: -42.18\n",
      "Average Loss: -0.00237\n",
      "Timesteps So Far: 109238\n",
      "Iteration took: 15.72 secs\n",
      "Current learning rate: 0.0028807719944150168\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Length: 676.0\n",
      "Average Episodic Return: 23.17\n",
      "Average Loss: -0.00235\n",
      "Timesteps So Far: 114646\n",
      "Iteration took: 15.48 secs\n",
      "Current learning rate: 0.002809469295581662\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Length: 738.71\n",
      "Average Episodic Return: -17.99\n",
      "Average Loss: -0.00232\n",
      "Timesteps So Far: 119817\n",
      "Iteration took: 14.47 secs\n",
      "Current learning rate: 0.0027399314274502086\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Length: 750.14\n",
      "Average Episodic Return: 47.27\n",
      "Average Loss: -0.00229\n",
      "Timesteps So Far: 125068\n",
      "Iteration took: 14.74 secs\n",
      "Current learning rate: 0.0026721147082602553\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 56.44\n",
      "Average Loss: -0.00229\n",
      "Timesteps So Far: 130068\n",
      "Iteration took: 17.3 secs\n",
      "Current learning rate: 0.0026059765374294363\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Length: 603.22\n",
      "Average Episodic Return: -24.57\n",
      "Average Loss: -0.00226\n",
      "Timesteps So Far: 135497\n",
      "Iteration took: 14.55 secs\n",
      "Current learning rate: 0.002541475368792919\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 60.86\n",
      "Average Loss: -0.00224\n",
      "Timesteps So Far: 140497\n",
      "Iteration took: 14.75 secs\n",
      "Current learning rate: 0.0024785706845052518\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Length: 502.0\n",
      "Average Episodic Return: 17.17\n",
      "Average Loss: -0.00219\n",
      "Timesteps So Far: 145517\n",
      "Iteration took: 12.33 secs\n",
      "Current learning rate: 0.00241722296958818\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Length: 638.33\n",
      "Average Episodic Return: -5.55\n",
      "Average Loss: -0.00215\n",
      "Timesteps So Far: 151262\n",
      "Iteration took: 15.47 secs\n",
      "Current learning rate: 0.0023573936871084285\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Length: 537.56\n",
      "Average Episodic Return: 3.58\n",
      "Average Loss: -0.00214\n",
      "Timesteps So Far: 156100\n",
      "Iteration took: 12.68 secs\n",
      "Current learning rate: 0.0022990452539698746\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Length: 611.25\n",
      "Average Episodic Return: -4.14\n",
      "Average Loss: -0.0021\n",
      "Timesteps So Far: 160990\n",
      "Iteration took: 12.25 secs\n",
      "Current learning rate: 0.0022421410173048845\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Length: 881.17\n",
      "Average Episodic Return: 49.43\n",
      "Average Loss: -0.0021\n",
      "Timesteps So Far: 166277\n",
      "Iteration took: 16.94 secs\n",
      "Current learning rate: 0.0021866452314500007\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Length: 660.88\n",
      "Average Episodic Return: -19.17\n",
      "Average Loss: -0.00211\n",
      "Timesteps So Far: 171564\n",
      "Iteration took: 14.34 secs\n",
      "Current learning rate: 0.002132523035491507\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Length: 909.33\n",
      "Average Episodic Return: -56.83\n",
      "Average Loss: -0.00209\n",
      "Timesteps So Far: 177020\n",
      "Iteration took: 13.93 secs\n",
      "Current learning rate: 0.0020797404313667683\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #36 --------------------\n",
      "Average Episodic Length: 724.0\n",
      "Average Episodic Return: -49.9\n",
      "Average Loss: -0.0021\n",
      "Timesteps So Far: 182088\n",
      "Iteration took: 14.98 secs\n",
      "Current learning rate: 0.002028264262507591\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #37 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 66.49\n",
      "Average Loss: -0.00205\n",
      "Timesteps So Far: 187088\n",
      "Iteration took: 14.79 secs\n",
      "Current learning rate: 0.001978062193012187\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #38 --------------------\n",
      "Average Episodic Length: 803.67\n",
      "Average Episodic Return: 0.84\n",
      "Average Loss: -0.00204\n",
      "Timesteps So Far: 191910\n",
      "Iteration took: 12.96 secs\n",
      "Current learning rate: 0.001929102687332657\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #39 --------------------\n",
      "Average Episodic Length: 821.14\n",
      "Average Episodic Return: -43.88\n",
      "Average Loss: -0.00204\n",
      "Timesteps So Far: 197658\n",
      "Iteration took: 17.89 secs\n",
      "Current learning rate: 0.0018813549904652318\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #40 --------------------\n",
      "Average Episodic Length: 957.0\n",
      "Average Episodic Return: 11.6\n",
      "Average Loss: -0.00202\n",
      "Timesteps So Far: 203400\n",
      "Iteration took: 16.24 secs\n",
      "Current learning rate: 0.0018347891086308342\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #41 --------------------\n",
      "Average Episodic Length: 886.83\n",
      "Average Episodic Return: 30.42\n",
      "Average Loss: -0.00199\n",
      "Timesteps So Far: 208721\n",
      "Iteration took: 15.23 secs\n",
      "Current learning rate: 0.0017893757904338177\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #42 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 65.85\n",
      "Average Loss: -0.00196\n",
      "Timesteps So Far: 213721\n",
      "Iteration took: 13.77 secs\n",
      "Current learning rate: 0.00174508650848705\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #43 --------------------\n",
      "Average Episodic Length: 974.8\n",
      "Average Episodic Return: -7.48\n",
      "Average Loss: -0.00195\n",
      "Timesteps So Far: 218595\n",
      "Iteration took: 12.54 secs\n",
      "Current learning rate: 0.0017018934414918018\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #44 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 50.51\n",
      "Average Loss: -0.00194\n",
      "Timesteps So Far: 223595\n",
      "Iteration took: 14.44 secs\n",
      "Current learning rate: 0.0016597694567611761\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #45 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 41.88\n",
      "Average Loss: -0.00193\n",
      "Timesteps So Far: 228595\n",
      "Iteration took: 13.7 secs\n",
      "Current learning rate: 0.0016186880931761088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #46 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 78.59\n",
      "Average Loss: -0.00191\n",
      "Timesteps So Far: 233595\n",
      "Iteration took: 14.07 secs\n",
      "Current learning rate: 0.0015786235445632254\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #47 --------------------\n",
      "Average Episodic Length: 924.33\n",
      "Average Episodic Return: 10.48\n",
      "Average Loss: -0.0019\n",
      "Timesteps So Far: 239141\n",
      "Iteration took: 16.05 secs\n",
      "Current learning rate: 0.00153955064348412\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #48 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 65.89\n",
      "Average Loss: -0.00188\n",
      "Timesteps So Far: 244141\n",
      "Iteration took: 13.35 secs\n",
      "Current learning rate: 0.0015014448454258682\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #49 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 64.98\n",
      "Average Loss: -0.00186\n",
      "Timesteps So Far: 249141\n",
      "Iteration took: 14.58 secs\n",
      "Current learning rate: 0.0014642822133828443\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #50 --------------------\n",
      "Average Episodic Length: 913.17\n",
      "Average Episodic Return: 8.39\n",
      "Average Loss: -0.00186\n",
      "Timesteps So Far: 254620\n",
      "Iteration took: 13.76 secs\n",
      "Current learning rate: 0.001428039402820158\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #51 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 74.88\n",
      "Average Loss: -0.00184\n",
      "Timesteps So Far: 259620\n",
      "Iteration took: 12.98 secs\n",
      "Current learning rate: 0.001392693647009266\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #52 --------------------\n",
      "Average Episodic Length: 875.0\n",
      "Average Episodic Return: 62.92\n",
      "Average Loss: -0.00182\n",
      "Timesteps So Far: 264870\n",
      "Iteration took: 14.19 secs\n",
      "Current learning rate: 0.0013582227427265433\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #53 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 76.44\n",
      "Average Loss: -0.00181\n",
      "Timesteps So Far: 269870\n",
      "Iteration took: 13.53 secs\n",
      "Current learning rate: 0.0013246050363058344\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #54 --------------------\n",
      "Average Episodic Length: 924.5\n",
      "Average Episodic Return: 4.52\n",
      "Average Loss: -0.00179\n",
      "Timesteps So Far: 275417\n",
      "Iteration took: 15.24 secs\n",
      "Current learning rate: 0.001291819410036221\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #55 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 11.71\n",
      "Average Loss: -0.00178\n",
      "Timesteps So Far: 280417\n",
      "Iteration took: 12.9 secs\n",
      "Current learning rate: 0.0012598452688964605\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #56 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 87.91\n",
      "Average Loss: -0.00177\n",
      "Timesteps So Far: 285417\n",
      "Iteration took: 13.66 secs\n",
      "Current learning rate: 0.0012286625276177663\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #57 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 107.54\n",
      "Average Loss: -0.00175\n",
      "Timesteps So Far: 290417\n",
      "Iteration took: 12.15 secs\n",
      "Current learning rate: 0.0011982515980667977\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #58 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 91.14\n",
      "Average Loss: -0.00174\n",
      "Timesteps So Far: 295417\n",
      "Iteration took: 13.9 secs\n",
      "Current learning rate: 0.0011685933769409384\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #59 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 116.03\n",
      "Average Loss: -0.00173\n",
      "Timesteps So Far: 300417\n",
      "Iteration took: 13.25 secs\n",
      "Current learning rate: 0.0011396692337681314\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #60 --------------------\n",
      "Average Episodic Length: 875.83\n",
      "Average Episodic Return: 96.01\n",
      "Average Loss: -0.00171\n",
      "Timesteps So Far: 305672\n",
      "Iteration took: 15.77 secs\n",
      "Current learning rate: 0.0011114609992037327\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #61 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 87.79\n",
      "Average Loss: -0.00169\n",
      "Timesteps So Far: 310672\n",
      "Iteration took: 17.74 secs\n",
      "Current learning rate: 0.0010839509536170334\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #62 --------------------\n",
      "Average Episodic Length: 902.17\n",
      "Average Episodic Return: 81.81\n",
      "Average Loss: -0.00167\n",
      "Timesteps So Far: 316085\n",
      "Iteration took: 14.71 secs\n",
      "Current learning rate: 0.001057121815960279\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #63 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 122.46\n",
      "Average Loss: -0.00165\n",
      "Timesteps So Far: 321085\n",
      "Iteration took: 12.21 secs\n",
      "Current learning rate: 0.0010309567329131943\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #64 --------------------\n",
      "Average Episodic Length: 998.8\n",
      "Average Episodic Return: 116.56\n",
      "Average Loss: -0.00163\n",
      "Timesteps So Far: 326079\n",
      "Iteration took: 14.34 secs\n",
      "Current learning rate: 0.0010054392682961946\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #65 --------------------\n",
      "Average Episodic Length: 1000.0\n",
      "Average Episodic Return: 96.82\n",
      "Average Loss: -0.00163\n",
      "Timesteps So Far: 331079\n",
      "Iteration took: 15.25 secs\n",
      "Current learning rate: 0.000980553392745634\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m total_timesteps_to_train \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps_to_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:67\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, total_timesteps)\u001b[0m\n\u001b[0;32m     64\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_so_far\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t_sim \u001b[38;5;241m<\u001b[39m total_timesteps:   \n\u001b[1;32m---> 67\u001b[0m     obs, acts, log_probs, ep_lens, advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     t_sim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(ep_lens)\n\u001b[0;32m     70\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:169\u001b[0m, in \u001b[0;36mPPO.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m batch_obs\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    167\u001b[0m ep_dones\u001b[38;5;241m.\u001b[39mappend(done)\n\u001b[1;32m--> 169\u001b[0m action, log_prob, val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m obs, rew, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    171\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:203\u001b[0m, in \u001b[0;36mPPO.get_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    Samples an action from the actor/critic network.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m mean, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(obs)\n\u001b[1;32m--> 203\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    206\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m    176\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\distributions\\distribution.py:66\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     65\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 66\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\distributions\\constraints.py:554\u001b[0m, in \u001b[0;36m_PositiveDefinite.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 554\u001b[0m     sym_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sym_check\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sym_check\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\distributions\\constraints.py:533\u001b[0m, in \u001b[0;36m_Symmetric.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m square_check\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m square_check\n\u001b[1;32m--> 533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mall(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_timesteps_to_train =  1_000_000\n",
    "\n",
    "ppo.train(total_timesteps_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "Run multiple episodes from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Query deterministic action from policy and run it\u001b[39;00m\n\u001b[0;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39mactor(obs)\n\u001b[1;32m---> 17\u001b[0m obs, rew, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;241m|\u001b[39m truncated\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Sum all episodic rewards as we go along\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:599\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    596\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:710\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    709\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
