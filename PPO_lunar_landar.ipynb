{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from model.ppo_parallel import PPO\n",
    "from model.network import ActorCritic\n",
    "from env.wrappers import LunarContinuous,LunarLanderWithKnownWind, LunarLanderWithUnknownWind\n",
    "from logger import WandbSummaryWritter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     'timesteps_per_batch': 1024 ,                # Number of timesteps to run per batch\n",
    "#     'max_timesteps_per_episode': 1200,           # Max number of timesteps per episode\n",
    "#     'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "#     'lr': 2.5e-4 ,                                # Learning rate of actor optimizer\n",
    "#     'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "#     'clip': 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "# }\n",
    "# hyperparameters = {'gamma': 0.999, 'lr_gamma': 0.995,\n",
    "#                    'max_timesteps_per_episode': 1600,\n",
    "# \t\t\t\t\t\t\t'clip_range': 0.2, 'lr': 0.005 }\n",
    "hyperparameters={}\n",
    "misc_hyperparameters = {\n",
    "    'env': LunarContinuous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = None\n",
    "if LOG:\n",
    "    logger = WandbSummaryWritter(project='lunar', config =hyperparameters)\n",
    "else:\n",
    "    logger=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:216: UserWarning: \u001b[33mWARN: wind_power value is recommended to be between 0.0 and 20.0, (current value: -19.8482351412719)\u001b[0m\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'ppo_parallel_checkpoints/charmed-armadillo-108/ppo_policy_960.pth'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "ppo = PPO(logger, LunarLanderWithUnknownWind, **hyperparameters, **misc_hyperparameters)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    pass\n",
    "    # env = LunarContinuous().make_environment()\n",
    "    # model = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    # model.load_state_dict(torch.load(checkpoint))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model\n",
    "\n",
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Length: 91.09\n",
      "Average Episodic Return: -245.37\n",
      "Average Loss: 0.00639\n",
      "Timesteps So Far: 4828\n",
      "Iteration took: 7.78 secs\n",
      "Current learning rate: 0.004876243765609375\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Length: 96.08\n",
      "Average Episodic Return: -221.84\n",
      "Average Loss: 0.00189\n",
      "Timesteps So Far: 9728\n",
      "Iteration took: 8.65 secs\n",
      "Current learning rate: 0.004755550652328859\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Length: 95.23\n",
      "Average Episodic Return: -240.42\n",
      "Average Loss: 0.00083\n",
      "Timesteps So Far: 14680\n",
      "Iteration took: 8.66 secs\n",
      "Current learning rate: 0.004637844844091639\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Length: 96.26\n",
      "Average Episodic Return: -243.33\n",
      "Average Loss: 2e-05\n",
      "Timesteps So Far: 19493\n",
      "Iteration took: 7.36 secs\n",
      "Current learning rate: 0.004523052401373088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Length: 98.27\n",
      "Average Episodic Return: -230.03\n",
      "Average Loss: -0.00055\n",
      "Timesteps So Far: 24308\n",
      "Iteration took: 7.06 secs\n",
      "Current learning rate: 0.004411101214744006\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Length: 98.27\n",
      "Average Episodic Return: -232.67\n",
      "Average Loss: -0.00076\n",
      "Timesteps So Far: 29123\n",
      "Iteration took: 7.4 secs\n",
      "Current learning rate: 0.0043019209595734804\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Length: 98.0\n",
      "Average Episodic Return: -198.25\n",
      "Average Loss: -0.00095\n",
      "Timesteps So Far: 33925\n",
      "Iteration took: 7.88 secs\n",
      "Current learning rate: 0.004195443051852896\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Length: 108.33\n",
      "Average Episodic Return: -235.34\n",
      "Average Loss: -0.00105\n",
      "Timesteps So Far: 38800\n",
      "Iteration took: 9.11 secs\n",
      "Current learning rate: 0.004091600605113372\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Length: 97.88\n",
      "Average Episodic Return: -217.1\n",
      "Average Loss: -0.00124\n",
      "Timesteps So Far: 43694\n",
      "Iteration took: 8.04 secs\n",
      "Current learning rate: 0.003990328388409525\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Length: 99.41\n",
      "Average Episodic Return: -221.44\n",
      "Average Loss: -0.0013\n",
      "Timesteps So Far: 48565\n",
      "Iteration took: 7.99 secs\n",
      "Current learning rate: 0.0038915627853432096\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_10.pth\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Length: 101.12\n",
      "Average Episodic Return: -191.49\n",
      "Average Loss: -0.00135\n",
      "Timesteps So Far: 53419\n",
      "Iteration took: 7.52 secs\n",
      "Current learning rate: 0.003795241754101456\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Length: 98.04\n",
      "Average Episodic Return: -174.7\n",
      "Average Loss: -0.0014\n",
      "Timesteps So Far: 58223\n",
      "Iteration took: 7.74 secs\n",
      "Current learning rate: 0.0037013047884835227\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Length: 96.98\n",
      "Average Episodic Return: -165.95\n",
      "Average Loss: -0.00135\n",
      "Timesteps So Far: 63072\n",
      "Iteration took: 7.96 secs\n",
      "Current learning rate: 0.0036096928798925805\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Length: 105.8\n",
      "Average Episodic Return: -194.23\n",
      "Average Loss: -0.00145\n",
      "Timesteps So Far: 67939\n",
      "Iteration took: 8.24 secs\n",
      "Current learning rate: 0.003520348480268149\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Length: 106.0\n",
      "Average Episodic Return: -180.09\n",
      "Average Loss: -0.00151\n",
      "Timesteps So Far: 72815\n",
      "Iteration took: 6.69 secs\n",
      "Current learning rate: 0.0034332154659359997\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Length: 96.1\n",
      "Average Episodic Return: -164.82\n",
      "Average Loss: -0.00147\n",
      "Timesteps So Far: 77716\n",
      "Iteration took: 6.56 secs\n",
      "Current learning rate: 0.003348239102352821\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Length: 104.96\n",
      "Average Episodic Return: -156.55\n",
      "Average Loss: -0.0016\n",
      "Timesteps So Far: 82544\n",
      "Iteration took: 7.39 secs\n",
      "Current learning rate: 0.0032653660097234946\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Length: 102.21\n",
      "Average Episodic Return: -163.25\n",
      "Average Loss: -0.00163\n",
      "Timesteps So Far: 87348\n",
      "Iteration took: 7.9 secs\n",
      "Current learning rate: 0.0031845441294693906\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Length: 142.21\n",
      "Average Episodic Return: -177.77\n",
      "Average Loss: -0.00169\n",
      "Timesteps So Far: 92183\n",
      "Iteration took: 10.41 secs\n",
      "Current learning rate: 0.0031057226915266094\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Length: 106.39\n",
      "Average Episodic Return: -168.39\n",
      "Average Loss: -0.00172\n",
      "Timesteps So Far: 97077\n",
      "Iteration took: 7.37 secs\n",
      "Current learning rate: 0.0030288521824536397\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_20.pth\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Length: 100.77\n",
      "Average Episodic Return: -128.69\n",
      "Average Loss: -0.00172\n",
      "Timesteps So Far: 101914\n",
      "Iteration took: 8.27 secs\n",
      "Current learning rate: 0.0029538843143283823\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Length: 103.21\n",
      "Average Episodic Return: -148.91\n",
      "Average Loss: -0.00171\n",
      "Timesteps So Far: 106765\n",
      "Iteration took: 8.16 secs\n",
      "Current learning rate: 0.002880771994415019\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Length: 151.03\n",
      "Average Episodic Return: -119.7\n",
      "Average Loss: -0.00174\n",
      "Timesteps So Far: 111598\n",
      "Iteration took: 13.84 secs\n",
      "Current learning rate: 0.0028094692955816644\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Length: 110.64\n",
      "Average Episodic Return: -100.55\n",
      "Average Loss: -0.00173\n",
      "Timesteps So Far: 116466\n",
      "Iteration took: 8.15 secs\n",
      "Current learning rate: 0.0027399314274502108\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Length: 102.23\n",
      "Average Episodic Return: -117.58\n",
      "Average Loss: -0.00174\n",
      "Timesteps So Far: 121271\n",
      "Iteration took: 7.81 secs\n",
      "Current learning rate: 0.002672114708260257\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Length: 105.46\n",
      "Average Episodic Return: -84.8\n",
      "Average Loss: -0.00166\n",
      "Timesteps So Far: 126122\n",
      "Iteration took: 7.82 secs\n",
      "Current learning rate: 0.002605976537429438\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Length: 112.67\n",
      "Average Episodic Return: -97.68\n",
      "Average Loss: -0.00166\n",
      "Timesteps So Far: 130967\n",
      "Iteration took: 8.12 secs\n",
      "Current learning rate: 0.0025414753687929208\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Length: 223.35\n",
      "Average Episodic Return: -119.66\n",
      "Average Loss: -0.00178\n",
      "Timesteps So Far: 136104\n",
      "Iteration took: 14.35 secs\n",
      "Current learning rate: 0.0024785706845052535\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Length: 173.81\n",
      "Average Episodic Return: -98.11\n",
      "Average Loss: -0.00175\n",
      "Timesteps So Far: 142361\n",
      "Iteration took: 17.87 secs\n",
      "Current learning rate: 0.0024172229695881807\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Length: 178.3\n",
      "Average Episodic Return: -102.69\n",
      "Average Loss: -0.0018\n",
      "Timesteps So Far: 147175\n",
      "Iteration took: 15.72 secs\n",
      "Current learning rate: 0.002357393687108429\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_30.pth\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Length: 192.23\n",
      "Average Episodic Return: -85.76\n",
      "Average Loss: -0.0018\n",
      "Timesteps So Far: 152942\n",
      "Iteration took: 13.9 secs\n",
      "Current learning rate: 0.002299045253969875\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Length: 155.39\n",
      "Average Episodic Return: -102.82\n",
      "Average Loss: -0.0018\n",
      "Timesteps So Far: 157759\n",
      "Iteration took: 7.62 secs\n",
      "Current learning rate: 0.002242141017304885\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Length: 235.52\n",
      "Average Episodic Return: -89.08\n",
      "Average Loss: -0.00179\n",
      "Timesteps So Far: 162705\n",
      "Iteration took: 10.31 secs\n",
      "Current learning rate: 0.002186645231450001\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Length: 705.0\n",
      "Average Episodic Return: -107.7\n",
      "Average Loss: -0.0018\n",
      "Timesteps So Far: 168345\n",
      "Iteration took: 25.85 secs\n",
      "Current learning rate: 0.0021325230354915076\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Length: 523.7\n",
      "Average Episodic Return: -140.69\n",
      "Average Loss: -0.00181\n",
      "Timesteps So Far: 173582\n",
      "Iteration took: 16.28 secs\n",
      "Current learning rate: 0.0020797404313667688\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #36 --------------------\n",
      "Average Episodic Length: 343.64\n",
      "Average Episodic Return: -103.72\n",
      "Average Loss: -0.00183\n",
      "Timesteps So Far: 178393\n",
      "Iteration took: 16.65 secs\n",
      "Current learning rate: 0.0020282642625075913\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #37 --------------------\n",
      "Average Episodic Length: 609.22\n",
      "Average Episodic Return: -90.69\n",
      "Average Loss: -0.00184\n",
      "Timesteps So Far: 183876\n",
      "Iteration took: 27.76 secs\n",
      "Current learning rate: 0.001978062193012188\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #38 --------------------\n",
      "Average Episodic Length: 344.29\n",
      "Average Episodic Return: -79.3\n",
      "Average Loss: -0.00185\n",
      "Timesteps So Far: 188696\n",
      "Iteration took: 17.14 secs\n",
      "Current learning rate: 0.0019291026873326578\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #39 --------------------\n",
      "Average Episodic Length: 511.17\n",
      "Average Episodic Return: -47.54\n",
      "Average Loss: -0.00188\n",
      "Timesteps So Far: 194830\n",
      "Iteration took: 21.13 secs\n",
      "Current learning rate: 0.0018813549904652326\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #40 --------------------\n",
      "Average Episodic Length: 370.69\n",
      "Average Episodic Return: -105.93\n",
      "Average Loss: -0.00187\n",
      "Timesteps So Far: 199649\n",
      "Iteration took: 15.22 secs\n",
      "Current learning rate: 0.001834789108630835\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_40.pth\n",
      "\n",
      "-------------------- Iteration #41 --------------------\n",
      "Average Episodic Length: 493.0\n",
      "Average Episodic Return: -50.44\n",
      "Average Loss: -0.00185\n",
      "Timesteps So Far: 204579\n",
      "Iteration took: 15.22 secs\n",
      "Current learning rate: 0.0017893757904338186\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #42 --------------------\n",
      "Average Episodic Length: 869.83\n",
      "Average Episodic Return: -81.54\n",
      "Average Loss: -0.00184\n",
      "Timesteps So Far: 209798\n",
      "Iteration took: 23.8 secs\n",
      "Current learning rate: 0.001745086508487051\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #43 --------------------\n",
      "Average Episodic Length: 192.92\n",
      "Average Episodic Return: -39.52\n",
      "Average Loss: -0.00184\n",
      "Timesteps So Far: 214621\n",
      "Iteration took: 11.89 secs\n",
      "Current learning rate: 0.0017018934414918026\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #44 --------------------\n",
      "Average Episodic Length: 770.43\n",
      "Average Episodic Return: -82.12\n",
      "Average Loss: -0.00185\n",
      "Timesteps So Far: 220014\n",
      "Iteration took: 18.66 secs\n",
      "Current learning rate: 0.0016597694567611774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #45 --------------------\n",
      "Average Episodic Length: 520.0\n",
      "Average Episodic Return: -41.79\n",
      "Average Loss: -0.00184\n",
      "Timesteps So Far: 225734\n",
      "Iteration took: 19.1 secs\n",
      "Current learning rate: 0.00161868809317611\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m total_timesteps_to_train \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps_to_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:65\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, total_timesteps)\u001b[0m\n\u001b[0;32m     62\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_so_far\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t_sim \u001b[38;5;241m<\u001b[39m total_timesteps:   \n\u001b[1;32m---> 65\u001b[0m     obs, acts, log_probs, ep_lens, advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     t_sim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(ep_lens)\n\u001b[0;32m     68\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:167\u001b[0m, in \u001b[0;36mPPO.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m batch_obs\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    165\u001b[0m ep_dones\u001b[38;5;241m.\u001b[39mappend(done)\n\u001b[1;32m--> 167\u001b[0m action, log_prob, val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m obs, rew, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    170\u001b[0m ep_rews\u001b[38;5;241m.\u001b[39mappend(rew)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo_parallel.py:198\u001b[0m, in \u001b[0;36mPPO.get_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        Samples an action from the actor/critic network.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     mean, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     dist \u001b[38;5;241m=\u001b[39m MultivariateNormal(mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_mat)\n\u001b[0;32m    202\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\network.py:82\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m        Build a network that maps environment observation -> action probabilities + value estimate.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     critic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(obs)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actions, critic\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\network.py:44\u001b[0m, in \u001b[0;36mFNN.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m---> 44\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     activation1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(obs))\n\u001b[0;32m     47\u001b[0m     activation2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(activation1))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_timesteps_to_train =  1_000_000\n",
    "\n",
    "ppo.train(total_timesteps_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "Run multiple episodes from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Query deterministic action from policy and run it\u001b[39;00m\n\u001b[0;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39mactor(obs)\n\u001b[1;32m---> 17\u001b[0m obs, rew, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;241m|\u001b[39m truncated\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Sum all episodic rewards as we go along\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:599\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    596\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:710\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    709\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
