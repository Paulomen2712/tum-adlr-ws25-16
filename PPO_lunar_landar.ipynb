{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ppo import PPO\n",
    "from env.wrappers import LunarContinuous, LunarLanderWithUnknownWind,LunarLanderWithKnownWind\n",
    "from utils.logger import WandbSummaryWritter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     'timesteps_per_batch': 1024 ,                # Number of timesteps to run per batch\n",
    "#     'max_timesteps_per_episode': 1200,           # Max number of timesteps per episode\n",
    "#     'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "#     'lr': 2.5e-4 ,                                # Learning rate of actor optimizer\n",
    "#     'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "# }\n",
    "# hyperparameters = {'gamma': 0.999, 'lr_gamma': 0.995,\n",
    "#                    'max_timesteps_per_episode': 1200,'lr': 0.005 }\n",
    "\n",
    "hyperparameters = {}\n",
    "\n",
    "misc_hyperparameters = {\n",
    "    'env': LunarContinuous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = False\n",
    "if LOG:\n",
    "    logger = WandbSummaryWritter(project='lunar', config =misc_hyperparameters['env']().load_hyperparameters())\n",
    "else:\n",
    "    logger=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'ppo_checkpoints/non_wandb/ppo_policy_50.pth'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "ppo = PPO(logger, **hyperparameters, **misc_hyperparameters)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ppo.restore_savestate(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model\n",
    "\n",
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -141.93\n",
      "Average Loss: 0.00406\n",
      "Average KL Divergence: 0.008285522199003026\n",
      "Iteration took: 9.72 secs, of which rollout took 8.05 secs and gradient updates took 1.67 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -121.53\n",
      "Average Loss: -8e-05\n",
      "Average KL Divergence: 0.0079110931514466\n",
      "Iteration took: 9.42 secs, of which rollout took 7.91 secs and gradient updates took 1.51 secs\n",
      "Current learning rate: 0.00495\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -134.38\n",
      "Average Loss: -0.00257\n",
      "Average KL Divergence: 0.008167712063086351\n",
      "Iteration took: 10.25 secs, of which rollout took 8.5 secs and gradient updates took 1.75 secs\n",
      "Current learning rate: 0.004851\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -95.43\n",
      "Average Loss: -0.0038\n",
      "Average KL Divergence: 0.008591951665254362\n",
      "Iteration took: 11.56 secs, of which rollout took 9.8 secs and gradient updates took 1.75 secs\n",
      "Current learning rate: 0.00470547\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -56.86\n",
      "Average Loss: -0.0048\n",
      "Average KL Divergence: 0.009499962503043486\n",
      "Iteration took: 11.65 secs, of which rollout took 9.86 secs and gradient updates took 1.78 secs\n",
      "Current learning rate: 0.0045172512\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -51.91\n",
      "Average Loss: -0.00499\n",
      "Average KL Divergence: 0.00935714830618683\n",
      "Iteration took: 12.99 secs, of which rollout took 11.03 secs and gradient updates took 1.96 secs\n",
      "Current learning rate: 0.00429138864\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -71.72\n",
      "Average Loss: -0.00563\n",
      "Average KL Divergence: 0.009615629560642585\n",
      "Iteration took: 16.47 secs, of which rollout took 14.44 secs and gradient updates took 2.02 secs\n",
      "Current learning rate: 0.0040339053216\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -18.46\n",
      "Average Loss: -0.00541\n",
      "Average KL Divergence: 0.009706980486282619\n",
      "Iteration took: 15.08 secs, of which rollout took 13.28 secs and gradient updates took 1.79 secs\n",
      "Current learning rate: 0.003751531949088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -31.39\n",
      "Average Loss: -0.00494\n",
      "Average KL Divergence: 0.009673742809046103\n",
      "Iteration took: 20.17 secs, of which rollout took 18.19 secs and gradient updates took 1.98 secs\n",
      "Current learning rate: 0.00345140939316096\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo.py:126\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (it\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 126\u001b[0m         val_rew, val_dur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_rew\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(val_rew)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_dur\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(val_dur)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo.py:275\u001b[0m, in \u001b[0;36mPPO.validate\u001b[1;34m(self, val_iter, should_record)\u001b[0m\n\u001b[0;32m    273\u001b[0m t_sim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    274\u001b[0m not_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mval_iter) \u001b[38;5;241m-\u001b[39m done\n\u001b[1;32m--> 275\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m not_done\n\u001b[0;32m    276\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39msample_action(torch\u001b[38;5;241m.\u001b[39mTensor(obs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m    277\u001b[0m obs, rew, next_done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "ppo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "Run multiple episodes from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 55.08478653,  76.26484522, 100.0668116 ,  84.75946008,\n",
       "         119.57279413, 122.86338491,  99.00859018,  82.65465626,\n",
       "          62.38822349,  47.74654677])],\n",
       " [array([1201, 1201, 1201, 1201, 1201, 1201, 1201, 1201, 1201, 1201])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ppo.validate(val_iter=30)\n",
    "ppo.device = 'cuda'\n",
    "ppo.validate(10)\n",
    "# ppo.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
