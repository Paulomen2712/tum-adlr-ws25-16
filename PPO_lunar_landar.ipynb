{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import wandb\n",
    "\n",
    "from model.ppo_parallel import PPO\n",
    "from model.network import ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  (2,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2',render_mode='human')\n",
    "def make_env():\n",
    "    return gym.make('LunarLanderContinuous-v2',render_mode='human')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'timesteps_per_batch': 4800 ,                # Number of timesteps to run per batch\n",
    "    'max_timesteps_per_episode': 1600,           # Max number of timesteps per episode\n",
    "    'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "    'lr': 0.005 ,                                # Learning rate of actor optimizer\n",
    "    'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "    'clip': 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "}\n",
    "\n",
    "misc_hyperparameters = {\n",
    "    'render': True,                              # If we should render during rollout\n",
    "    'render_every_i': 10 ,                       # Only render every n iterations\n",
    "    'save_freq': 10  ,                           # How often we save in number of iterations\n",
    "    'num_workers': 2  ,\n",
    "    'seed': None \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpmsaraiva2712\u001b[0m (\u001b[33mpmsaraiva2712-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\wandb\\run-20241115_215324-3lhpd52u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/3lhpd52u' target=\"_blank\">gentle-voice-29</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/3lhpd52u' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/3lhpd52u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"lunar\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config= hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Length: 110.34\n",
      "Average Episodic Return: -280.51\n",
      "Average Loss: -0.00156\n",
      "Timesteps So Far: 4855\n",
      "Iteration took: 57.61 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Length: 110.67\n",
      "Average Episodic Return: -243.2\n",
      "Average Loss: -0.00279\n",
      "Timesteps So Far: 9835\n",
      "Iteration took: 62.7 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Length: 111.2\n",
      "Average Episodic Return: -182.87\n",
      "Average Loss: -0.00226\n",
      "Timesteps So Far: 14839\n",
      "Iteration took: 63.2 secs\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LOAD_MODEL = False\n",
    "model = None\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    checkpoint = 'ppo_parallel_checkpoints/ppo_policy_5.pth'\n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "ppo = PPO(model=model, **hyperparameters, **misc_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps_to_train = 20000\n",
    "ppo.learn(total_timesteps_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #78 --------------------\n",
      "Average Episodic Length: 1251.8\n",
      "Average Episodic Return: -87.72\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 409072\n",
      "Iteration took: 613.35 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #79 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 4.47\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 413872\n",
      "Iteration took: 99.37 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #80 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 43.11\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 418672\n",
      "Iteration took: 98.96 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #81 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 43.65\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 423472\n",
      "Iteration took: 98.72 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #82 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 48.49\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 428272\n",
      "Iteration took: 98.93 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #83 --------------------\n",
      "Average Episodic Length: 1049.0\n",
      "Average Episodic Return: -93.09\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 433517\n",
      "Iteration took: 108.36 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #84 --------------------\n",
      "Average Episodic Length: 1191.2\n",
      "Average Episodic Return: -119.59\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 439473\n",
      "Iteration took: 122.62 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #85 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 39.79\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 444273\n",
      "Iteration took: 98.89 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #86 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 50.88\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 449073\n",
      "Iteration took: 99.05 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #87 --------------------\n",
      "Average Episodic Length: 1527.75\n",
      "Average Episodic Return: 57.7\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 455184\n",
      "Iteration took: 125.99 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #88 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -14.04\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 459984\n",
      "Iteration took: 98.69 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #89 --------------------\n",
      "Average Episodic Length: 1138.6\n",
      "Average Episodic Return: -14.25\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 465677\n",
      "Iteration took: 117.21 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #90 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -18.93\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 470477\n",
      "Iteration took: 98.86 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #91 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 0.72\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 475277\n",
      "Iteration took: 98.6 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #92 --------------------\n",
      "Average Episodic Length: 1335.75\n",
      "Average Episodic Return: -54.06\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 480620\n",
      "Iteration took: 109.96 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #93 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 10.94\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 485420\n",
      "Iteration took: 99.26 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #94 --------------------\n",
      "Average Episodic Length: 1431.75\n",
      "Average Episodic Return: 27.37\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 491147\n",
      "Iteration took: 118.41 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #95 --------------------\n",
      "Average Episodic Length: 1455.5\n",
      "Average Episodic Return: 15.62\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 496969\n",
      "Iteration took: 119.48 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #96 --------------------\n",
      "Average Episodic Length: 1167.0\n",
      "Average Episodic Return: 24.4\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 502804\n",
      "Iteration took: 120.02 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #97 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 53.22\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 507604\n",
      "Iteration took: 98.89 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #98 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -14.67\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 512404\n",
      "Iteration took: 98.67 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #99 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -65.25\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 517204\n",
      "Iteration took: 98.59 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #100 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 30.68\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 522004\n",
      "Iteration took: 98.68 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #101 --------------------\n",
      "Average Episodic Length: 1569.5\n",
      "Average Episodic Return: 42.34\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 528282\n",
      "Iteration took: 129.19 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #102 --------------------\n",
      "Average Episodic Length: 1459.0\n",
      "Average Episodic Return: 11.14\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 534118\n",
      "Iteration took: 119.94 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #103 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 26.88\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 538918\n",
      "Iteration took: 98.67 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #104 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 45.92\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 543718\n",
      "Iteration took: 98.85 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #105 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -55.41\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 548518\n",
      "Iteration took: 99.05 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #106 --------------------\n",
      "Average Episodic Length: 1263.5\n",
      "Average Episodic Return: 12.27\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 553572\n",
      "Iteration took: 103.73 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #107 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -6.6\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 558372\n",
      "Iteration took: 98.75 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #108 --------------------\n",
      "Average Episodic Length: 1503.25\n",
      "Average Episodic Return: 65.03\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 564385\n",
      "Iteration took: 123.78 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #109 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -26.39\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 569185\n",
      "Iteration took: 98.81 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #110 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 32.14\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 573985\n",
      "Iteration took: 98.62 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #111 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: -0.15\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 578785\n",
      "Iteration took: 98.77 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #112 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 28.96\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 583585\n",
      "Iteration took: 98.81 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #113 --------------------\n",
      "Average Episodic Length: 1324.25\n",
      "Average Episodic Return: -59.07\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 588882\n",
      "Iteration took: 109.13 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #114 --------------------\n",
      "Average Episodic Length: 1420.0\n",
      "Average Episodic Return: 67.61\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 594562\n",
      "Iteration took: 109.6 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #115 --------------------\n",
      "Average Episodic Length: 1301.0\n",
      "Average Episodic Return: -16.02\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 599766\n",
      "Iteration took: 107.06 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #116 --------------------\n",
      "Average Episodic Length: 1600.0\n",
      "Average Episodic Return: 8.47\n",
      "Average Loss: -2e-05\n",
      "Timesteps So Far: 604566\n",
      "Iteration took: 99.0 secs\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.learn(600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Query deterministic action from policy and run it\u001b[39;00m\n\u001b[0;32m     16\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpolicy(obs)\n\u001b[1;32m---> 17\u001b[0m obs, rew, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;241m|\u001b[39m truncated\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Sum all episodic rewards as we go along\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:599\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    596\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:710\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    709\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\t\tobs, _ = env.reset()\n",
    "\t\tdone = False\n",
    "\n",
    "\t\t# number of timesteps so far\n",
    "\t\tt = 0\n",
    "\n",
    "\t\t# Logging data\n",
    "\t\tep_len = 0            # episodic length\n",
    "\t\tep_ret = 0            # episodic return\n",
    "\n",
    "\t\twhile not done:\n",
    "\t\t\tt += 1\n",
    "\n",
    "\t\t\t# Query deterministic action from policy and run it\n",
    "\t\t\taction, _ = model.policy(obs)\n",
    "\t\t\tobs, rew, terminated, truncated, _ = env.step(action.detach().numpy())\n",
    "\t\t\tdone = terminated | truncated\n",
    "\n",
    "\t\t\t# Sum all episodic rewards as we go along\n",
    "\t\t\tep_ret += rew\n",
    "\t\t\t\n",
    "\t\t# Track episodic length\n",
    "\t\tep_len = t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
