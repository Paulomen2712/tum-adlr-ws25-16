{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ppo_parallel import PPO\n",
    "from env.wrappers import LunarContinuous, LunarLanderWithUnknownWind,LunarLanderWithKnownWind\n",
    "from logger import WandbSummaryWritter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     'timesteps_per_batch': 1024 ,                # Number of timesteps to run per batch\n",
    "#     'max_timesteps_per_episode': 1200,           # Max number of timesteps per episode\n",
    "#     'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "#     'lr': 2.5e-4 ,                                # Learning rate of actor optimizer\n",
    "#     'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "# }\n",
    "# hyperparameters = {'gamma': 0.999, 'lr_gamma': 0.995,\n",
    "#                    'max_timesteps_per_episode': 1200,'lr': 0.005 }\n",
    "\n",
    "hyperparameters = {}\n",
    "\n",
    "misc_hyperparameters = {\n",
    "    'env': LunarContinuous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = False\n",
    "if LOG:\n",
    "    logger = WandbSummaryWritter(project='lunar', config =hyperparameters)\n",
    "else:\n",
    "    logger=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'ppo_parallel_checkpoints/charmed-armadillo-108/ppo_policy_960.pth'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "ppo = PPO(logger, **hyperparameters, **misc_hyperparameters)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ppo.restore_savestate(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model\n",
    "\n",
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Length: 112.59\n",
      "Average Episodic Return: -262.38\n",
      "Average Loss: -0.00673\n",
      "Average KL: 0.006456820759922266\n",
      "Timesteps So Far: 20041\n",
      "Iteration took: 32.48 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Length: 116.93\n",
      "Average Episodic Return: -198.71\n",
      "Average Loss: -0.0065\n",
      "Average KL: 0.007075848989188671\n",
      "Timesteps So Far: 40153\n",
      "Iteration took: 30.68 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Length: 110.81\n",
      "Average Episodic Return: -150.34\n",
      "Average Loss: -0.00658\n",
      "Average KL: 0.007680495735257864\n",
      "Timesteps So Far: 60209\n",
      "Iteration took: 28.04 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Length: 119.44\n",
      "Average Episodic Return: -91.18\n",
      "Average Loss: -0.00801\n",
      "Average KL: 0.008449590764939785\n",
      "Timesteps So Far: 80275\n",
      "Iteration took: 28.44 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Length: 132.5\n",
      "Average Episodic Return: -64.37\n",
      "Average Loss: -0.00866\n",
      "Average KL: 0.008836121298372746\n",
      "Timesteps So Far: 100282\n",
      "Iteration took: 30.41 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Length: 129.56\n",
      "Average Episodic Return: -41.68\n",
      "Average Loss: -0.00905\n",
      "Average KL: 0.009431502781808376\n",
      "Timesteps So Far: 120364\n",
      "Iteration took: 30.56 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Length: 192.94\n",
      "Average Episodic Return: -34.58\n",
      "Average Loss: -0.00875\n",
      "Average KL: 0.009668397717177868\n",
      "Timesteps So Far: 140430\n",
      "Iteration took: 43.94 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Length: 410.12\n",
      "Average Episodic Return: -17.17\n",
      "Average Loss: -0.00842\n",
      "Average KL: 0.009749865159392357\n",
      "Timesteps So Far: 160526\n",
      "Iteration took: 58.08 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Length: 599.8\n",
      "Average Episodic Return: -26.78\n",
      "Average Loss: -0.00768\n",
      "Average KL: 0.009739313274621964\n",
      "Timesteps So Far: 181519\n",
      "Iteration took: 67.92 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Length: 811.0\n",
      "Average Episodic Return: -20.58\n",
      "Average Loss: -0.00701\n",
      "Average KL: 0.009558620862662792\n",
      "Timesteps So Far: 202605\n",
      "Iteration took: 63.2 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Length: 839.33\n",
      "Average Episodic Return: 22.3\n",
      "Average Loss: -0.0064\n",
      "Average KL: 0.009390845894813538\n",
      "Timesteps So Far: 222749\n",
      "Iteration took: 63.38 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Length: 971.14\n",
      "Average Episodic Return: 8.69\n",
      "Average Loss: -0.00603\n",
      "Average KL: 0.009226509369909763\n",
      "Timesteps So Far: 243143\n",
      "Iteration took: 69.58 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Length: 1070.79\n",
      "Average Episodic Return: 22.83\n",
      "Average Loss: -0.00559\n",
      "Average KL: 0.009121103212237358\n",
      "Timesteps So Far: 263488\n",
      "Iteration took: 68.02 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Length: 1157.5\n",
      "Average Episodic Return: 32.64\n",
      "Average Loss: -0.00518\n",
      "Average KL: 0.009002470411360264\n",
      "Timesteps So Far: 284323\n",
      "Iteration took: 72.56 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Length: 1092.05\n",
      "Average Episodic Return: 35.23\n",
      "Average Loss: -0.00501\n",
      "Average KL: 0.008875890634953976\n",
      "Timesteps So Far: 305072\n",
      "Iteration took: 70.29 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Length: 1100.63\n",
      "Average Episodic Return: 45.72\n",
      "Average Loss: -0.00476\n",
      "Average KL: 0.008856634609401226\n",
      "Timesteps So Far: 325984\n",
      "Iteration took: 64.42 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Length: 1121.72\n",
      "Average Episodic Return: 59.22\n",
      "Average Loss: -0.00446\n",
      "Average KL: 0.008734796196222305\n",
      "Timesteps So Far: 346175\n",
      "Iteration took: 61.23 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Length: 1104.84\n",
      "Average Episodic Return: 93.45\n",
      "Average Loss: -0.00438\n",
      "Average KL: 0.008741472847759724\n",
      "Timesteps So Far: 367167\n",
      "Iteration took: 62.37 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Length: 1059.05\n",
      "Average Episodic Return: 58.39\n",
      "Average Loss: -0.0043\n",
      "Average KL: 0.00879008136689663\n",
      "Timesteps So Far: 388348\n",
      "Iteration took: 59.19 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Length: 1110.68\n",
      "Average Episodic Return: 92.41\n",
      "Average Loss: -0.00405\n",
      "Average KL: 0.008740848861634731\n",
      "Timesteps So Far: 409451\n",
      "Iteration took: 59.46 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Length: 1055.55\n",
      "Average Episodic Return: 81.62\n",
      "Average Loss: -0.00357\n",
      "Average KL: 0.008895812556147575\n",
      "Timesteps So Far: 430562\n",
      "Iteration took: 60.1 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Length: 1054.0\n",
      "Average Episodic Return: 95.12\n",
      "Average Loss: -0.00337\n",
      "Average KL: 0.00900398287922144\n",
      "Timesteps So Far: 450588\n",
      "Iteration took: 53.96 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Length: 1013.55\n",
      "Average Episodic Return: 73.64\n",
      "Average Loss: -0.00293\n",
      "Average KL: 0.009031523019075394\n",
      "Timesteps So Far: 470859\n",
      "Iteration took: 53.2 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_timesteps_to_train =  1_00_000\n",
    "\n",
    "ppo.train(total_timesteps_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "Run multiple episodes from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ActorCritic' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo.py:275\u001b[0m, in \u001b[0;36mPPO.test\u001b[1;34m(self, env_class)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_class\u001b[38;5;241m=\u001b[39mLunarContinuous):\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m()\n\u001b[0;32m    276\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_class(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ActorCritic' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "ppo.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
