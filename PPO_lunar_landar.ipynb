{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ppo import PPO\n",
    "from env.wrappers import LunarContinuous, LunarLanderWithUnknownWind,LunarLanderWithKnownWind\n",
    "from utils.logger import WandbSummaryWritter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     'timesteps_per_batch': 1024 ,                # Number of timesteps to run per batch\n",
    "#     'max_timesteps_per_episode': 1200,           # Max number of timesteps per episode\n",
    "#     'n_updates_per_iteration': 5,                # Number of times to update actor/critic per iteration\n",
    "#     'lr': 2.5e-4 ,                                # Learning rate of actor optimizer\n",
    "#     'gamma': 0.95,                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "# }\n",
    "# hyperparameters = {'gamma': 0.999, 'lr_gamma': 0.995,\n",
    "#                    'max_timesteps_per_episode': 1200,'lr': 0.005 }\n",
    "\n",
    "hyperparameters = {}\n",
    "\n",
    "misc_hyperparameters = {\n",
    "    'env': LunarContinuous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = False\n",
    "if LOG:\n",
    "    logger = WandbSummaryWritter(project='lunar', config =misc_hyperparameters['env']().load_hyperparameters())\n",
    "else:\n",
    "    logger=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'ppo_checkpoints/non_wandb/ppo_policy_50.pth'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "ppo = PPO(logger, **hyperparameters, **misc_hyperparameters)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ppo.restore_savestate(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model\n",
    "\n",
    "Train model for specified amount of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -10530.97\n",
      "Average Loss: 0.00211\n",
      "Average KL Divergence: 0.008579644936924943\n",
      "Iteration took: 8.69 secs, of which rollout took 6.96 secs and gradient updates took 1.73 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -9612.74\n",
      "Average Loss: -0.00181\n",
      "Average KL Divergence: 0.008028923712247258\n",
      "Iteration took: 9.92 secs, of which rollout took 8.3 secs and gradient updates took 1.61 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -8082.7\n",
      "Average Loss: -0.00263\n",
      "Average KL Divergence: 0.007990598172263838\n",
      "Iteration took: 9.88 secs, of which rollout took 8.41 secs and gradient updates took 1.46 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -5887.18\n",
      "Average Loss: -0.00379\n",
      "Average KL Divergence: 0.009798310983350907\n",
      "Iteration took: 9.82 secs, of which rollout took 8.34 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -3900.97\n",
      "Average Loss: -0.00493\n",
      "Average KL Divergence: 0.010428287242090077\n",
      "Iteration took: 10.12 secs, of which rollout took 8.61 secs and gradient updates took 1.5 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -2290.48\n",
      "Average Loss: -0.00523\n",
      "Average KL Divergence: 0.010618061902903556\n",
      "Iteration took: 10.02 secs, of which rollout took 8.49 secs and gradient updates took 1.52 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -1482.63\n",
      "Average Loss: -0.00512\n",
      "Average KL Divergence: 0.011293214970079473\n",
      "Iteration took: 12.09 secs, of which rollout took 10.62 secs and gradient updates took 1.46 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -1710.22\n",
      "Average Loss: -0.00466\n",
      "Average KL Divergence: 0.011463023184227093\n",
      "Iteration took: 13.4 secs, of which rollout took 11.89 secs and gradient updates took 1.5 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -609.38\n",
      "Average Loss: -0.00457\n",
      "Average KL Divergence: 0.011598747538061779\n",
      "Iteration took: 19.67 secs, of which rollout took 18.19 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Return: -511.23\n",
      "Average Loss: -0.00414\n",
      "Average KL Divergence: 0.011631648037650148\n",
      "Iteration took: 35.73 secs, of which rollout took 17.33 secs and gradient updates took 1.49 secs\n",
      "Current learning rate: 0.005\n",
      "Average Validation Return: -24.5 secs\n",
      "Average Validation Duration: 856.8\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Return: -65.76\n",
      "Average Loss: -0.00374\n",
      "Average KL Divergence: 0.011520057505214136\n",
      "Iteration took: 22.69 secs, of which rollout took 21.2 secs and gradient updates took 1.48 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Return: -750.14\n",
      "Average Loss: -0.00347\n",
      "Average KL Divergence: 0.01212275768187615\n",
      "Iteration took: 19.85 secs, of which rollout took 18.37 secs and gradient updates took 1.48 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Return: -406.29\n",
      "Average Loss: -0.0032\n",
      "Average KL Divergence: 0.012216507852668958\n",
      "Iteration took: 18.23 secs, of which rollout took 16.75 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Return: -98.34\n",
      "Average Loss: -0.00327\n",
      "Average KL Divergence: 0.01225125537357324\n",
      "Iteration took: 20.64 secs, of which rollout took 19.16 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Return: 229.48\n",
      "Average Loss: -0.00287\n",
      "Average KL Divergence: 0.012159410576737056\n",
      "Iteration took: 22.03 secs, of which rollout took 20.49 secs and gradient updates took 1.53 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Return: 189.97\n",
      "Average Loss: -0.00243\n",
      "Average KL Divergence: 0.012118658885667221\n",
      "Iteration took: 22.19 secs, of which rollout took 20.63 secs and gradient updates took 1.56 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Return: -6.43\n",
      "Average Loss: -0.00198\n",
      "Average KL Divergence: 0.011876950241259385\n",
      "Iteration took: 20.21 secs, of which rollout took 18.58 secs and gradient updates took 1.63 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Return: 293.77\n",
      "Average Loss: -0.00177\n",
      "Average KL Divergence: 0.011779318722426803\n",
      "Iteration took: 24.14 secs, of which rollout took 22.64 secs and gradient updates took 1.5 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Return: -32.87\n",
      "Average Loss: -0.00142\n",
      "Average KL Divergence: 0.01174214213724129\n",
      "Iteration took: 18.28 secs, of which rollout took 16.48 secs and gradient updates took 1.79 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Return: 108.67\n",
      "Average Loss: -0.00123\n",
      "Average KL Divergence: 0.011732917741218468\n",
      "Iteration took: 33.3 secs, of which rollout took 17.45 secs and gradient updates took 1.69 secs\n",
      "Current learning rate: 0.005\n",
      "Average Validation Return: 18.74 secs\n",
      "Average Validation Duration: 1192.2\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Return: 139.51\n",
      "Average Loss: -0.00097\n",
      "Average KL Divergence: 0.011613658492800173\n",
      "Iteration took: 20.26 secs, of which rollout took 18.46 secs and gradient updates took 1.79 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Return: 20.06\n",
      "Average Loss: -0.00083\n",
      "Average KL Divergence: 0.011550168021595076\n",
      "Iteration took: 20.66 secs, of which rollout took 18.82 secs and gradient updates took 1.84 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Return: 333.23\n",
      "Average Loss: -0.00058\n",
      "Average KL Divergence: 0.011620471563103574\n",
      "Iteration took: 19.51 secs, of which rollout took 17.99 secs and gradient updates took 1.51 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Return: 294.82\n",
      "Average Loss: -0.00027\n",
      "Average KL Divergence: 0.011527302387346968\n",
      "Iteration took: 20.71 secs, of which rollout took 18.85 secs and gradient updates took 1.86 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Return: 500.13\n",
      "Average Loss: -0.00014\n",
      "Average KL Divergence: 0.011467616503371915\n",
      "Iteration took: 19.29 secs, of which rollout took 17.61 secs and gradient updates took 1.68 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Return: 276.24\n",
      "Average Loss: -3e-05\n",
      "Average KL Divergence: 0.01138637290149349\n",
      "Iteration took: 16.88 secs, of which rollout took 15.34 secs and gradient updates took 1.54 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Return: 455.59\n",
      "Average Loss: 0.00017\n",
      "Average KL Divergence: 0.011422664948565553\n",
      "Iteration took: 16.08 secs, of which rollout took 14.27 secs and gradient updates took 1.8 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Return: 398.08\n",
      "Average Loss: 0.00011\n",
      "Average KL Divergence: 0.011336624034844242\n",
      "Iteration took: 17.31 secs, of which rollout took 15.68 secs and gradient updates took 1.62 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Return: 323.81\n",
      "Average Loss: 0.0001\n",
      "Average KL Divergence: 0.011278334356983756\n",
      "Iteration took: 17.5 secs, of which rollout took 15.6 secs and gradient updates took 1.89 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Return: 652.64\n",
      "Average Loss: 0.00015\n",
      "Average KL Divergence: 0.011175838989794702\n",
      "Iteration took: 32.72 secs, of which rollout took 16.64 secs and gradient updates took 1.59 secs\n",
      "Current learning rate: 0.005\n",
      "Average Validation Return: 92.07 secs\n",
      "Average Validation Duration: 1201.0\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Return: 426.29\n",
      "Average Loss: 0.00015\n",
      "Average KL Divergence: 0.011055677586201983\n",
      "Iteration took: 21.14 secs, of which rollout took 19.35 secs and gradient updates took 1.78 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Return: 328.6\n",
      "Average Loss: 0.00016\n",
      "Average KL Divergence: 0.011025324763160638\n",
      "Iteration took: 18.18 secs, of which rollout took 16.58 secs and gradient updates took 1.6 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Return: 682.78\n",
      "Average Loss: 0.00029\n",
      "Average KL Divergence: 0.010968145835356057\n",
      "Iteration took: 16.35 secs, of which rollout took 14.66 secs and gradient updates took 1.69 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Return: 445.41\n",
      "Average Loss: 0.00038\n",
      "Average KL Divergence: 0.010927917275781338\n",
      "Iteration took: 16.18 secs, of which rollout took 14.57 secs and gradient updates took 1.6 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Return: 466.7\n",
      "Average Loss: 0.00056\n",
      "Average KL Divergence: 0.010951664430042177\n",
      "Iteration took: 15.58 secs, of which rollout took 14.03 secs and gradient updates took 1.55 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #36 --------------------\n",
      "Average Episodic Return: 619.56\n",
      "Average Loss: 0.00065\n",
      "Average KL Divergence: 0.010950161433336384\n",
      "Iteration took: 17.7 secs, of which rollout took 16.06 secs and gradient updates took 1.64 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #37 --------------------\n",
      "Average Episodic Return: 571.31\n",
      "Average Loss: 0.00072\n",
      "Average KL Divergence: 0.01084917295525711\n",
      "Iteration took: 18.11 secs, of which rollout took 16.41 secs and gradient updates took 1.7 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #38 --------------------\n",
      "Average Episodic Return: 267.76\n",
      "Average Loss: 0.00073\n",
      "Average KL Divergence: 0.0107773412756551\n",
      "Iteration took: 16.25 secs, of which rollout took 14.59 secs and gradient updates took 1.65 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #39 --------------------\n",
      "Average Episodic Return: 675.1\n",
      "Average Loss: 0.0007\n",
      "Average KL Divergence: 0.010697493704274126\n",
      "Iteration took: 16.81 secs, of which rollout took 15.21 secs and gradient updates took 1.59 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #40 --------------------\n",
      "Average Episodic Return: 538.82\n",
      "Average Loss: 0.00085\n",
      "Average KL Divergence: 0.01076270516489811\n",
      "Iteration took: 28.93 secs, of which rollout took 16.47 secs and gradient updates took 1.52 secs\n",
      "Current learning rate: 0.005\n",
      "Average Validation Return: 74.24 secs\n",
      "Average Validation Duration: 1113.0\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #41 --------------------\n",
      "Average Episodic Return: 442.3\n",
      "Average Loss: 0.00087\n",
      "Average KL Divergence: 0.010719420083587101\n",
      "Iteration took: 16.25 secs, of which rollout took 14.42 secs and gradient updates took 1.82 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #42 --------------------\n",
      "Average Episodic Return: 358.91\n",
      "Average Loss: 0.00086\n",
      "Average KL Divergence: 0.010658818474570451\n",
      "Iteration took: 19.74 secs, of which rollout took 18.08 secs and gradient updates took 1.66 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #43 --------------------\n",
      "Average Episodic Return: 752.2\n",
      "Average Loss: 0.00089\n",
      "Average KL Divergence: 0.010654850700466222\n",
      "Iteration took: 17.29 secs, of which rollout took 15.57 secs and gradient updates took 1.72 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #44 --------------------\n",
      "Average Episodic Return: 364.17\n",
      "Average Loss: 0.00089\n",
      "Average KL Divergence: 0.010605744356209092\n",
      "Iteration took: 17.68 secs, of which rollout took 16.2 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #45 --------------------\n",
      "Average Episodic Return: 332.76\n",
      "Average Loss: 0.00097\n",
      "Average KL Divergence: 0.010605983722154984\n",
      "Iteration took: 17.24 secs, of which rollout took 15.67 secs and gradient updates took 1.56 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #46 --------------------\n",
      "Average Episodic Return: 417.81\n",
      "Average Loss: 0.00095\n",
      "Average KL Divergence: 0.010579309127040965\n",
      "Iteration took: 19.2 secs, of which rollout took 17.66 secs and gradient updates took 1.54 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #47 --------------------\n",
      "Average Episodic Return: 442.03\n",
      "Average Loss: 0.00104\n",
      "Average KL Divergence: 0.010596310706682701\n",
      "Iteration took: 20.56 secs, of which rollout took 19.1 secs and gradient updates took 1.45 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #48 --------------------\n",
      "Average Episodic Return: 457.77\n",
      "Average Loss: 0.00109\n",
      "Average KL Divergence: 0.01055971848911106\n",
      "Iteration took: 19.96 secs, of which rollout took 18.42 secs and gradient updates took 1.54 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #49 --------------------\n",
      "Average Episodic Return: 325.98\n",
      "Average Loss: 0.00112\n",
      "Average KL Divergence: 0.010555268167489504\n",
      "Iteration took: 19.52 secs, of which rollout took 17.77 secs and gradient updates took 1.75 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #50 --------------------\n",
      "Average Episodic Return: 519.52\n",
      "Average Loss: 0.00111\n",
      "Average KL Divergence: 0.010558192518394412\n",
      "Iteration took: 30.13 secs, of which rollout took 15.85 secs and gradient updates took 1.57 secs\n",
      "Current learning rate: 0.005\n",
      "Average Validation Return: 30.11 secs\n",
      "Average Validation Duration: 967.8\n",
      "------------------------------------------------------\n",
      "\n",
      "Checkpoint saved at ./ppo_checkpoints/non_wandb/ppo_policy_50.pth\n",
      "\n",
      "-------------------- Iteration #51 --------------------\n",
      "Average Episodic Return: 533.77\n",
      "Average Loss: 0.00112\n",
      "Average KL Divergence: 0.010534440829906013\n",
      "Iteration took: 17.82 secs, of which rollout took 16.06 secs and gradient updates took 1.74 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #52 --------------------\n",
      "Average Episodic Return: 393.57\n",
      "Average Loss: 0.00111\n",
      "Average KL Divergence: 0.010523022278027162\n",
      "Iteration took: 18.72 secs, of which rollout took 17.12 secs and gradient updates took 1.6 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #53 --------------------\n",
      "Average Episodic Return: 465.4\n",
      "Average Loss: 0.00115\n",
      "Average KL Divergence: 0.010517360605693218\n",
      "Iteration took: 17.78 secs, of which rollout took 16.21 secs and gradient updates took 1.57 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #54 --------------------\n",
      "Average Episodic Return: 456.78\n",
      "Average Loss: 0.00105\n",
      "Average KL Divergence: 0.01055284924111778\n",
      "Iteration took: 18.57 secs, of which rollout took 16.84 secs and gradient updates took 1.72 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #55 --------------------\n",
      "Average Episodic Return: 562.64\n",
      "Average Loss: 0.00109\n",
      "Average KL Divergence: 0.010573685276283457\n",
      "Iteration took: 17.68 secs, of which rollout took 16.17 secs and gradient updates took 1.5 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #56 --------------------\n",
      "Average Episodic Return: 651.98\n",
      "Average Loss: 0.00109\n",
      "Average KL Divergence: 0.010550727159968108\n",
      "Iteration took: 15.35 secs, of which rollout took 13.71 secs and gradient updates took 1.64 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #57 --------------------\n",
      "Average Episodic Return: 533.84\n",
      "Average Loss: 0.00232\n",
      "Average KL Divergence: 0.014377748620796658\n",
      "Iteration took: 16.44 secs, of which rollout took 14.89 secs and gradient updates took 1.54 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #58 --------------------\n",
      "Average Episodic Return: 395.04\n",
      "Average Loss: 0.00249\n",
      "Average KL Divergence: 0.014679057710197013\n",
      "Iteration took: 16.29 secs, of which rollout took 14.64 secs and gradient updates took 1.64 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #59 --------------------\n",
      "Average Episodic Return: 611.51\n",
      "Average Loss: 0.00251\n",
      "Average KL Divergence: 0.014577909186712052\n",
      "Iteration took: 16.75 secs, of which rollout took 15.12 secs and gradient updates took 1.62 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #60 --------------------\n",
      "Average Episodic Return: 407.4\n",
      "Average Loss: 0.00289\n",
      "Average KL Divergence: 0.01500514886971458\n",
      "Iteration took: 28.5 secs, of which rollout took 16.04 secs and gradient updates took 1.65 secs\n",
      "Current learning rate: 0.005\n",
      "Average Validation Return: 148.22 secs\n",
      "Average Validation Duration: 1087.4\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #61 --------------------\n",
      "Average Episodic Return: 589.22\n",
      "Average Loss: 0.00296\n",
      "Average KL Divergence: 0.014926984520161678\n",
      "Iteration took: 17.74 secs, of which rollout took 16.02 secs and gradient updates took 1.72 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo.py:68\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m rollout_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()  \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# obs, acts, log_probs, advantages, returns = self.rollout()\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m obs, acts, log_probs, values, advantages, returns, rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m rollout_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_so_far\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m it \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo.py:198\u001b[0m, in \u001b[0;36mPPO.rollout2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m     actions[step] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    197\u001b[0m     logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[1;32m--> 198\u001b[0m     rewards[step] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrew\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    199\u001b[0m     next_obs, next_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(next_obs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mTensor(next_done)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    201\u001b[0m next_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mget_value(next_obs)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "Run multiple episodes from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ppo.validate(val_iter=30)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ppo.test()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\ppo.py:274\u001b[0m, in \u001b[0;36mPPO.validate\u001b[1;34m(self, val_iter, should_record)\u001b[0m\n\u001b[0;32m    272\u001b[0m not_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mval_iter) \u001b[38;5;241m-\u001b[39m done\n\u001b[0;32m    273\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m not_done\n\u001b[1;32m--> 274\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m obs, rew, next_done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    276\u001b[0m ep_ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rew \u001b[38;5;241m*\u001b[39m not_done\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\network.py:98\u001b[0m, in \u001b[0;36mActorCritic.sample_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[1;32m---> 98\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     dist \u001b[38;5;241m=\u001b[39m MultivariateNormal(mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_mat)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\model\\network.py:44\u001b[0m, in \u001b[0;36mFNN.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[1;32m---> 44\u001b[0m     activation1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     45\u001b[0m     activation2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(activation1))\n\u001b[0;32m     46\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(activation2)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmsar\\.conda\\envs\\adlr\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# ppo.validate(val_iter=30)\n",
    "ppo.device = 'cpu'\n",
    "ppo.validate(10)\n",
    "# ppo.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
