{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from model.ppo3 import PPO\n",
    "from env.wrappers import LunarContinuous\n",
    "from utils.logger import WandbSummaryWritter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_hyperparameters = {\n",
    "    'save_freq': 0 ,  \n",
    "    'val_freq': 10,\n",
    "    'val_iter': 10,\n",
    "    'env': LunarContinuous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOTAL_TIMESTEPS_TO_TRAIN = 500\n",
    "VAL_ITER = 20\n",
    "MAX_RUN_COUNT = 30\n",
    "sweep_config = {\n",
    "    'method': 'bayes', \n",
    "    'metric': {\n",
    "        'name': 'validation_rewards',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 0.1\n",
    "        },\n",
    "        'gamma': {\n",
    "            'min': 0.9,\n",
    "            'max': 1.\n",
    "        },\n",
    "        # 'lr_gamma': {\n",
    "        #     'min': 0.999,\n",
    "        #     'max': 1.\n",
    "        # },\n",
    "        'lam': {\n",
    "            'min': 0.9,\n",
    "            'max': 1.\n",
    "        },\n",
    "        'max_grad_norm': {\n",
    "            \"distribution\": \"q_log_uniform\",\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 10, \n",
    "        },\n",
    "        'n_updates_per_iteration': {\n",
    "            'values': list(range(1, 21))\n",
    "        },\n",
    "        'num_envs': {\n",
    "            'values': list(range(1, 100))\n",
    "        },\n",
    "        'anneal_lr': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'num_steps': {\n",
    "            'distribution': 'q_uniform',\n",
    "            'min': 300,\n",
    "            'max': 4000,\n",
    "            'q': 100\n",
    "        },'batches': {\n",
    "            'distribution': 'q_uniform',\n",
    "            \"min\": 1,     # 2^0\n",
    "            \"max\": 1024,  # 2^10\n",
    "            \"q\": 2 \n",
    "        }\n",
    "    },\n",
    "     \"constraints\": [\n",
    "        {\"params\": [\"num_envs\", \"num_steps\"], \"max_product\": MAX_TOTAL_TIMESTEPS_TO_TRAIN}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config = None):\n",
    "    logger = WandbSummaryWritter(project='lunar', config =config)\n",
    "    ppo = ppo = PPO(logger,**misc_hyperparameters) if config is None else PPO(summary_writter=logger, **config, **misc_hyperparameters)\n",
    "    ppo.train()\n",
    "\n",
    "    val_rews, val_dur = ppo.validate(VAL_ITER, False)\n",
    "\n",
    "    wandb.log({\n",
    "        \"validation_rewards\": val_rews,\n",
    "        \"validation_duration\": val_dur\n",
    "        # \"max_reward_video\": wandb.Video(f\"videos\\\\rl-video-episode-{np.argmax(val_rews)}.mp4\", fps=4, format=\"mp4\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('constraints' was unexpected)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. max_grad_norm uses q_log_uniform, where min/max specify base-e exponents. Use q_log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7ig4yvcl\n",
      "Sweep URL: https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/7ig4yvcl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q6vwh2dc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_lr: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches: 146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9729489254725824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlam: 0.9141777110536656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.05286455878929416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 5591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_updates_per_iteration: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_steps: 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpmsaraiva2712\u001b[0m (\u001b[33mpmsaraiva2712-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\wandb\\run-20241213_200818-q6vwh2dc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/q6vwh2dc' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/7ig4yvcl' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/7ig4yvcl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/7ig4yvcl' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/7ig4yvcl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/q6vwh2dc' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/q6vwh2dc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -199.22\n",
      "Average Loss: 0.0006\n",
      "Average KL Divergence: 0.005822817034249266\n",
      "Iteration took: 12.53 secs, of which rollout took 10.25 secs and gradient updates took 2.28 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -255.5\n",
      "Average Loss: -0.00253\n",
      "Average KL Divergence: 0.006206803589707131\n",
      "Iteration took: 12.36 secs, of which rollout took 10.35 secs and gradient updates took 1.99 secs\n",
      "Current learning rate: 0.00495\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -136.41\n",
      "Average Loss: -0.00338\n",
      "Average KL Divergence: 0.007447491814573717\n",
      "Iteration took: 12.39 secs, of which rollout took 10.21 secs and gradient updates took 2.15 secs\n",
      "Current learning rate: 0.004851\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -97.06\n",
      "Average Loss: -0.00278\n",
      "Average KL Divergence: 0.007771789457178522\n",
      "Iteration took: 14.3 secs, of which rollout took 12.32 secs and gradient updates took 1.96 secs\n",
      "Current learning rate: 0.00470547\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -79.86\n",
      "Average Loss: -0.00321\n",
      "Average KL Divergence: 0.007927486071171188\n",
      "Iteration took: 12.34 secs, of which rollout took 10.31 secs and gradient updates took 2.01 secs\n",
      "Current learning rate: 0.0045172512\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -46.42\n",
      "Average Loss: -0.00335\n",
      "Average KL Divergence: 0.00826607487154416\n",
      "Iteration took: 12.5 secs, of which rollout took 10.43 secs and gradient updates took 2.05 secs\n",
      "Current learning rate: 0.00429138864\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -38.84\n",
      "Average Loss: -0.00391\n",
      "Average KL Divergence: 0.008583352652584796\n",
      "Iteration took: 13.8 secs, of which rollout took 11.79 secs and gradient updates took 1.99 secs\n",
      "Current learning rate: 0.0040339053216\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -91.29\n",
      "Average Loss: -0.00366\n",
      "Average KL Divergence: 0.008517476102173085\n",
      "Iteration took: 21.18 secs, of which rollout took 19.14 secs and gradient updates took 2.02 secs\n",
      "Current learning rate: 0.003751531949088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -58.5\n",
      "Average Loss: -0.00366\n",
      "Average KL Divergence: 0.00855555948815428\n",
      "Iteration took: 21.23 secs, of which rollout took 19.62 secs and gradient updates took 1.6 secs\n",
      "Current learning rate: 0.00345140939316096\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Return: -71.41\n",
      "Average Loss: -0.00361\n",
      "Average KL Divergence: 0.008694123839929593\n",
      "Iteration took: 58.91 secs, of which rollout took 23.3 secs and gradient updates took 2.16 secs\n",
      "Current learning rate: 0.0031407825477764734\n",
      "Average Validation Return: -64.72 secs\n",
      "Average Validation Duration: 1125.0\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Return: -66.01\n",
      "Average Loss: -0.0033\n",
      "Average KL Divergence: 0.008636820767809014\n",
      "Iteration took: 18.75 secs, of which rollout took 17.2 secs and gradient updates took 1.54 secs\n",
      "Current learning rate: 0.0028267042929988264\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Return: 14.55\n",
      "Average Loss: -0.00323\n",
      "Average KL Divergence: 0.008533744743762567\n",
      "Iteration took: 20.05 secs, of which rollout took 18.53 secs and gradient updates took 1.5 secs\n",
      "Current learning rate: 0.0025157668207689554\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Return: -29.12\n",
      "Average Loss: -0.00301\n",
      "Average KL Divergence: 0.008335686154546185\n",
      "Iteration took: 22.68 secs, of which rollout took 21.05 secs and gradient updates took 1.62 secs\n",
      "Current learning rate: 0.0022138748022766806\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Return: 58.41\n",
      "Average Loss: -0.00287\n",
      "Average KL Divergence: 0.008154948251180216\n",
      "Iteration took: 20.9 secs, of which rollout took 19.35 secs and gradient updates took 1.53 secs\n",
      "Current learning rate: 0.001926071077980712\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Return: 39.95\n",
      "Average Loss: -0.00272\n",
      "Average KL Divergence: 0.008005434648985685\n",
      "Iteration took: 20.62 secs, of which rollout took 19.12 secs and gradient updates took 1.49 secs\n",
      "Current learning rate: 0.0016564211270634125\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Return: 45.34\n",
      "Average Loss: -0.00267\n",
      "Average KL Divergence: 0.007858108759275239\n",
      "Iteration took: 21.97 secs, of which rollout took 20.43 secs and gradient updates took 1.53 secs\n",
      "Current learning rate: 0.0014079579580039006\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Return: 20.91\n",
      "Average Loss: -0.00259\n",
      "Average KL Divergence: 0.007668274039219506\n",
      "Iteration took: 19.67 secs, of which rollout took 18.05 secs and gradient updates took 1.6 secs\n",
      "Current learning rate: 0.0011826846847232766\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Return: 55.63\n",
      "Average Loss: -0.00248\n",
      "Average KL Divergence: 0.00750876494514332\n",
      "Iteration took: 20.14 secs, of which rollout took 18.52 secs and gradient updates took 1.61 secs\n",
      "Current learning rate: 0.0009816282883203195\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Return: 57.36\n",
      "Average Loss: -0.00238\n",
      "Average KL Divergence: 0.007306427234874741\n",
      "Iteration took: 23.45 secs, of which rollout took 21.82 secs and gradient updates took 1.62 secs\n",
      "Current learning rate: 0.000804935196422662\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Return: 5.06\n",
      "Average Loss: -0.00224\n",
      "Average KL Divergence: 0.007053752754742565\n",
      "Iteration took: 50.18 secs, of which rollout took 18.85 secs and gradient updates took 1.48 secs\n",
      "Current learning rate: 0.0006519975091023563\n",
      "Average Validation Return: 44.95 secs\n",
      "Average Validation Duration: 1201.0\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Return: 66.56\n",
      "Average Loss: -0.0022\n",
      "Average KL Divergence: 0.006838909472113523\n",
      "Iteration took: 22.37 secs, of which rollout took 20.73 secs and gradient updates took 1.63 secs\n",
      "Current learning rate: 0.000521598007281885\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Return: 50.76\n",
      "Average Loss: -0.00222\n",
      "Average KL Divergence: 0.006708625086140557\n",
      "Iteration took: 21.28 secs, of which rollout took 19.79 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 0.00041206242575268917\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Return: 37.61\n",
      "Average Loss: -0.00217\n",
      "Average KL Divergence: 0.006548066929302875\n",
      "Iteration took: 17.32 secs, of which rollout took 15.44 secs and gradient updates took 1.86 secs\n",
      "Current learning rate: 0.00032140869208709756\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Return: 57.54\n",
      "Average Loss: -0.00208\n",
      "Average KL Divergence: 0.0062953033363838855\n",
      "Iteration took: 20.52 secs, of which rollout took 18.89 secs and gradient updates took 1.62 secs\n",
      "Current learning rate: 0.00024748469290706513\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Return: 71.5\n",
      "Average Loss: -0.00205\n",
      "Average KL Divergence: 0.0060996550569340025\n",
      "Iteration took: 21.93 secs, of which rollout took 20.23 secs and gradient updates took 1.69 secs\n",
      "Current learning rate: 0.0001880883666093695\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Return: 58.45\n",
      "Average Loss: -0.00207\n",
      "Average KL Divergence: 0.005918372347637179\n",
      "Iteration took: 22.62 secs, of which rollout took 20.96 secs and gradient updates took 1.65 secs\n",
      "Current learning rate: 0.00014106627495702714\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Return: 74.72\n",
      "Average Loss: -0.002\n",
      "Average KL Divergence: 0.005723717905269025\n",
      "Iteration took: 23.34 secs, of which rollout took 21.61 secs and gradient updates took 1.72 secs\n",
      "Current learning rate: 0.00010438904346820009\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Return: 59.68\n",
      "Average Loss: -0.00197\n",
      "Average KL Divergence: 0.005524978267138305\n",
      "Iteration took: 19.69 secs, of which rollout took 18.24 secs and gradient updates took 1.44 secs\n",
      "Current learning rate: 7.620400173178606e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Return: 31.77\n",
      "Average Loss: -0.00187\n",
      "Average KL Divergence: 0.00534093722227595\n",
      "Iteration took: 20.02 secs, of which rollout took 18.54 secs and gradient updates took 1.47 secs\n",
      "Current learning rate: 5.486688124688596e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Return: 53.66\n",
      "Average Loss: -0.00178\n",
      "Average KL Divergence: 0.005164468673917104\n",
      "Iteration took: 48.5 secs, of which rollout took 20.76 secs and gradient updates took 1.86 secs\n",
      "Current learning rate: 3.8955485685289034e-05\n",
      "Average Validation Return: -7.65 secs\n",
      "Average Validation Duration: 1170.3\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Return: 47.71\n",
      "Average Loss: -0.00167\n",
      "Average KL Divergence: 0.004999126479274366\n",
      "Iteration took: 20.99 secs, of which rollout took 19.43 secs and gradient updates took 1.53 secs\n",
      "Current learning rate: 2.726883997970232e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Return: 33.01\n",
      "Average Loss: -0.00167\n",
      "Average KL Divergence: 0.004843111617529469\n",
      "Iteration took: 20.92 secs, of which rollout took 19.25 secs and gradient updates took 1.66 secs\n",
      "Current learning rate: 1.88154995859946e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Return: 62.76\n",
      "Average Loss: -0.00161\n",
      "Average KL Divergence: 0.004696805144621168\n",
      "Iteration took: 23.41 secs, of which rollout took 21.64 secs and gradient updates took 1.74 secs\n",
      "Current learning rate: 1.2794539718476328e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Return: 69.04\n",
      "Average Loss: -0.00166\n",
      "Average KL Divergence: 0.004558831431003489\n",
      "Iteration took: 20.47 secs, of which rollout took 18.66 secs and gradient updates took 1.8 secs\n",
      "Current learning rate: 8.57234161137914e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Return: 55.07\n",
      "Average Loss: -0.00162\n",
      "Average KL Divergence: 0.004428631502784038\n",
      "Iteration took: 21.93 secs, of which rollout took 20.13 secs and gradient updates took 1.79 secs\n",
      "Current learning rate: 5.657745463510232e-06\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"lunar\")\n",
    "wandb.agent(sweep_id, function=train_model, count=MAX_RUN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'lr': 0.033773968633186116, 'lam': 0.965122224947915, 'gamma': 0.9391731546579618, 'lr_gamma': 0.99964198121568, 'max_grad_norm': 1.0549291822676827, 'n_sgd_batches': 8, 'timesteps_per_batch': 6600, 'n_updates_per_iteration': 17, 'max_timesteps_per_episode': 800}\n",
      "Best Metrics: {'val_rewards': -117.685825451997, '_runtime': 18.8714706, '_step': 2, '_timestamp': 1733497014.9633105, '_wandb': {'runtime': 18}, 'average_episode_lengths': 97.5, 'average_episode_rewards': -214.25900286086735, 'average_loss': 0.001098420703783631, 'learning_rate': 0.004990008995201681, 'max_reward_video': {'_type': 'video-file', 'path': 'media/videos/max_reward_video_2_de1368eb4a9cffe45bc9.mp4', 'sha256': 'de1368eb4a9cffe45bc98fb1781e53c0619e8cb010e012a453b94f289e4f54ad', 'size': 10645}, 'simulated_iterations': 2, 'simulated_timesteps': 9719, 'validation_duration': 76.4}\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "# Fetch the sweep object\n",
    "sweep = api.sweep(f\"pmsaraiva2712-tum/lunar/{sweep_id}\")\n",
    "\n",
    "# Fetch all runs from the sweep\n",
    "runs = sweep.runs\n",
    "\n",
    "# Sort runs by the metric you are optimizing for, e.g., 'val_loss'\n",
    "best_run = sorted(runs, key=lambda run: run.summary.get('val_rewards', float('-inf')), reverse=True)[0]\n",
    "\n",
    "# Extract best hyperparameters and metrics\n",
    "best_params = best_run.config\n",
    "best_metrics = best_run.summary\n",
    "\n",
    "# Print the best hyperparameters and metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Metrics:\", best_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
