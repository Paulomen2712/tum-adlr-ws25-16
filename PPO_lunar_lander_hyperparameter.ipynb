{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import wandb\n",
    "import torch\n",
    "from model.ppo_3 import PPO\n",
    "import numpy as np\n",
    "from env.wrappers import LunarContinuous, LunarLanderWithUnknownWind,LunarLanderWithKnownWind\n",
    "from utils.logger import WandbSummaryWritter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_hyperparameters = {\n",
    "    'save_freq': 0 ,  \n",
    "    'val_freq': 10,\n",
    "    'val_iter': 10,\n",
    "    'env': LunarLanderWithKnownWind\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_ITER = 100\n",
    "MAX_RUN_COUNT = 30\n",
    "sweep_config = {\n",
    "    'method': 'bayes', \n",
    "    'metric': {\n",
    "        'name': 'mean_validation_reward',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'actor_lr': {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 0.009\n",
    "        },\n",
    "        # 'critic_lr': {\n",
    "        #     \"distribution\": \"uniform\",\n",
    "        #     \"min\": 1e-5,\n",
    "        #     \"max\": 0.1\n",
    "        # },\n",
    "        'adp_lr': {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 0.009\n",
    "        },\n",
    "        # 'gamma': {\n",
    "        #     'min': 0.9,\n",
    "        #     'max': 1.\n",
    "        # },\n",
    "        # 'lam': {\n",
    "        #     'min': 0.9,\n",
    "        #     'max': 1.\n",
    "        # },\n",
    "        'max_grad_norm': {\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 1., \n",
    "        },\n",
    "        'n_updates_per_iteration': {\n",
    "            'values': list(range(1, 21))\n",
    "        },\n",
    "        'num_envs': {\n",
    "            'values': list(range(1, 50))\n",
    "        },\n",
    "        'anneal_lr': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        # 'num_steps': {\n",
    "        #     'distribution': 'q_uniform',\n",
    "        #     'min': 300,\n",
    "        #     'max': 4000,\n",
    "        #     'q': 100\n",
    "        # },\n",
    "        # 'adp_num_steps': {\n",
    "        #     'distribution': 'q_uniform',\n",
    "        #     'min': 200,\n",
    "        #     'max': 1000,\n",
    "        #     'q': 10\n",
    "        # },\n",
    "        'num_steps': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 100,\n",
    "            'max': 1500\n",
    "        },\n",
    "        'adp_num_steps': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 100,\n",
    "            'max': 800\n",
    "        },\n",
    "        'anneal_discount': {\n",
    "            'distribution': 'q_uniform',\n",
    "            'min': 1,\n",
    "            'max': 1000,\n",
    "            'q': 10\n",
    "        },\n",
    "        'n_sgd_batches': {\n",
    "            'distribution': 'q_uniform',\n",
    "            \"min\": 1,     # 2^0\n",
    "            \"max\": 1024,  # 2^10\n",
    "            \"q\": 2 \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config = None):\n",
    "    logger = WandbSummaryWritter(project='lunar', config =config)\n",
    "    ppo = PPO(logger,**misc_hyperparameters) if wandb.config is None else PPO(summary_writter=logger, **wandb.config, **misc_hyperparameters)\n",
    "    ppo.train()\n",
    "\n",
    "    base_val_rews, base_val_dur = ppo.validate(VAL_ITER, False, False)\n",
    "    adp_val_rews, adp_val_dur = ppo.validate(VAL_ITER, False, True)\n",
    "    wind_vals, base_z, adpt_z = ppo.validate_encoders()\n",
    "    ppo.env.close()\n",
    "\n",
    "    # hist, bin_edges = np.histogram(val_rews, bins=20)\n",
    "    # print(f\"Len bin_edges: {len(bin_edges)}\")\n",
    "    # Debugging prints\n",
    "    wandb.log({\n",
    "        \"validation_rewards\": base_val_rews,\n",
    "        \"mean_validation_reward\": np.mean(base_val_rews),\n",
    "        \"validation_duration\": base_val_dur,\n",
    "        \"maximum_base_validation_reward\": np.max(base_val_rews),\n",
    "        \"adp_validation_rewards\": adp_val_rews,\n",
    "        \"adp_validation_duration\": adp_val_dur,\n",
    "        \"adp_maximum_base_validation_reward\": np.max(adp_val_rews),\n",
    "\n",
    "        \"Encoder Outputs\": wandb.plot.line_series(\n",
    "                xs=wind_vals,\n",
    "                ys=[base_z, adpt_z],\n",
    "                keys=[\"Base Encoder\", \"Adaptive Encoder\"],\n",
    "                title=\"Encoder Outputs vs Wind\",\n",
    "                xname=\"Wind Value\"\n",
    "            )\n",
    "\n",
    "        # \"max_reward_video\": wandb.Video(f\"./videos/rl-video{np.argmax(adp_val_rews)}-episode-{np.argmax(adp_val_rews)}.mp4\", fps=4, format=\"mp4\")\n",
    "    })\n",
    "    logger.save_histogram(base_val_rews, \"Base Validation Rewards\")\n",
    "    logger.save_histogram(base_val_dur, \"Base Validation Duration\")\n",
    "    logger.save_histogram(adp_val_rews, \"Adaptive Validation Rewards\")\n",
    "    logger.save_histogram(adp_val_dur, \"Adaptive Validation Duration\")\n",
    "    logger.save_model(ppo.policy, \"base_model\")\n",
    "    logger.save_model(ppo.adapt_policy, \"adp_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xoxts7ry\n",
      "Sweep URL: https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 96hsmq2q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactor_lr: 0.002843456264033954\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadp_lr: 0.0013946050760227774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadp_num_steps: 162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_discount: 810\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_lr: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.400312904518112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_sgd_batches: 1006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_updates_per_iteration: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_steps: 633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamedrostom\u001b[0m (\u001b[33mpmsaraiva2712-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mohamedrostom62/ADLR/tum-adlr-ws25-16/wandb/run-20250116_124736-96hsmq2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/96hsmq2q' target=\"_blank\">lilac-sweep-1</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/96hsmq2q' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/96hsmq2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -318.14\n",
      "Average Actor Loss: -0.16976\n",
      "Average Critic Loss: 537.7633963299121\n",
      "Average KL Divergence: 0.057129782401028506\n",
      "Iteration took: 35.05 secs, of which rollout took 2.78 secs and gradient updates took 32.27 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -466.43\n",
      "Average Actor Loss: -0.19381\n",
      "Average Critic Loss: 656.180673928231\n",
      "Average KL Divergence: 0.034101993665184356\n",
      "Iteration took: 35.51 secs, of which rollout took 2.71 secs and gradient updates took 32.79 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -275.38\n",
      "Average Actor Loss: -0.19935\n",
      "Average Critic Loss: 650.9459664046119\n",
      "Average KL Divergence: 0.03027536095456224\n",
      "Iteration took: 35.58 secs, of which rollout took 2.78 secs and gradient updates took 32.79 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -192.09\n",
      "Average Actor Loss: -0.20266\n",
      "Average Critic Loss: 557.1398915764984\n",
      "Average KL Divergence: 0.026166918755348108\n",
      "Iteration took: 36.1 secs, of which rollout took 3.04 secs and gradient updates took 33.05 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -208.42\n",
      "Average Actor Loss: -0.20374\n",
      "Average Critic Loss: 489.2167254381049\n",
      "Average KL Divergence: 0.02472057089488535\n",
      "Iteration took: 35.67 secs, of which rollout took 3.17 secs and gradient updates took 32.49 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -114.74\n",
      "Average Actor Loss: -0.2046\n",
      "Average Critic Loss: 430.3775404035985\n",
      "Average KL Divergence: 0.023407489494920736\n",
      "Iteration took: 35.79 secs, of which rollout took 3.42 secs and gradient updates took 32.36 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -84.7\n",
      "Average Actor Loss: -0.20615\n",
      "Average Critic Loss: 384.4841309391019\n",
      "Average KL Divergence: 0.022302284246936418\n",
      "Iteration took: 35.88 secs, of which rollout took 3.33 secs and gradient updates took 32.55 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -108.23\n",
      "Average Actor Loss: -0.20685\n",
      "Average Critic Loss: 351.3239927062311\n",
      "Average KL Divergence: 0.021721731744689807\n",
      "Iteration took: 34.68 secs, of which rollout took 2.93 secs and gradient updates took 31.75 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -74.24\n",
      "Average Actor Loss: -0.20663\n",
      "Average Critic Loss: 327.60240885430113\n",
      "Average KL Divergence: 0.02207880176297622\n",
      "Iteration took: 37.66 secs, of which rollout took 5.87 secs and gradient updates took 31.78 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Return: -48.66\n",
      "Average Actor Loss: -0.20616\n",
      "Average Critic Loss: 319.6790117746098\n",
      "Average KL Divergence: 0.021672592160206002\n",
      "Iteration took: 40.71 secs, of which rollout took 5.81 secs and gradient updates took 31.4 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: -28.46\n",
      "Average Validation Duration: 206.2 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Return: -48.41\n",
      "Average Actor Loss: -0.20641\n",
      "Average Critic Loss: 303.6015553482916\n",
      "Average KL Divergence: 0.021216607035719107\n",
      "Iteration took: 35.34 secs, of which rollout took 3.71 secs and gradient updates took 31.62 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Return: -95.15\n",
      "Average Actor Loss: -0.20643\n",
      "Average Critic Loss: 290.37624943270947\n",
      "Average KL Divergence: 0.02085303790598068\n",
      "Iteration took: 35.47 secs, of which rollout took 4.11 secs and gradient updates took 31.36 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Return: 2.47\n",
      "Average Actor Loss: -0.2068\n",
      "Average Critic Loss: 273.57495882690927\n",
      "Average KL Divergence: 0.020499571439823436\n",
      "Iteration took: 35.66 secs, of which rollout took 3.83 secs and gradient updates took 31.83 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Return: -80.72\n",
      "Average Actor Loss: -0.20685\n",
      "Average Critic Loss: 260.5701462107009\n",
      "Average KL Divergence: 0.020161650416858964\n",
      "Iteration took: 37.21 secs, of which rollout took 5.27 secs and gradient updates took 31.93 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Return: -91.35\n",
      "Average Actor Loss: -0.2063\n",
      "Average Critic Loss: 254.08066392482257\n",
      "Average KL Divergence: 0.020670287282406347\n",
      "Iteration took: 37.72 secs, of which rollout took 6.05 secs and gradient updates took 31.66 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Return: -28.94\n",
      "Average Actor Loss: -0.20648\n",
      "Average Critic Loss: 243.73597988388903\n",
      "Average KL Divergence: 0.02034084598398308\n",
      "Iteration took: 40.57 secs, of which rollout took 8.7 secs and gradient updates took 31.87 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Return: -9.43\n",
      "Average Actor Loss: -0.20658\n",
      "Average Critic Loss: 233.82305823413637\n",
      "Average KL Divergence: 0.020079728542617425\n",
      "Iteration took: 40.68 secs, of which rollout took 8.79 secs and gradient updates took 31.88 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Return: 2.86\n",
      "Average Actor Loss: -0.20667\n",
      "Average Critic Loss: 223.78337509753453\n",
      "Average KL Divergence: 0.01984505630174142\n",
      "Iteration took: 40.98 secs, of which rollout took 9.09 secs and gradient updates took 31.88 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Return: -7.04\n",
      "Average Actor Loss: -0.20664\n",
      "Average Critic Loss: 214.89715104740023\n",
      "Average KL Divergence: 0.019743987196970725\n",
      "Iteration took: 41.4 secs, of which rollout took 9.72 secs and gradient updates took 31.67 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Return: 12.37\n",
      "Average Actor Loss: -0.20684\n",
      "Average Critic Loss: 207.08834122318564\n",
      "Average KL Divergence: 0.0195589836283077\n",
      "Iteration took: 47.93 secs, of which rollout took 8.14 secs and gradient updates took 32.14 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 1.3\n",
      "Average Validation Duration: 481.7 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Return: -39.86\n",
      "Average Actor Loss: -0.2065\n",
      "Average Critic Loss: 201.4465325419879\n",
      "Average KL Divergence: 0.019731899608479993\n",
      "Iteration took: 41.28 secs, of which rollout took 9.18 secs and gradient updates took 32.09 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Return: 34.83\n",
      "Average Actor Loss: -0.20641\n",
      "Average Critic Loss: 195.05732429188384\n",
      "Average KL Divergence: 0.01965585611551146\n",
      "Iteration took: 40.42 secs, of which rollout took 8.76 secs and gradient updates took 31.65 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Return: 12.74\n",
      "Average Actor Loss: -0.20641\n",
      "Average Critic Loss: 188.47526902948442\n",
      "Average KL Divergence: 0.019607504973624273\n",
      "Iteration took: 41.29 secs, of which rollout took 9.19 secs and gradient updates took 32.09 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Return: 16.51\n",
      "Average Actor Loss: -0.20618\n",
      "Average Critic Loss: 183.0250872757935\n",
      "Average KL Divergence: 0.01959067685944367\n",
      "Iteration took: 41.64 secs, of which rollout took 9.66 secs and gradient updates took 31.97 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Return: -44.42\n",
      "Average Actor Loss: -0.20613\n",
      "Average Critic Loss: 178.26959645398782\n",
      "Average KL Divergence: 0.019550248768371484\n",
      "Iteration took: 39.39 secs, of which rollout took 7.23 secs and gradient updates took 32.15 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Return: 43.23\n",
      "Average Actor Loss: -0.20612\n",
      "Average Critic Loss: 172.9200213205184\n",
      "Average KL Divergence: 0.01947127402796416\n",
      "Iteration took: 41.37 secs, of which rollout took 8.96 secs and gradient updates took 32.4 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Return: 74.18\n",
      "Average Actor Loss: -0.20619\n",
      "Average Critic Loss: 167.49742063434527\n",
      "Average KL Divergence: 0.019396760919712562\n",
      "Iteration took: 41.26 secs, of which rollout took 9.32 secs and gradient updates took 31.93 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Return: 47.45\n",
      "Average Actor Loss: -0.20619\n",
      "Average Critic Loss: 163.010399266283\n",
      "Average KL Divergence: 0.019362429194405287\n",
      "Iteration took: 40.19 secs, of which rollout took 8.48 secs and gradient updates took 31.71 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Return: 13.47\n",
      "Average Actor Loss: -0.2063\n",
      "Average Critic Loss: 159.54040877871842\n",
      "Average KL Divergence: 0.01935338535550961\n",
      "Iteration took: 39.1 secs, of which rollout took 7.49 secs and gradient updates took 31.6 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Return: 39.78\n",
      "Average Actor Loss: -0.20644\n",
      "Average Critic Loss: 155.7321950855241\n",
      "Average KL Divergence: 0.019302186362600962\n",
      "Iteration took: 47.74 secs, of which rollout took 8.83 secs and gradient updates took 32.24 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 59.2\n",
      "Average Validation Duration: 519.4 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Return: 80.34\n",
      "Average Actor Loss: -0.2063\n",
      "Average Critic Loss: 152.64404086406918\n",
      "Average KL Divergence: 0.019485117725963823\n",
      "Iteration took: 41.07 secs, of which rollout took 8.92 secs and gradient updates took 32.14 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Return: 104.41\n",
      "Average Actor Loss: -0.20639\n",
      "Average Critic Loss: 148.40521357265\n",
      "Average KL Divergence: 0.019310189008554437\n",
      "Iteration took: 40.2 secs, of which rollout took 8.36 secs and gradient updates took 31.84 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Return: 74.02\n",
      "Average Actor Loss: -0.2063\n",
      "Average Critic Loss: 144.71138919877882\n",
      "Average KL Divergence: 0.019267368454553173\n",
      "Iteration took: 40.71 secs, of which rollout took 8.76 secs and gradient updates took 31.94 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Return: 47.29\n",
      "Average Actor Loss: -0.20629\n",
      "Average Critic Loss: 141.8623451255007\n",
      "Average KL Divergence: 0.019157848363741597\n",
      "Iteration took: 39.44 secs, of which rollout took 7.95 secs and gradient updates took 31.49 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Return: 41.91\n",
      "Average Actor Loss: -0.20616\n",
      "Average Critic Loss: 140.0156322994638\n",
      "Average KL Divergence: 0.01917852980168984\n",
      "Iteration took: 39.09 secs, of which rollout took 7.55 secs and gradient updates took 31.53 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #36 --------------------\n",
      "Average Episodic Return: 83.57\n",
      "Average Actor Loss: -0.20624\n",
      "Average Critic Loss: 137.90036224700037\n",
      "Average KL Divergence: 0.019117917100817963\n",
      "Iteration took: 38.61 secs, of which rollout took 7.9 secs and gradient updates took 30.71 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #37 --------------------\n",
      "Average Episodic Return: 96.26\n",
      "Average Actor Loss: -0.20625\n",
      "Average Critic Loss: 134.76148583628378\n",
      "Average KL Divergence: 0.01896913932166259\n",
      "Iteration took: 39.54 secs, of which rollout took 8.57 secs and gradient updates took 30.96 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #38 --------------------\n",
      "Average Episodic Return: 107.81\n",
      "Average Actor Loss: -0.20629\n",
      "Average Critic Loss: 131.83978027358236\n",
      "Average KL Divergence: 0.018833671897165928\n",
      "Iteration took: 38.62 secs, of which rollout took 7.85 secs and gradient updates took 30.75 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #39 --------------------\n",
      "Average Episodic Return: 69.63\n",
      "Average Actor Loss: -0.20659\n",
      "Average Critic Loss: 129.1940177240005\n",
      "Average KL Divergence: 0.03341155636733027\n",
      "Iteration took: 38.07 secs, of which rollout took 7.43 secs and gradient updates took 30.63 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #40 --------------------\n",
      "Average Episodic Return: 112.93\n",
      "Average Actor Loss: -0.20642\n",
      "Average Critic Loss: 126.32937722646179\n",
      "Average KL Divergence: 0.033055182522907775\n",
      "Iteration took: 45.17 secs, of which rollout took 8.24 secs and gradient updates took 30.77 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 106.1\n",
      "Average Validation Duration: 544.4 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #41 --------------------\n",
      "Average Episodic Return: 124.61\n",
      "Average Actor Loss: -0.20638\n",
      "Average Critic Loss: 123.86558257117244\n",
      "Average KL Divergence: 0.0326747678684318\n",
      "Iteration took: 39.21 secs, of which rollout took 8.17 secs and gradient updates took 31.03 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #42 --------------------\n",
      "Average Episodic Return: 158.46\n",
      "Average Actor Loss: -0.20642\n",
      "Average Critic Loss: 121.03906179743808\n",
      "Average KL Divergence: 0.032247654212997856\n",
      "Iteration took: 39.08 secs, of which rollout took 7.86 secs and gradient updates took 31.22 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #43 --------------------\n",
      "Average Episodic Return: 146.6\n",
      "Average Actor Loss: -0.20637\n",
      "Average Critic Loss: 118.43708684139737\n",
      "Average KL Divergence: 0.03185512993679664\n",
      "Iteration took: 38.72 secs, of which rollout took 7.72 secs and gradient updates took 30.99 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #44 --------------------\n",
      "Average Episodic Return: 131.52\n",
      "Average Actor Loss: -0.2062\n",
      "Average Critic Loss: 116.02212218301611\n",
      "Average KL Divergence: 0.03155125868832717\n",
      "Iteration took: 38.52 secs, of which rollout took 7.53 secs and gradient updates took 30.98 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #45 --------------------\n",
      "Average Episodic Return: 140.25\n",
      "Average Actor Loss: -0.20623\n",
      "Average Critic Loss: 113.68921380520302\n",
      "Average KL Divergence: 0.03119298055068031\n",
      "Iteration took: 39.25 secs, of which rollout took 7.91 secs and gradient updates took 31.33 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #46 --------------------\n",
      "Average Episodic Return: 157.88\n",
      "Average Actor Loss: -0.20628\n",
      "Average Critic Loss: 111.37973482113009\n",
      "Average KL Divergence: 0.030777459563004598\n",
      "Iteration took: 39.2 secs, of which rollout took 7.4 secs and gradient updates took 31.79 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #47 --------------------\n",
      "Average Episodic Return: 129.44\n",
      "Average Actor Loss: -0.20601\n",
      "Average Critic Loss: 109.29744478891054\n",
      "Average KL Divergence: 0.03219738391746909\n",
      "Iteration took: 39.19 secs, of which rollout took 7.53 secs and gradient updates took 31.66 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #48 --------------------\n",
      "Average Episodic Return: 88.75\n",
      "Average Actor Loss: -0.20646\n",
      "Average Critic Loss: 108.2218035666237\n",
      "Average KL Divergence: 0.03416898625869235\n",
      "Iteration took: 38.89 secs, of which rollout took 7.51 secs and gradient updates took 31.37 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #49 --------------------\n",
      "Average Episodic Return: 159.85\n",
      "Average Actor Loss: -0.20652\n",
      "Average Critic Loss: 106.63696039415943\n",
      "Average KL Divergence: 0.03375826890838399\n",
      "Iteration took: 38.42 secs, of which rollout took 7.19 secs and gradient updates took 31.23 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #50 --------------------\n",
      "Average Episodic Return: 134.65\n",
      "Average Actor Loss: -0.20604\n",
      "Average Critic Loss: 104.56870951134626\n",
      "Average KL Divergence: 0.033458013282617015\n",
      "Iteration took: 44.21 secs, of which rollout took 7.31 secs and gradient updates took 31.48 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 181.9\n",
      "Average Validation Duration: 634.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #51 --------------------\n",
      "Average Episodic Return: 155.03\n",
      "Average Actor Loss: -0.20606\n",
      "Average Critic Loss: 103.01345587387854\n",
      "Average KL Divergence: 0.033114524635266776\n",
      "Iteration took: 38.86 secs, of which rollout took 7.2 secs and gradient updates took 31.65 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #52 --------------------\n",
      "Average Episodic Return: 164.3\n",
      "Average Actor Loss: -0.2061\n",
      "Average Critic Loss: 101.11168739042004\n",
      "Average KL Divergence: 0.03271636508408377\n",
      "Iteration took: 39.66 secs, of which rollout took 8.09 secs and gradient updates took 31.56 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #53 --------------------\n",
      "Average Episodic Return: 138.59\n",
      "Average Actor Loss: -0.20616\n",
      "Average Critic Loss: 99.7067293692359\n",
      "Average KL Divergence: 0.03253111098114801\n",
      "Iteration took: 40.14 secs, of which rollout took 8.45 secs and gradient updates took 31.69 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #54 --------------------\n",
      "Average Episodic Return: 156.82\n",
      "Average Actor Loss: -0.20613\n",
      "Average Critic Loss: 97.8995757718873\n",
      "Average KL Divergence: 0.03216274254174534\n",
      "Iteration took: 39.19 secs, of which rollout took 7.64 secs and gradient updates took 31.54 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #55 --------------------\n",
      "Average Episodic Return: 143.06\n",
      "Average Actor Loss: -0.20615\n",
      "Average Critic Loss: 96.20435099516884\n",
      "Average KL Divergence: 0.03186840111382161\n",
      "Iteration took: 38.36 secs, of which rollout took 6.99 secs and gradient updates took 31.36 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #56 --------------------\n",
      "Average Episodic Return: 118.94\n",
      "Average Actor Loss: -0.20615\n",
      "Average Critic Loss: 95.18461216957037\n",
      "Average KL Divergence: 0.031576508586268814\n",
      "Iteration took: 39.37 secs, of which rollout took 7.79 secs and gradient updates took 31.58 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #57 --------------------\n",
      "Average Episodic Return: 155.99\n",
      "Average Actor Loss: -0.20614\n",
      "Average Critic Loss: 93.57951177652564\n",
      "Average KL Divergence: 0.031302994078129275\n",
      "Iteration took: 39.52 secs, of which rollout took 7.99 secs and gradient updates took 31.52 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #58 --------------------\n",
      "Average Episodic Return: 108.95\n",
      "Average Actor Loss: -0.20589\n",
      "Average Critic Loss: 92.64308124015635\n",
      "Average KL Divergence: 0.03126110458514508\n",
      "Iteration took: 38.82 secs, of which rollout took 7.48 secs and gradient updates took 31.33 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #59 --------------------\n",
      "Average Episodic Return: 138.29\n",
      "Average Actor Loss: -0.20596\n",
      "Average Critic Loss: 91.39913274819229\n",
      "Average KL Divergence: 0.030994340064835344\n",
      "Iteration took: 39.04 secs, of which rollout took 7.09 secs and gradient updates took 31.94 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #60 --------------------\n",
      "Average Episodic Return: 120.87\n",
      "Average Actor Loss: -0.2059\n",
      "Average Critic Loss: 90.77471174898692\n",
      "Average KL Divergence: 0.030774641316676187\n",
      "Iteration took: 45.69 secs, of which rollout took 7.23 secs and gradient updates took 32.47 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 129.45\n",
      "Average Validation Duration: 594.3 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #61 --------------------\n",
      "Average Episodic Return: 137.01\n",
      "Average Actor Loss: -0.2059\n",
      "Average Critic Loss: 89.90170559969557\n",
      "Average KL Divergence: 0.030535569792435734\n",
      "Iteration took: 38.2 secs, of which rollout took 6.52 secs and gradient updates took 31.67 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #62 --------------------\n",
      "Average Episodic Return: 158.93\n",
      "Average Actor Loss: -0.2059\n",
      "Average Critic Loss: 88.49938743319032\n",
      "Average KL Divergence: 0.03029889472668541\n",
      "Iteration took: 39.0 secs, of which rollout took 7.34 secs and gradient updates took 31.64 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #63 --------------------\n",
      "Average Episodic Return: 99.66\n",
      "Average Actor Loss: -0.20606\n",
      "Average Critic Loss: 87.4101202509597\n",
      "Average KL Divergence: 0.03357618061200219\n",
      "Iteration took: 38.73 secs, of which rollout took 7.02 secs and gradient updates took 31.7 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #64 --------------------\n",
      "Average Episodic Return: 161.8\n",
      "Average Actor Loss: -0.20586\n",
      "Average Critic Loss: 86.09179692709358\n",
      "Average KL Divergence: 0.03344603588531552\n",
      "Iteration took: 39.59 secs, of which rollout took 7.98 secs and gradient updates took 31.6 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #65 --------------------\n",
      "Average Episodic Return: 131.52\n",
      "Average Actor Loss: -0.20586\n",
      "Average Critic Loss: 84.81363349185055\n",
      "Average KL Divergence: 0.03321805093861556\n",
      "Iteration took: 39.0 secs, of which rollout took 7.66 secs and gradient updates took 31.34 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #66 --------------------\n",
      "Average Episodic Return: 148.2\n",
      "Average Actor Loss: -0.2058\n",
      "Average Critic Loss: 83.74473341448729\n",
      "Average KL Divergence: 0.03301107241736305\n",
      "Iteration took: 39.05 secs, of which rollout took 7.61 secs and gradient updates took 31.43 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #67 --------------------\n",
      "Average Episodic Return: 130.46\n",
      "Average Actor Loss: -0.20587\n",
      "Average Critic Loss: 83.46153713392562\n",
      "Average KL Divergence: 0.032742114410428125\n",
      "Iteration took: 38.65 secs, of which rollout took 6.6 secs and gradient updates took 32.03 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #68 --------------------\n",
      "Average Episodic Return: 109.55\n",
      "Average Actor Loss: -0.20588\n",
      "Average Critic Loss: 83.04787400793984\n",
      "Average KL Divergence: 0.03252056371854775\n",
      "Iteration took: 39.08 secs, of which rollout took 6.99 secs and gradient updates took 32.08 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #69 --------------------\n",
      "Average Episodic Return: 143.38\n",
      "Average Actor Loss: -0.2058\n",
      "Average Critic Loss: 81.88080136433565\n",
      "Average KL Divergence: 0.032285869377057054\n",
      "Iteration took: 38.93 secs, of which rollout took 7.07 secs and gradient updates took 31.86 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #70 --------------------\n",
      "Average Episodic Return: 161.52\n",
      "Average Actor Loss: -0.20582\n",
      "Average Critic Loss: 81.1251328426967\n",
      "Average KL Divergence: 0.03206290025730483\n",
      "Iteration took: 44.53 secs, of which rollout took 7.21 secs and gradient updates took 31.54 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 159.38\n",
      "Average Validation Duration: 569.4 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #71 --------------------\n",
      "Average Episodic Return: 156.21\n",
      "Average Actor Loss: -0.20584\n",
      "Average Critic Loss: 80.02619054467371\n",
      "Average KL Divergence: 0.03181661925480716\n",
      "Iteration took: 39.22 secs, of which rollout took 7.19 secs and gradient updates took 32.03 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #72 --------------------\n",
      "Average Episodic Return: 150.47\n",
      "Average Actor Loss: -0.20586\n",
      "Average Critic Loss: 79.13293629842703\n",
      "Average KL Divergence: 0.03187143622768114\n",
      "Iteration took: 39.26 secs, of which rollout took 7.18 secs and gradient updates took 32.07 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #73 --------------------\n",
      "Average Episodic Return: 152.94\n",
      "Average Actor Loss: -0.20578\n",
      "Average Critic Loss: 78.1456374729709\n",
      "Average KL Divergence: 0.031748616790826086\n",
      "Iteration took: 39.08 secs, of which rollout took 7.28 secs and gradient updates took 31.8 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #74 --------------------\n",
      "Average Episodic Return: 132.1\n",
      "Average Actor Loss: -0.20587\n",
      "Average Critic Loss: 77.91248163232348\n",
      "Average KL Divergence: 0.03156848445521373\n",
      "Iteration took: 38.29 secs, of which rollout took 6.6 secs and gradient updates took 31.68 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #75 --------------------\n",
      "Average Episodic Return: 49.23\n",
      "Average Actor Loss: -0.20575\n",
      "Average Critic Loss: 78.02885527692095\n",
      "Average KL Divergence: 0.031693779168942386\n",
      "Iteration took: 38.3 secs, of which rollout took 6.51 secs and gradient updates took 31.78 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #76 --------------------\n",
      "Average Episodic Return: 170.96\n",
      "Average Actor Loss: -0.20575\n",
      "Average Critic Loss: 77.22967589984437\n",
      "Average KL Divergence: 0.03147965405108406\n",
      "Iteration took: 39.37 secs, of which rollout took 7.45 secs and gradient updates took 31.91 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #77 --------------------\n",
      "Average Episodic Return: 79.95\n",
      "Average Actor Loss: -0.2057\n",
      "Average Critic Loss: 77.83047100488626\n",
      "Average KL Divergence: 0.03156945753962613\n",
      "Iteration took: 37.72 secs, of which rollout took 5.92 secs and gradient updates took 31.8 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #78 --------------------\n",
      "Average Episodic Return: 135.41\n",
      "Average Actor Loss: -0.20572\n",
      "Average Critic Loss: 77.79359808106331\n",
      "Average KL Divergence: 0.03137079393714198\n",
      "Iteration took: 38.8 secs, of which rollout took 6.08 secs and gradient updates took 32.7 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #79 --------------------\n",
      "Average Episodic Return: 135.99\n",
      "Average Actor Loss: -0.20566\n",
      "Average Critic Loss: 77.41075052556734\n",
      "Average KL Divergence: 0.03119355724888482\n",
      "Iteration took: 38.61 secs, of which rollout took 6.79 secs and gradient updates took 31.81 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #80 --------------------\n",
      "Average Episodic Return: 102.25\n",
      "Average Actor Loss: -0.20566\n",
      "Average Critic Loss: 77.46918779404191\n",
      "Average KL Divergence: 0.03099076317089342\n",
      "Iteration took: 42.71 secs, of which rollout took 5.86 secs and gradient updates took 32.42 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 87.82\n",
      "Average Validation Duration: 413.9 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #81 --------------------\n",
      "Average Episodic Return: 91.77\n",
      "Average Actor Loss: -0.20561\n",
      "Average Critic Loss: 78.01747710462044\n",
      "Average KL Divergence: 0.030786478653671304\n",
      "Iteration took: 37.19 secs, of which rollout took 5.24 secs and gradient updates took 31.95 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #82 --------------------\n",
      "Average Episodic Return: 90.18\n",
      "Average Actor Loss: -0.20557\n",
      "Average Critic Loss: 78.54639019430884\n",
      "Average KL Divergence: 0.03077498233885354\n",
      "Iteration took: 36.53 secs, of which rollout took 5.17 secs and gradient updates took 31.35 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #83 --------------------\n",
      "Average Episodic Return: 70.27\n",
      "Average Actor Loss: -0.20548\n",
      "Average Critic Loss: 78.81865453520673\n",
      "Average KL Divergence: 0.030641166793873346\n",
      "Iteration took: 35.96 secs, of which rollout took 4.51 secs and gradient updates took 31.44 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #84 --------------------\n",
      "Average Episodic Return: 101.13\n",
      "Average Actor Loss: -0.20548\n",
      "Average Critic Loss: 78.74645190851096\n",
      "Average KL Divergence: 0.03046706034582685\n",
      "Iteration took: 36.31 secs, of which rollout took 5.05 secs and gradient updates took 31.25 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #85 --------------------\n",
      "Average Episodic Return: 109.34\n",
      "Average Actor Loss: -0.20547\n",
      "Average Critic Loss: 78.61322501047059\n",
      "Average KL Divergence: 0.030334317405498593\n",
      "Iteration took: 36.0 secs, of which rollout took 4.78 secs and gradient updates took 31.2 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #86 --------------------\n",
      "Average Episodic Return: 95.08\n",
      "Average Actor Loss: -0.20548\n",
      "Average Critic Loss: 78.34312617354881\n",
      "Average KL Divergence: 0.03111768646971034\n",
      "Iteration took: 37.54 secs, of which rollout took 6.32 secs and gradient updates took 31.21 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #87 --------------------\n",
      "Average Episodic Return: 76.24\n",
      "Average Actor Loss: -0.20554\n",
      "Average Critic Loss: 78.55873965490379\n",
      "Average KL Divergence: 0.03469361933951334\n",
      "Iteration took: 36.96 secs, of which rollout took 5.83 secs and gradient updates took 31.12 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #88 --------------------\n",
      "Average Episodic Return: 94.65\n",
      "Average Actor Loss: -0.20553\n",
      "Average Critic Loss: 78.42967411278654\n",
      "Average KL Divergence: 0.034478771681124516\n",
      "Iteration took: 37.44 secs, of which rollout took 5.99 secs and gradient updates took 31.45 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #89 --------------------\n",
      "Average Episodic Return: 120.36\n",
      "Average Actor Loss: -0.20549\n",
      "Average Critic Loss: 78.56139202244749\n",
      "Average KL Divergence: 0.034378741615622425\n",
      "Iteration took: 36.62 secs, of which rollout took 5.66 secs and gradient updates took 30.96 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #90 --------------------\n",
      "Average Episodic Return: 104.67\n",
      "Average Actor Loss: -0.20546\n",
      "Average Critic Loss: 78.29668203787958\n",
      "Average KL Divergence: 0.034273035693073904\n",
      "Iteration took: 42.61 secs, of which rollout took 6.39 secs and gradient updates took 31.1 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 108.9\n",
      "Average Validation Duration: 440.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #91 --------------------\n",
      "Average Episodic Return: 107.03\n",
      "Average Actor Loss: -0.20543\n",
      "Average Critic Loss: 77.92052310782697\n",
      "Average KL Divergence: 0.034668480061155055\n",
      "Iteration took: 37.33 secs, of which rollout took 6.46 secs and gradient updates took 30.87 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #92 --------------------\n",
      "Average Episodic Return: 97.85\n",
      "Average Actor Loss: -0.20548\n",
      "Average Critic Loss: 77.86346161764457\n",
      "Average KL Divergence: 0.035563257730302056\n",
      "Iteration took: 37.01 secs, of which rollout took 5.91 secs and gradient updates took 31.09 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #93 --------------------\n",
      "Average Episodic Return: 77.85\n",
      "Average Actor Loss: -0.20552\n",
      "Average Critic Loss: 77.51960614211475\n",
      "Average KL Divergence: 0.035632798949361305\n",
      "Iteration took: 37.63 secs, of which rollout took 6.04 secs and gradient updates took 31.59 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #94 --------------------\n",
      "Average Episodic Return: 144.66\n",
      "Average Actor Loss: -0.20543\n",
      "Average Critic Loss: 76.89357484592617\n",
      "Average KL Divergence: 0.035541103993504235\n",
      "Iteration took: 38.23 secs, of which rollout took 7.08 secs and gradient updates took 31.14 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #95 --------------------\n",
      "Average Episodic Return: 137.65\n",
      "Average Actor Loss: -0.20539\n",
      "Average Critic Loss: 76.32627056230875\n",
      "Average KL Divergence: 0.03541738512672822\n",
      "Iteration took: 38.11 secs, of which rollout took 6.95 secs and gradient updates took 31.15 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #96 --------------------\n",
      "Average Episodic Return: 137.92\n",
      "Average Actor Loss: -0.20542\n",
      "Average Critic Loss: 75.88308052845757\n",
      "Average KL Divergence: 0.03518559482721131\n",
      "Iteration took: 38.05 secs, of which rollout took 6.51 secs and gradient updates took 31.53 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #97 --------------------\n",
      "Average Episodic Return: 87.93\n",
      "Average Actor Loss: -0.20541\n",
      "Average Critic Loss: 75.71305538281186\n",
      "Average KL Divergence: 0.03504942100596393\n",
      "Iteration took: 37.54 secs, of which rollout took 6.02 secs and gradient updates took 31.51 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #98 --------------------\n",
      "Average Episodic Return: 65.99\n",
      "Average Actor Loss: -0.20503\n",
      "Average Critic Loss: 75.78818191392621\n",
      "Average KL Divergence: 0.03605408918870987\n",
      "Iteration took: 37.08 secs, of which rollout took 5.82 secs and gradient updates took 31.25 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #99 --------------------\n",
      "Average Episodic Return: 142.36\n",
      "Average Actor Loss: -0.20503\n",
      "Average Critic Loss: 75.20298989996508\n",
      "Average KL Divergence: 0.03587015039873289\n",
      "Iteration took: 38.13 secs, of which rollout took 6.77 secs and gradient updates took 31.36 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #100 --------------------\n",
      "Average Episodic Return: 123.21\n",
      "Average Actor Loss: -0.20505\n",
      "Average Critic Loss: 74.8887846883817\n",
      "Average KL Divergence: 0.03568700181084321\n",
      "Iteration took: 42.96 secs, of which rollout took 6.12 secs and gradient updates took 31.37 secs\n",
      "Current actor learning rate: 0.002843456264033954\n",
      "Current critic learning rate: 0.002843456264033954\n",
      "Average Validation Return: 144.07\n",
      "Average Validation Duration: 547.6 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average adp Loss: 67.47485\n",
      "Iteration took: 20.74 secs, of which rollout took 0.77 secs and gradient updates took 19.97 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average adp Loss: 67.29264\n",
      "Iteration took: 21.22 secs, of which rollout took 0.75 secs and gradient updates took 20.46 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average adp Loss: 58.81617\n",
      "Iteration took: 21.74 secs, of which rollout took 0.74 secs and gradient updates took 20.99 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average adp Loss: 58.44414\n",
      "Iteration took: 21.44 secs, of which rollout took 0.76 secs and gradient updates took 20.68 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average adp Loss: 54.4415\n",
      "Iteration took: 21.86 secs, of which rollout took 0.75 secs and gradient updates took 21.11 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average adp Loss: 49.80692\n",
      "Iteration took: 21.83 secs, of which rollout took 0.75 secs and gradient updates took 21.07 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average adp Loss: 48.0605\n",
      "Iteration took: 21.86 secs, of which rollout took 0.71 secs and gradient updates took 21.15 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average adp Loss: 46.16266\n",
      "Iteration took: 22.2 secs, of which rollout took 0.79 secs and gradient updates took 21.41 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average adp Loss: 44.29391\n",
      "Iteration took: 22.44 secs, of which rollout took 0.76 secs and gradient updates took 21.68 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average adp Loss: 41.93339\n",
      "Iteration took: 22.39 secs, of which rollout took 0.76 secs and gradient updates took 21.63 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average adp Loss: 41.03702\n",
      "Iteration took: 22.39 secs, of which rollout took 0.7 secs and gradient updates took 21.68 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average adp Loss: 39.24978\n",
      "Iteration took: 22.61 secs, of which rollout took 0.72 secs and gradient updates took 21.88 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average adp Loss: 38.96755\n",
      "Iteration took: 22.35 secs, of which rollout took 0.75 secs and gradient updates took 21.59 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average adp Loss: 37.66861\n",
      "Iteration took: 22.96 secs, of which rollout took 0.78 secs and gradient updates took 22.18 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average adp Loss: 36.82783\n",
      "Iteration took: 22.55 secs, of which rollout took 0.75 secs and gradient updates took 21.79 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average adp Loss: 35.92327\n",
      "Iteration took: 22.92 secs, of which rollout took 0.73 secs and gradient updates took 22.19 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average adp Loss: 35.04686\n",
      "Iteration took: 22.72 secs, of which rollout took 0.74 secs and gradient updates took 21.97 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average adp Loss: 34.61967\n",
      "Iteration took: 22.41 secs, of which rollout took 0.76 secs and gradient updates took 21.65 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average adp Loss: 34.77857\n",
      "Iteration took: 22.64 secs, of which rollout took 0.78 secs and gradient updates took 21.85 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average adp Loss: 33.80614\n",
      "Iteration took: 22.99 secs, of which rollout took 0.73 secs and gradient updates took 22.25 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average adp Loss: 33.06026\n",
      "Iteration took: 22.55 secs, of which rollout took 0.74 secs and gradient updates took 21.8 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average adp Loss: 32.76232\n",
      "Iteration took: 22.71 secs, of which rollout took 0.72 secs and gradient updates took 21.98 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average adp Loss: 32.26594\n",
      "Iteration took: 22.59 secs, of which rollout took 0.75 secs and gradient updates took 21.84 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average adp Loss: 32.02642\n",
      "Iteration took: 22.54 secs, of which rollout took 0.7 secs and gradient updates took 21.83 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average adp Loss: 31.64977\n",
      "Iteration took: 22.6 secs, of which rollout took 0.73 secs and gradient updates took 21.86 secs\n",
      "Current adp learning rate: 0.0013946050760227774\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>adp_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>adp_maximum_base_validation_reward</td><td>▁</td></tr><tr><td>average_actor_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>average_adapt_loss</td><td>██▆▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>average_critic_loss</td><td>█▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>average_episode_rewards</td><td>▁▅▅▅▆▆▆▆▇▆▇▇▇▇▇▇███▇█▇█▇████▇█▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>critic_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iteration_compute</td><td>▅▅▅▅▅▅▆█▆█▇▆▆▆▆▆▆▅▆▆▆▇▆▆▆▅▅▅▇▅▅▇▁▁▁▁▂▁▂▁</td></tr><tr><td>maximum_base_validation_reward</td><td>▁</td></tr><tr><td>mean_validation_reward</td><td>▁</td></tr><tr><td>simulated_iterations</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████▁▂▂▂▃</td></tr><tr><td>val_durs</td><td>▁▆▆▇█▇▇▄▅▇</td></tr><tr><td>val_rews</td><td>▁▂▄▅█▆▇▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_learning_rate</td><td>0.00284</td></tr><tr><td>adp_learning_rate</td><td>0.00139</td></tr><tr><td>adp_maximum_base_validation_reward</td><td>230.90439</td></tr><tr><td>average_actor_loss</td><td>-0.20505</td></tr><tr><td>average_adapt_loss</td><td>31.64977</td></tr><tr><td>average_critic_loss</td><td>74.88878</td></tr><tr><td>average_episode_rewards</td><td>123.21326</td></tr><tr><td>critic_learning_rate</td><td>0.00284</td></tr><tr><td>iteration_compute</td><td>22.59974</td></tr><tr><td>maximum_base_validation_reward</td><td>222.15281</td></tr><tr><td>mean_validation_reward</td><td>114.81778</td></tr><tr><td>simulated_iterations</td><td>25</td></tr><tr><td>val_durs</td><td>547.6</td></tr><tr><td>val_rews</td><td>144.07402</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-1</strong> at: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/96hsmq2q' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/96hsmq2q</a><br/> View project at: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250116_124736-96hsmq2q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8fim0vr6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactor_lr: 0.0056686456129156765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadp_lr: 0.0006122672225982947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadp_num_steps: 233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_discount: 590\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_lr: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.6483368782972401\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_sgd_batches: 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_updates_per_iteration: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_steps: 288\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mohamedrostom62/ADLR/tum-adlr-ws25-16/wandb/run-20250116_140332-8fim0vr6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8fim0vr6' target=\"_blank\">different-sweep-2</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8fim0vr6' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8fim0vr6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -214.56\n",
      "Average Actor Loss: -0.14719\n",
      "Average Critic Loss: 778.9089217930789\n",
      "Average KL Divergence: 0.03680762658700036\n",
      "Iteration took: 5.89 secs, of which rollout took 2.12 secs and gradient updates took 3.77 secs\n",
      "Current actor learning rate: 0.0056686456129156765\n",
      "Current critic learning rate: 0.0056686456129156765\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -178.82\n",
      "Average Actor Loss: -0.18147\n",
      "Average Critic Loss: 647.2260967095838\n",
      "Average KL Divergence: 0.02438050328448588\n",
      "Iteration took: 6.15 secs, of which rollout took 2.19 secs and gradient updates took 3.95 secs\n",
      "Current actor learning rate: 0.005660430184491161\n",
      "Current critic learning rate: 0.005660430184491161\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -160.76\n",
      "Average Actor Loss: -0.19353\n",
      "Average Critic Loss: 503.80879632715715\n",
      "Average KL Divergence: 0.01961660792358007\n",
      "Iteration took: 6.1 secs, of which rollout took 2.07 secs and gradient updates took 4.03 secs\n",
      "Current actor learning rate: 0.005644023140478144\n",
      "Current critic learning rate: 0.005644023140478144\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -147.91\n",
      "Average Actor Loss: -0.19945\n",
      "Average Critic Loss: 414.62278227419654\n",
      "Average KL Divergence: 0.01722584589072137\n",
      "Iteration took: 6.16 secs, of which rollout took 2.21 secs and gradient updates took 3.95 secs\n",
      "Current actor learning rate: 0.005619483909432586\n",
      "Current critic learning rate: 0.005619483909432586\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -125.68\n",
      "Average Actor Loss: -0.20403\n",
      "Average Critic Loss: 348.6448132378409\n",
      "Average KL Divergence: 0.015970143886352626\n",
      "Iteration took: 6.13 secs, of which rollout took 2.3 secs and gradient updates took 3.82 secs\n",
      "Current actor learning rate: 0.005586907191117035\n",
      "Current critic learning rate: 0.005586907191117035\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -115.31\n",
      "Average Actor Loss: -0.20632\n",
      "Average Critic Loss: 310.71885785129774\n",
      "Average KL Divergence: 0.016054354273822083\n",
      "Iteration took: 6.34 secs, of which rollout took 2.42 secs and gradient updates took 3.92 secs\n",
      "Current actor learning rate: 0.0055464223563987955\n",
      "Current critic learning rate: 0.0055464223563987955\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -87.74\n",
      "Average Actor Loss: -0.207\n",
      "Average Critic Loss: 286.7178059414646\n",
      "Average KL Divergence: 0.015583192615860216\n",
      "Iteration took: 6.36 secs, of which rollout took 2.48 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 0.005498192596777937\n",
      "Current critic learning rate: 0.005498192596777937\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -75.6\n",
      "Average Actor Loss: -0.20856\n",
      "Average Critic Loss: 261.5691253003745\n",
      "Average KL Divergence: 0.015091383921604834\n",
      "Iteration took: 6.84 secs, of which rollout took 2.79 secs and gradient updates took 4.04 secs\n",
      "Current actor learning rate: 0.005442413831303378\n",
      "Current critic learning rate: 0.005442413831303378\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -70.94\n",
      "Average Actor Loss: -0.20926\n",
      "Average Critic Loss: 247.04578435882857\n",
      "Average KL Divergence: 0.015454101568591689\n",
      "Iteration took: 7.13 secs, of which rollout took 3.04 secs and gradient updates took 4.09 secs\n",
      "Current actor learning rate: 0.005379313381085367\n",
      "Current critic learning rate: 0.005379313381085367\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Return: -62.74\n",
      "Average Actor Loss: -0.21022\n",
      "Average Critic Loss: 234.21784340715556\n",
      "Average KL Divergence: 0.015252377582160768\n",
      "Iteration took: 9.08 secs, of which rollout took 3.78 secs and gradient updates took 3.98 secs\n",
      "Current actor learning rate: 0.005309148423940775\n",
      "Current critic learning rate: 0.005309148423940775\n",
      "Average Validation Return: -35.09\n",
      "Average Validation Duration: 209.9 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average Episodic Return: -19.21\n",
      "Average Actor Loss: -0.21024\n",
      "Average Critic Loss: 226.52582025482837\n",
      "Average KL Divergence: 0.015219046805797156\n",
      "Iteration took: 8.34 secs, of which rollout took 4.44 secs and gradient updates took 3.89 secs\n",
      "Current actor learning rate: 0.005232204243883663\n",
      "Current critic learning rate: 0.005232204243883663\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average Episodic Return: -32.63\n",
      "Average Actor Loss: -0.20988\n",
      "Average Critic Loss: 221.89722209400446\n",
      "Average KL Divergence: 0.015259282798466339\n",
      "Iteration took: 8.65 secs, of which rollout took 4.85 secs and gradient updates took 3.8 secs\n",
      "Current actor learning rate: 0.005148792292169576\n",
      "Current critic learning rate: 0.005148792292169576\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average Episodic Return: 2.06\n",
      "Average Actor Loss: -0.21005\n",
      "Average Critic Loss: 217.18718571015938\n",
      "Average KL Divergence: 0.015066471022186948\n",
      "Iteration took: 8.52 secs, of which rollout took 4.64 secs and gradient updates took 3.88 secs\n",
      "Current actor learning rate: 0.005059248078392714\n",
      "Current critic learning rate: 0.005059248078392714\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average Episodic Return: 31.27\n",
      "Average Actor Loss: -0.21018\n",
      "Average Critic Loss: 207.12130910868134\n",
      "Average KL Divergence: 0.014723535573308069\n",
      "Iteration took: 9.09 secs, of which rollout took 5.16 secs and gradient updates took 3.92 secs\n",
      "Current actor learning rate: 0.0049639289116983586\n",
      "Current critic learning rate: 0.0049639289116983586\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average Episodic Return: 27.54\n",
      "Average Actor Loss: -0.2096\n",
      "Average Critic Loss: 197.5830973263225\n",
      "Average KL Divergence: 0.014702499455122305\n",
      "Iteration took: 9.32 secs, of which rollout took 5.25 secs and gradient updates took 4.07 secs\n",
      "Current actor learning rate: 0.004863211513489986\n",
      "Current critic learning rate: 0.004863211513489986\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average Episodic Return: 31.26\n",
      "Average Actor Loss: -0.21\n",
      "Average Critic Loss: 186.86546962058733\n",
      "Average KL Divergence: 0.014435981949554968\n",
      "Iteration took: 8.96 secs, of which rollout took 5.15 secs and gradient updates took 3.79 secs\n",
      "Current actor learning rate: 0.0047574895240662905\n",
      "Current critic learning rate: 0.0047574895240662905\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average Episodic Return: 53.51\n",
      "Average Actor Loss: -0.21024\n",
      "Average Critic Loss: 176.6493055633114\n",
      "Average KL Divergence: 0.014092984671866251\n",
      "Iteration took: 9.44 secs, of which rollout took 5.46 secs and gradient updates took 3.97 secs\n",
      "Current actor learning rate: 0.004647170926406782\n",
      "Current critic learning rate: 0.004647170926406782\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average Episodic Return: 52.88\n",
      "Average Actor Loss: -0.21043\n",
      "Average Critic Loss: 167.47880480509286\n",
      "Average KL Divergence: 0.013723975767773072\n",
      "Iteration took: 9.1 secs, of which rollout took 5.26 secs and gradient updates took 3.83 secs\n",
      "Current actor learning rate: 0.004532675410828644\n",
      "Current critic learning rate: 0.004532675410828644\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average Episodic Return: 56.68\n",
      "Average Actor Loss: -0.21048\n",
      "Average Critic Loss: 159.17763984453504\n",
      "Average KL Divergence: 0.013463258128799704\n",
      "Iteration took: 9.24 secs, of which rollout took 5.51 secs and gradient updates took 3.72 secs\n",
      "Current actor learning rate: 0.004414431704459201\n",
      "Current critic learning rate: 0.004414431704459201\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average Episodic Return: 66.05\n",
      "Average Actor Loss: -0.21076\n",
      "Average Critic Loss: 151.7107834766127\n",
      "Average KL Divergence: 0.013236791119783455\n",
      "Iteration took: 11.15 secs, of which rollout took 5.47 secs and gradient updates took 3.71 secs\n",
      "Current actor learning rate: 0.004292874889408875\n",
      "Current critic learning rate: 0.004292874889408875\n",
      "Average Validation Return: 101.36\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average Episodic Return: 65.57\n",
      "Average Actor Loss: -0.21105\n",
      "Average Critic Loss: 144.9218935391709\n",
      "Average KL Divergence: 0.013079164879904997\n",
      "Iteration took: 9.36 secs, of which rollout took 5.38 secs and gradient updates took 3.97 secs\n",
      "Current actor learning rate: 0.004168443733194125\n",
      "Current critic learning rate: 0.004168443733194125\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average Episodic Return: 102.17\n",
      "Average Actor Loss: -0.211\n",
      "Average Critic Loss: 139.13950034022167\n",
      "Average KL Divergence: 0.012938654992431558\n",
      "Iteration took: 9.02 secs, of which rollout took 5.29 secs and gradient updates took 3.72 secs\n",
      "Current actor learning rate: 0.004041578054357782\n",
      "Current critic learning rate: 0.004041578054357782\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average Episodic Return: 104.32\n",
      "Average Actor Loss: -0.21123\n",
      "Average Critic Loss: 133.6692881312529\n",
      "Average KL Divergence: 0.012745343561654798\n",
      "Iteration took: 9.45 secs, of which rollout took 5.44 secs and gradient updates took 4.0 secs\n",
      "Current actor learning rate: 0.003912716145378258\n",
      "Current critic learning rate: 0.003912716145378258\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average Episodic Return: 99.86\n",
      "Average Actor Loss: -0.21061\n",
      "Average Critic Loss: 129.23120012794737\n",
      "Average KL Divergence: 0.012779993477288986\n",
      "Iteration took: 9.36 secs, of which rollout took 5.47 secs and gradient updates took 3.88 secs\n",
      "Current actor learning rate: 0.0037822922738656494\n",
      "Current critic learning rate: 0.0037822922738656494\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average Episodic Return: 143.75\n",
      "Average Actor Loss: -0.21078\n",
      "Average Critic Loss: 124.67149542693424\n",
      "Average KL Divergence: 0.012565176651235223\n",
      "Iteration took: 9.33 secs, of which rollout took 5.46 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 0.003650734281731192\n",
      "Current critic learning rate: 0.003650734281731192\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #26 --------------------\n",
      "Average Episodic Return: 115.5\n",
      "Average Actor Loss: -0.21092\n",
      "Average Critic Loss: 120.51370622279397\n",
      "Average KL Divergence: 0.012404798005097434\n",
      "Iteration took: 9.77 secs, of which rollout took 5.65 secs and gradient updates took 4.11 secs\n",
      "Current actor learning rate: 0.003518461300509047\n",
      "Current critic learning rate: 0.003518461300509047\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #27 --------------------\n",
      "Average Episodic Return: 97.43\n",
      "Average Actor Loss: -0.21098\n",
      "Average Critic Loss: 117.65232587473302\n",
      "Average KL Divergence: 0.012256545095817929\n",
      "Iteration took: 8.85 secs, of which rollout took 5.07 secs and gradient updates took 3.77 secs\n",
      "Current actor learning rate: 0.0033858815993304454\n",
      "Current critic learning rate: 0.0033858815993304454\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #28 --------------------\n",
      "Average Episodic Return: 106.5\n",
      "Average Actor Loss: -0.21113\n",
      "Average Critic Loss: 114.28023530127874\n",
      "Average KL Divergence: 0.012118990567284976\n",
      "Iteration took: 8.89 secs, of which rollout took 5.17 secs and gradient updates took 3.71 secs\n",
      "Current actor learning rate: 0.0032533905802262105\n",
      "Current critic learning rate: 0.0032533905802262105\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #29 --------------------\n",
      "Average Episodic Return: 95.78\n",
      "Average Actor Loss: -0.21132\n",
      "Average Critic Loss: 111.47830713092638\n",
      "Average KL Divergence: 0.012016110830928737\n",
      "Iteration took: 9.04 secs, of which rollout took 5.06 secs and gradient updates took 3.97 secs\n",
      "Current actor learning rate: 0.003121368933492393\n",
      "Current critic learning rate: 0.003121368933492393\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #30 --------------------\n",
      "Average Episodic Return: 140.64\n",
      "Average Actor Loss: -0.21141\n",
      "Average Critic Loss: 108.33284076995858\n",
      "Average KL Divergence: 0.011869819595572456\n",
      "Iteration took: 10.92 secs, of which rollout took 5.36 secs and gradient updates took 3.78 secs\n",
      "Current actor learning rate: 0.002990180963823872\n",
      "Current critic learning rate: 0.002990180963823872\n",
      "Average Validation Return: 120.54\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #31 --------------------\n",
      "Average Episodic Return: 120.0\n",
      "Average Actor Loss: -0.21148\n",
      "Average Critic Loss: 105.41586098557335\n",
      "Average KL Divergence: 0.011706081861972273\n",
      "Iteration took: 9.33 secs, of which rollout took 5.59 secs and gradient updates took 3.73 secs\n",
      "Current actor learning rate: 0.00286017309583153\n",
      "Current critic learning rate: 0.00286017309583153\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #32 --------------------\n",
      "Average Episodic Return: 126.54\n",
      "Average Actor Loss: -0.21154\n",
      "Average Critic Loss: 102.6555716712597\n",
      "Average KL Divergence: 0.011642310615195281\n",
      "Iteration took: 9.68 secs, of which rollout took 5.61 secs and gradient updates took 4.07 secs\n",
      "Current actor learning rate: 0.002731672565439099\n",
      "Current critic learning rate: 0.002731672565439099\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #33 --------------------\n",
      "Average Episodic Return: 131.55\n",
      "Average Actor Loss: -0.2117\n",
      "Average Critic Loss: 100.47643755518247\n",
      "Average KL Divergence: 0.011554764899830861\n",
      "Iteration took: 9.1 secs, of which rollout took 5.1 secs and gradient updates took 4.0 secs\n",
      "Current actor learning rate: 0.0026049863015346767\n",
      "Current critic learning rate: 0.0026049863015346767\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #34 --------------------\n",
      "Average Episodic Return: 130.35\n",
      "Average Actor Loss: -0.21181\n",
      "Average Critic Loss: 98.7985382055234\n",
      "Average KL Divergence: 0.011462587973739747\n",
      "Iteration took: 8.81 secs, of which rollout took 5.04 secs and gradient updates took 3.77 secs\n",
      "Current actor learning rate: 0.0024804000001569316\n",
      "Current critic learning rate: 0.0024804000001569316\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #35 --------------------\n",
      "Average Episodic Return: 125.45\n",
      "Average Actor Loss: -0.21195\n",
      "Average Critic Loss: 96.34641304076978\n",
      "Average KL Divergence: 0.011382572035144665\n",
      "Iteration took: 9.15 secs, of which rollout took 5.39 secs and gradient updates took 3.75 secs\n",
      "Current actor learning rate: 0.0023581773914535466\n",
      "Current critic learning rate: 0.0023581773914535466\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #36 --------------------\n",
      "Average Episodic Return: 140.63\n",
      "Average Actor Loss: -0.21208\n",
      "Average Critic Loss: 94.00834873519621\n",
      "Average KL Divergence: 0.011297552086051375\n",
      "Iteration took: 9.33 secs, of which rollout took 5.49 secs and gradient updates took 3.83 secs\n",
      "Current actor learning rate: 0.0022385596976841637\n",
      "Current critic learning rate: 0.0022385596976841637\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #37 --------------------\n",
      "Average Episodic Return: 121.12\n",
      "Average Actor Loss: -0.21222\n",
      "Average Critic Loss: 93.17177326288468\n",
      "Average KL Divergence: 0.011236326996189849\n",
      "Iteration took: 8.47 secs, of which rollout took 4.73 secs and gradient updates took 3.73 secs\n",
      "Current actor learning rate: 0.002121765278674555\n",
      "Current critic learning rate: 0.002121765278674555\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #38 --------------------\n",
      "Average Episodic Return: 135.71\n",
      "Average Actor Loss: -0.21229\n",
      "Average Critic Loss: 91.73090664489146\n",
      "Average KL Divergence: 0.011097914709011579\n",
      "Iteration took: 8.59 secs, of which rollout took 4.83 secs and gradient updates took 3.75 secs\n",
      "Current actor learning rate: 0.002007989459383311\n",
      "Current critic learning rate: 0.002007989459383311\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #39 --------------------\n",
      "Average Episodic Return: 158.88\n",
      "Average Actor Loss: -0.21235\n",
      "Average Critic Loss: 89.73193227463803\n",
      "Average KL Divergence: 0.010985766133689477\n",
      "Iteration took: 9.05 secs, of which rollout took 5.27 secs and gradient updates took 3.77 secs\n",
      "Current actor learning rate: 0.0018974045326346648\n",
      "Current critic learning rate: 0.0018974045326346648\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #40 --------------------\n",
      "Average Episodic Return: 149.82\n",
      "Average Actor Loss: -0.21245\n",
      "Average Critic Loss: 87.74382672766065\n",
      "Average KL Divergence: 0.010876421469429907\n",
      "Iteration took: 10.95 secs, of which rollout took 5.45 secs and gradient updates took 3.85 secs\n",
      "Current actor learning rate: 0.0017901599286161837\n",
      "Current critic learning rate: 0.0017901599286161837\n",
      "Average Validation Return: 147.85\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #41 --------------------\n",
      "Average Episodic Return: 138.43\n",
      "Average Actor Loss: -0.21249\n",
      "Average Critic Loss: 86.09676195824368\n",
      "Average KL Divergence: 0.010763103508467915\n",
      "Iteration took: 9.09 secs, of which rollout took 5.29 secs and gradient updates took 3.79 secs\n",
      "Current actor learning rate: 0.001686382541450028\n",
      "Current critic learning rate: 0.001686382541450028\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #42 --------------------\n",
      "Average Episodic Return: 131.05\n",
      "Average Actor Loss: -0.21255\n",
      "Average Critic Loss: 84.52150313662897\n",
      "Average KL Divergence: 0.010648165293407971\n",
      "Iteration took: 9.17 secs, of which rollout took 5.34 secs and gradient updates took 3.82 secs\n",
      "Current actor learning rate: 0.0015861772020305336\n",
      "Current critic learning rate: 0.0015861772020305336\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #43 --------------------\n",
      "Average Episodic Return: 131.91\n",
      "Average Actor Loss: -0.21262\n",
      "Average Critic Loss: 83.25918213873312\n",
      "Average KL Divergence: 0.010542256715020858\n",
      "Iteration took: 8.75 secs, of which rollout took 4.98 secs and gradient updates took 3.76 secs\n",
      "Current actor learning rate: 0.001489627285385197\n",
      "Current critic learning rate: 0.001489627285385197\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #44 --------------------\n",
      "Average Episodic Return: 145.51\n",
      "Average Actor Loss: -0.21269\n",
      "Average Critic Loss: 81.8495758479152\n",
      "Average KL Divergence: 0.010439702180851712\n",
      "Iteration took: 9.07 secs, of which rollout took 5.1 secs and gradient updates took 3.97 secs\n",
      "Current actor learning rate: 0.0013967954400640904\n",
      "Current critic learning rate: 0.0013967954400640904\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #45 --------------------\n",
      "Average Episodic Return: 146.75\n",
      "Average Actor Loss: -0.21277\n",
      "Average Critic Loss: 80.51741781167405\n",
      "Average KL Divergence: 0.01035762152150829\n",
      "Iteration took: 8.9 secs, of which rollout took 5.13 secs and gradient updates took 3.76 secs\n",
      "Current actor learning rate: 0.001307724426494786\n",
      "Current critic learning rate: 0.001307724426494786\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #46 --------------------\n",
      "Average Episodic Return: 153.99\n",
      "Average Actor Loss: -0.21287\n",
      "Average Critic Loss: 78.9846755099943\n",
      "Average KL Divergence: 0.010260113504180768\n",
      "Iteration took: 8.85 secs, of which rollout took 5.13 secs and gradient updates took 3.71 secs\n",
      "Current actor learning rate: 0.0012224380508538218\n",
      "Current critic learning rate: 0.0012224380508538218\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #47 --------------------\n",
      "Average Episodic Return: 159.71\n",
      "Average Actor Loss: -0.21296\n",
      "Average Critic Loss: 77.82650078186599\n",
      "Average KL Divergence: 0.010166169811706675\n",
      "Iteration took: 8.73 secs, of which rollout took 4.92 secs and gradient updates took 3.81 secs\n",
      "Current actor learning rate: 0.0011409421807969003\n",
      "Current critic learning rate: 0.0011409421807969003\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #48 --------------------\n",
      "Average Episodic Return: 167.01\n",
      "Average Actor Loss: -0.21299\n",
      "Average Critic Loss: 76.84850370758303\n",
      "Average KL Divergence: 0.010050430503118638\n",
      "Iteration took: 9.12 secs, of which rollout took 5.27 secs and gradient updates took 3.83 secs\n",
      "Current actor learning rate: 0.0010632258293513143\n",
      "Current critic learning rate: 0.0010632258293513143\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #49 --------------------\n",
      "Average Episodic Return: 163.9\n",
      "Average Actor Loss: -0.21311\n",
      "Average Critic Loss: 75.89804247271073\n",
      "Average KL Divergence: 0.009977050433498873\n",
      "Iteration took: 8.77 secs, of which rollout took 4.95 secs and gradient updates took 3.81 secs\n",
      "Current actor learning rate: 0.0009892622933964403\n",
      "Current critic learning rate: 0.0009892622933964403\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #50 --------------------\n",
      "Average Episodic Return: 170.02\n",
      "Average Actor Loss: -0.21319\n",
      "Average Critic Loss: 74.58813462938139\n",
      "Average KL Divergence: 0.00987533662232449\n",
      "Iteration took: 10.34 secs, of which rollout took 4.96 secs and gradient updates took 3.79 secs\n",
      "Current actor learning rate: 0.0009190103334306061\n",
      "Current critic learning rate: 0.0009190103334306061\n",
      "Average Validation Return: 168.74\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #51 --------------------\n",
      "Average Episodic Return: 168.81\n",
      "Average Actor Loss: -0.21328\n",
      "Average Critic Loss: 73.5675407008286\n",
      "Average KL Divergence: 0.009781188614702755\n",
      "Iteration took: 8.99 secs, of which rollout took 5.09 secs and gradient updates took 3.89 secs\n",
      "Current actor learning rate: 0.0008524153817327361\n",
      "Current critic learning rate: 0.0008524153817327361\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #52 --------------------\n",
      "Average Episodic Return: 169.18\n",
      "Average Actor Loss: -0.21335\n",
      "Average Critic Loss: 72.52473572315994\n",
      "Average KL Divergence: 0.009670853493303391\n",
      "Iteration took: 9.04 secs, of which rollout took 4.96 secs and gradient updates took 4.07 secs\n",
      "Current actor learning rate: 0.000789410766561186\n",
      "Current critic learning rate: 0.000789410766561186\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #53 --------------------\n",
      "Average Episodic Return: 177.08\n",
      "Average Actor Loss: -0.21337\n",
      "Average Critic Loss: 71.29252048716133\n",
      "Average KL Divergence: 0.009563891240698646\n",
      "Iteration took: 8.68 secs, of which rollout took 4.89 secs and gradient updates took 3.78 secs\n",
      "Current actor learning rate: 0.0007299189406754155\n",
      "Current critic learning rate: 0.0007299189406754155\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #54 --------------------\n",
      "Average Episodic Return: 168.72\n",
      "Average Actor Loss: -0.21345\n",
      "Average Critic Loss: 70.1025405635409\n",
      "Average KL Divergence: 0.00945449229056364\n",
      "Iteration took: 8.64 secs, of which rollout took 4.78 secs and gradient updates took 3.85 secs\n",
      "Current actor learning rate: 0.0006738527032032459\n",
      "Current critic learning rate: 0.0006738527032032459\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #55 --------------------\n",
      "Average Episodic Return: 165.96\n",
      "Average Actor Loss: -0.21351\n",
      "Average Critic Loss: 69.51246366123745\n",
      "Average KL Divergence: 0.009382502566064497\n",
      "Iteration took: 8.57 secs, of which rollout took 4.69 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 0.0006211164046916875\n",
      "Current critic learning rate: 0.0006211164046916875\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #56 --------------------\n",
      "Average Episodic Return: 166.14\n",
      "Average Actor Loss: -0.21357\n",
      "Average Critic Loss: 68.36334438825392\n",
      "Average KL Divergence: 0.009283515322434767\n",
      "Iteration took: 8.84 secs, of which rollout took 4.95 secs and gradient updates took 3.88 secs\n",
      "Current actor learning rate: 0.0005716071260568428\n",
      "Current critic learning rate: 0.0005716071260568428\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #57 --------------------\n",
      "Average Episodic Return: 170.23\n",
      "Average Actor Loss: -0.21365\n",
      "Average Critic Loss: 67.26679195847373\n",
      "Average KL Divergence: 0.009185167044028246\n",
      "Iteration took: 8.78 secs, of which rollout took 4.91 secs and gradient updates took 3.86 secs\n",
      "Current actor learning rate: 0.0005252158230725193\n",
      "Current critic learning rate: 0.0005252158230725193\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #58 --------------------\n",
      "Average Episodic Return: 170.74\n",
      "Average Actor Loss: -0.21369\n",
      "Average Critic Loss: 66.43158032838258\n",
      "Average KL Divergence: 0.00908994779135548\n",
      "Iteration took: 8.91 secs, of which rollout took 4.93 secs and gradient updates took 3.97 secs\n",
      "Current actor learning rate: 0.0004818284289926156\n",
      "Current critic learning rate: 0.0004818284289926156\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #59 --------------------\n",
      "Average Episodic Return: 177.19\n",
      "Average Actor Loss: -0.21377\n",
      "Average Critic Loss: 65.406523835525\n",
      "Average KL Divergence: 0.008991159439871232\n",
      "Iteration took: 9.26 secs, of which rollout took 5.22 secs and gradient updates took 4.03 secs\n",
      "Current actor learning rate: 0.00044132690887439574\n",
      "Current critic learning rate: 0.00044132690887439574\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #60 --------------------\n",
      "Average Episodic Return: 160.9\n",
      "Average Actor Loss: -0.21382\n",
      "Average Critic Loss: 64.8392020366593\n",
      "Average KL Divergence: 0.008910177536611837\n",
      "Iteration took: 10.38 secs, of which rollout took 4.84 secs and gradient updates took 3.89 secs\n",
      "Current actor learning rate: 0.0004035902601445561\n",
      "Current critic learning rate: 0.0004035902601445561\n",
      "Average Validation Return: 151.09\n",
      "Average Validation Duration: 272.8 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #61 --------------------\n",
      "Average Episodic Return: 175.7\n",
      "Average Actor Loss: -0.21386\n",
      "Average Critic Loss: 64.1734199239341\n",
      "Average KL Divergence: 0.008823901123010935\n",
      "Iteration took: 8.7 secs, of which rollout took 4.67 secs and gradient updates took 4.02 secs\n",
      "Current actor learning rate: 0.0003684954549145947\n",
      "Current critic learning rate: 0.0003684954549145947\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #62 --------------------\n",
      "Average Episodic Return: 176.08\n",
      "Average Actor Loss: -0.21391\n",
      "Average Critic Loss: 63.231638110708616\n",
      "Average KL Divergence: 0.008750658607456684\n",
      "Iteration took: 8.78 secs, of which rollout took 4.92 secs and gradient updates took 3.85 secs\n",
      "Current actor learning rate: 0.0003359183204946088\n",
      "Current critic learning rate: 0.0003359183204946088\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #63 --------------------\n",
      "Average Episodic Return: 174.43\n",
      "Average Actor Loss: -0.21395\n",
      "Average Critic Loss: 62.461309911757866\n",
      "Average KL Divergence: 0.008659545259331743\n",
      "Iteration took: 8.65 secs, of which rollout took 4.73 secs and gradient updates took 3.92 secs\n",
      "Current actor learning rate: 0.00030573435546465846\n",
      "Current critic learning rate: 0.00030573435546465846\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #64 --------------------\n",
      "Average Episodic Return: 164.61\n",
      "Average Actor Loss: -0.214\n",
      "Average Critic Loss: 62.14200051546959\n",
      "Average KL Divergence: 0.008574966153244474\n",
      "Iteration took: 8.41 secs, of which rollout took 4.54 secs and gradient updates took 3.86 secs\n",
      "Current actor learning rate: 0.00027781947953092875\n",
      "Current critic learning rate: 0.00027781947953092875\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #65 --------------------\n",
      "Average Episodic Return: 178.44\n",
      "Average Actor Loss: -0.21402\n",
      "Average Critic Loss: 61.32817476669842\n",
      "Average KL Divergence: 0.008476232531514288\n",
      "Iteration took: 8.62 secs, of which rollout took 4.78 secs and gradient updates took 3.83 secs\n",
      "Current actor learning rate: 0.00025205071621211796\n",
      "Current critic learning rate: 0.00025205071621211796\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #66 --------------------\n",
      "Average Episodic Return: 172.31\n",
      "Average Actor Loss: -0.21409\n",
      "Average Critic Loss: 60.488875855814726\n",
      "Average KL Divergence: 0.00840092302096745\n",
      "Iteration took: 8.76 secs, of which rollout took 4.91 secs and gradient updates took 3.85 secs\n",
      "Current actor learning rate: 0.00022830680816315034\n",
      "Current critic learning rate: 0.00022830680816315034\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #67 --------------------\n",
      "Average Episodic Return: 172.28\n",
      "Average Actor Loss: -0.2141\n",
      "Average Critic Loss: 60.060228917049756\n",
      "Average KL Divergence: 0.008309533752980585\n",
      "Iteration took: 8.67 secs, of which rollout took 4.79 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 0.00020646876564319684\n",
      "Current critic learning rate: 0.00020646876564319684\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #68 --------------------\n",
      "Average Episodic Return: 165.46\n",
      "Average Actor Loss: -0.21413\n",
      "Average Critic Loss: 59.544981390899345\n",
      "Average KL Divergence: 0.008235022304262079\n",
      "Iteration took: 8.49 secs, of which rollout took 4.58 secs and gradient updates took 3.9 secs\n",
      "Current actor learning rate: 0.0001864203492691473\n",
      "Current critic learning rate: 0.0001864203492691473\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #69 --------------------\n",
      "Average Episodic Return: 167.52\n",
      "Average Actor Loss: -0.21417\n",
      "Average Critic Loss: 59.05666623967996\n",
      "Average KL Divergence: 0.008148605340690278\n",
      "Iteration took: 8.71 secs, of which rollout took 4.74 secs and gradient updates took 3.96 secs\n",
      "Current actor learning rate: 0.00016804848876146323\n",
      "Current critic learning rate: 0.00016804848876146323\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #70 --------------------\n",
      "Average Episodic Return: 173.77\n",
      "Average Actor Loss: -0.21422\n",
      "Average Critic Loss: 58.647921530648574\n",
      "Average KL Divergence: 0.008083027811688261\n",
      "Iteration took: 10.21 secs, of which rollout took 4.81 secs and gradient updates took 3.85 secs\n",
      "Current actor learning rate: 0.00015124363988531692\n",
      "Current critic learning rate: 0.00015124363988531692\n",
      "Average Validation Return: 175.79\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #71 --------------------\n",
      "Average Episodic Return: 168.65\n",
      "Average Actor Loss: -0.21425\n",
      "Average Critic Loss: 58.43209262400652\n",
      "Average KL Divergence: 0.008004889794810625\n",
      "Iteration took: 8.37 secs, of which rollout took 4.44 secs and gradient updates took 3.93 secs\n",
      "Current actor learning rate: 0.000135900082215792\n",
      "Current critic learning rate: 0.000135900082215792\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #72 --------------------\n",
      "Average Episodic Return: 169.99\n",
      "Average Actor Loss: -0.2143\n",
      "Average Critic Loss: 57.84883142448426\n",
      "Average KL Divergence: 0.00792239912756995\n",
      "Iteration took: 8.7 secs, of which rollout took 4.84 secs and gradient updates took 3.86 secs\n",
      "Current actor learning rate: 0.00012191616071242789\n",
      "Current critic learning rate: 0.00012191616071242789\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #73 --------------------\n",
      "Average Episodic Return: 172.69\n",
      "Average Actor Loss: -0.21431\n",
      "Average Critic Loss: 57.27563263287632\n",
      "Average KL Divergence: 0.007841820761128722\n",
      "Iteration took: 8.61 secs, of which rollout took 4.75 secs and gradient updates took 3.86 secs\n",
      "Current actor learning rate: 0.00010919447437721803\n",
      "Current critic learning rate: 0.00010919447437721803\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #74 --------------------\n",
      "Average Episodic Return: 175.8\n",
      "Average Actor Loss: -0.21437\n",
      "Average Critic Loss: 56.92745490379366\n",
      "Average KL Divergence: 0.007768834741173745\n",
      "Iteration took: 8.48 secs, of which rollout took 4.59 secs and gradient updates took 3.89 secs\n",
      "Current actor learning rate: 9.76420154938312e-05\n",
      "Current critic learning rate: 9.76420154938312e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #75 --------------------\n",
      "Average Episodic Return: 171.39\n",
      "Average Actor Loss: -0.21438\n",
      "Average Critic Loss: 56.515893851334475\n",
      "Average KL Divergence: 0.007676475743353472\n",
      "Iteration took: 8.62 secs, of which rollout took 4.74 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 8.717026310753627e-05\n",
      "Current critic learning rate: 8.717026310753627e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #76 --------------------\n",
      "Average Episodic Return: 172.1\n",
      "Average Actor Loss: -0.21442\n",
      "Average Critic Loss: 56.0911006768783\n",
      "Average KL Divergence: 0.007597870087698579\n",
      "Iteration took: 8.64 secs, of which rollout took 4.73 secs and gradient updates took 3.89 secs\n",
      "Current actor learning rate: 7.769523450889102e-05\n",
      "Current critic learning rate: 7.769523450889102e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #77 --------------------\n",
      "Average Episodic Return: 177.18\n",
      "Average Actor Loss: -0.21445\n",
      "Average Critic Loss: 55.43700664869115\n",
      "Average KL Divergence: 0.007508565432514711\n",
      "Iteration took: 8.76 secs, of which rollout took 4.72 secs and gradient updates took 4.03 secs\n",
      "Current actor learning rate: 6.913749853399868e-05\n",
      "Current critic learning rate: 6.913749853399868e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #78 --------------------\n",
      "Average Episodic Return: 171.14\n",
      "Average Actor Loss: -0.21447\n",
      "Average Critic Loss: 54.984506797528645\n",
      "Average KL Divergence: 0.00743131838749617\n",
      "Iteration took: 8.66 secs, of which rollout took 4.79 secs and gradient updates took 3.86 secs\n",
      "Current actor learning rate: 6.142215449469739e-05\n",
      "Current critic learning rate: 6.142215449469739e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #79 --------------------\n",
      "Average Episodic Return: 172.38\n",
      "Average Actor Loss: -0.21449\n",
      "Average Critic Loss: 54.49051731513599\n",
      "Average KL Divergence: 0.007356315233826262\n",
      "Iteration took: 8.82 secs, of which rollout took 4.91 secs and gradient updates took 3.9 secs\n",
      "Current actor learning rate: 5.447878050834029e-05\n",
      "Current critic learning rate: 5.447878050834029e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #80 --------------------\n",
      "Average Episodic Return: 184.13\n",
      "Average Actor Loss: -0.2145\n",
      "Average Critic Loss: 53.92340516038535\n",
      "Average KL Divergence: 0.007275407756160502\n",
      "Iteration took: 10.15 secs, of which rollout took 4.79 secs and gradient updates took 3.94 secs\n",
      "Current actor learning rate: 4.824135491390712e-05\n",
      "Current critic learning rate: 4.824135491390712e-05\n",
      "Average Validation Return: 153.18\n",
      "Average Validation Duration: 252.4 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #81 --------------------\n",
      "Average Episodic Return: 176.05\n",
      "Average Actor Loss: -0.21452\n",
      "Average Critic Loss: 53.45921784739923\n",
      "Average KL Divergence: 0.007196820148733371\n",
      "Iteration took: 8.55 secs, of which rollout took 4.66 secs and gradient updates took 3.88 secs\n",
      "Current actor learning rate: 4.2648154344178764e-05\n",
      "Current critic learning rate: 4.2648154344178764e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #82 --------------------\n",
      "Average Episodic Return: 162.63\n",
      "Average Actor Loss: -0.21452\n",
      "Average Critic Loss: 53.62647894553753\n",
      "Average KL Divergence: 0.007115636219297292\n",
      "Iteration took: 8.25 secs, of which rollout took 4.41 secs and gradient updates took 3.84 secs\n",
      "Current actor learning rate: 3.7641631877688215e-05\n",
      "Current critic learning rate: 3.7641631877688215e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #83 --------------------\n",
      "Average Episodic Return: 180.2\n",
      "Average Actor Loss: -0.21454\n",
      "Average Critic Loss: 53.0631435860392\n",
      "Average KL Divergence: 0.00704606450089309\n",
      "Iteration took: 8.59 secs, of which rollout took 4.73 secs and gradient updates took 3.85 secs\n",
      "Current actor learning rate: 3.3168278524107876e-05\n",
      "Current critic learning rate: 3.3168278524107876e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #84 --------------------\n",
      "Average Episodic Return: 171.68\n",
      "Average Actor Loss: -0.21456\n",
      "Average Critic Loss: 52.698972798864006\n",
      "Average KL Divergence: 0.006966149041764721\n",
      "Iteration took: 8.44 secs, of which rollout took 4.57 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 2.9178471107439827e-05\n",
      "Current critic learning rate: 2.9178471107439827e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #85 --------------------\n",
      "Average Episodic Return: 161.55\n",
      "Average Actor Loss: -0.21456\n",
      "Average Critic Loss: 52.60984414519723\n",
      "Average KL Divergence: 0.0068914923656890805\n",
      "Iteration took: 8.55 secs, of which rollout took 4.58 secs and gradient updates took 3.96 secs\n",
      "Current actor learning rate: 2.5626309407403674e-05\n",
      "Current critic learning rate: 2.5626309407403674e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #86 --------------------\n",
      "Average Episodic Return: 176.77\n",
      "Average Actor Loss: -0.21458\n",
      "Average Critic Loss: 52.070813676718615\n",
      "Average KL Divergence: 0.006822281098679148\n",
      "Iteration took: 9.05 secs, of which rollout took 5.13 secs and gradient updates took 3.91 secs\n",
      "Current actor learning rate: 2.246944520504235e-05\n",
      "Current critic learning rate: 2.246944520504235e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #87 --------------------\n",
      "Average Episodic Return: 171.09\n",
      "Average Actor Loss: -0.21458\n",
      "Average Critic Loss: 51.54558948552741\n",
      "Average KL Divergence: 0.006750190927185573\n",
      "Iteration took: 8.88 secs, of which rollout took 5.09 secs and gradient updates took 3.78 secs\n",
      "Current actor learning rate: 1.9668905657747217e-05\n",
      "Current critic learning rate: 1.9668905657747217e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #88 --------------------\n",
      "Average Episodic Return: 172.87\n",
      "Average Actor Loss: -0.21459\n",
      "Average Critic Loss: 51.11684335764425\n",
      "Average KL Divergence: 0.006676477920254045\n",
      "Iteration took: 8.59 secs, of which rollout took 4.81 secs and gradient updates took 3.78 secs\n",
      "Current actor learning rate: 1.7188913205248656e-05\n",
      "Current critic learning rate: 1.7188913205248656e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #89 --------------------\n",
      "Average Episodic Return: 176.19\n",
      "Average Actor Loss: -0.2146\n",
      "Average Critic Loss: 50.60555253708807\n",
      "Average KL Divergence: 0.006603756919008153\n",
      "Iteration took: 8.73 secs, of which rollout took 4.93 secs and gradient updates took 3.8 secs\n",
      "Current actor learning rate: 1.4996703984869118e-05\n",
      "Current critic learning rate: 1.4996703984869118e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #90 --------------------\n",
      "Average Episodic Return: 170.83\n",
      "Average Actor Loss: -0.21462\n",
      "Average Critic Loss: 50.4558686899731\n",
      "Average KL Divergence: 0.006539245679994086\n",
      "Iteration took: 10.36 secs, of which rollout took 4.75 secs and gradient updates took 3.96 secs\n",
      "Current actor learning rate: 1.3062346514357014e-05\n",
      "Current critic learning rate: 1.3062346514357014e-05\n",
      "Average Validation Return: 176.28\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #91 --------------------\n",
      "Average Episodic Return: 178.96\n",
      "Average Actor Loss: -0.21462\n",
      "Average Critic Loss: 49.96372366202103\n",
      "Average KL Divergence: 0.006468463726698632\n",
      "Iteration took: 8.99 secs, of which rollout took 4.9 secs and gradient updates took 4.08 secs\n",
      "Current actor learning rate: 1.1358562186397404e-05\n",
      "Current critic learning rate: 1.1358562186397404e-05\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #92 --------------------\n",
      "Average Episodic Return: 174.62\n",
      "Average Actor Loss: -0.21462\n",
      "Average Critic Loss: 49.57887315916102\n",
      "Average KL Divergence: 0.0063995896391815495\n",
      "Iteration took: 8.56 secs, of which rollout took 4.73 secs and gradient updates took 3.82 secs\n",
      "Current actor learning rate: 9.860548912539195e-06\n",
      "Current critic learning rate: 9.860548912539195e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #93 --------------------\n",
      "Average Episodic Return: 176.66\n",
      "Average Actor Loss: -0.21463\n",
      "Average Critic Loss: 49.20568207279462\n",
      "Average KL Divergence: 0.006332043893467437\n",
      "Iteration took: 8.64 secs, of which rollout took 4.76 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 8.54580905753397e-06\n",
      "Current critic learning rate: 8.54580905753397e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #94 --------------------\n",
      "Average Episodic Return: 180.05\n",
      "Average Actor Loss: -0.21463\n",
      "Average Critic Loss: 48.75309374053775\n",
      "Average KL Divergence: 0.006265104023579648\n",
      "Iteration took: 8.49 secs, of which rollout took 4.65 secs and gradient updates took 3.84 secs\n",
      "Current actor learning rate: 7.393982619344608e-06\n",
      "Current critic learning rate: 7.393982619344608e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #95 --------------------\n",
      "Average Episodic Return: 172.63\n",
      "Average Actor Loss: -0.21463\n",
      "Average Critic Loss: 48.42716590622547\n",
      "Average KL Divergence: 0.006199611178114123\n",
      "Iteration took: 8.51 secs, of which rollout took 4.67 secs and gradient updates took 3.84 secs\n",
      "Current actor learning rate: 6.3866864364194004e-06\n",
      "Current critic learning rate: 6.3866864364194004e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #96 --------------------\n",
      "Average Episodic Return: 173.76\n",
      "Average Actor Loss: -0.21463\n",
      "Average Critic Loss: 48.199762354418056\n",
      "Average KL Divergence: 0.006136197995727066\n",
      "Iteration took: 8.48 secs, of which rollout took 4.57 secs and gradient updates took 3.9 secs\n",
      "Current actor learning rate: 5.5073600429993385e-06\n",
      "Current critic learning rate: 5.5073600429993385e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #97 --------------------\n",
      "Average Episodic Return: 175.07\n",
      "Average Actor Loss: -0.21463\n",
      "Average Critic Loss: 48.000394775829534\n",
      "Average KL Divergence: 0.006073125894387997\n",
      "Iteration took: 8.7 secs, of which rollout took 4.83 secs and gradient updates took 3.86 secs\n",
      "Current actor learning rate: 4.741118645712474e-06\n",
      "Current critic learning rate: 4.741118645712474e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #98 --------------------\n",
      "Average Episodic Return: 173.09\n",
      "Average Actor Loss: -0.21462\n",
      "Average Critic Loss: 47.95218090210317\n",
      "Average KL Divergence: 0.00601137820756342\n",
      "Iteration took: 8.72 secs, of which rollout took 4.8 secs and gradient updates took 3.91 secs\n",
      "Current actor learning rate: 4.074613560735503e-06\n",
      "Current critic learning rate: 4.074613560735503e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #99 --------------------\n",
      "Average Episodic Return: 168.17\n",
      "Average Actor Loss: -0.21462\n",
      "Average Critic Loss: 47.65127784451392\n",
      "Average KL Divergence: 0.005951390645024495\n",
      "Iteration took: 8.64 secs, of which rollout took 4.7 secs and gradient updates took 3.94 secs\n",
      "Current actor learning rate: 3.4959003303701708e-06\n",
      "Current critic learning rate: 3.4959003303701708e-06\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #100 --------------------\n",
      "Average Episodic Return: 178.07\n",
      "Average Actor Loss: -0.21462\n",
      "Average Critic Loss: 47.23236773559081\n",
      "Average KL Divergence: 0.0058919336422476295\n",
      "Iteration took: 10.1 secs, of which rollout took 4.63 secs and gradient updates took 3.87 secs\n",
      "Current actor learning rate: 2.99431463079532e-06\n",
      "Current critic learning rate: 2.99431463079532e-06\n",
      "Average Validation Return: 177.8\n",
      "Average Validation Duration: 289.0 secs\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average adp Loss: 147.33034\n",
      "Iteration took: 4.76 secs, of which rollout took 3.28 secs and gradient updates took 1.47 secs\n",
      "Current adp learning rate: 0.0006122672225982947\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average adp Loss: 131.05411\n",
      "Iteration took: 4.93 secs, of which rollout took 3.33 secs and gradient updates took 1.59 secs\n",
      "Current adp learning rate: 0.0006112716661387853\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average adp Loss: 123.8358\n",
      "Iteration took: 4.75 secs, of which rollout took 3.19 secs and gradient updates took 1.55 secs\n",
      "Current adp learning rate: 0.0006092837908017486\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average adp Loss: 117.33779\n",
      "Iteration took: 4.82 secs, of which rollout took 3.3 secs and gradient updates took 1.52 secs\n",
      "Current adp learning rate: 0.0006063116747490571\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average adp Loss: 113.32969\n",
      "Iteration took: 4.79 secs, of which rollout took 3.32 secs and gradient updates took 1.47 secs\n",
      "Current adp learning rate: 0.0006023681841815835\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average adp Loss: 112.80874\n",
      "Iteration took: 4.72 secs, of which rollout took 3.23 secs and gradient updates took 1.49 secs\n",
      "Current adp learning rate: 0.0005974708818711641\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average adp Loss: 111.17155\n",
      "Iteration took: 4.75 secs, of which rollout took 3.27 secs and gradient updates took 1.47 secs\n",
      "Current adp learning rate: 0.0005916418976577869\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average adp Loss: 110.25768\n",
      "Iteration took: 4.73 secs, of which rollout took 3.25 secs and gradient updates took 1.47 secs\n",
      "Current adp learning rate: 0.0005849077622372918\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average adp Loss: 107.14912\n",
      "Iteration took: 4.75 secs, of which rollout took 3.26 secs and gradient updates took 1.49 secs\n",
      "Current adp learning rate: 0.0005772992059805465\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average adp Loss: 106.05868\n",
      "Iteration took: 4.78 secs, of which rollout took 3.29 secs and gradient updates took 1.49 secs\n",
      "Current adp learning rate: 0.0005688509249174165\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #11 --------------------\n",
      "Average adp Loss: 105.43469\n",
      "Iteration took: 4.69 secs, of which rollout took 3.21 secs and gradient updates took 1.48 secs\n",
      "Current adp learning rate: 0.000559601316382174\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #12 --------------------\n",
      "Average adp Loss: 103.98393\n",
      "Iteration took: 4.77 secs, of which rollout took 3.27 secs and gradient updates took 1.49 secs\n",
      "Current adp learning rate: 0.00054959218714607\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #13 --------------------\n",
      "Average adp Loss: 101.08892\n",
      "Iteration took: 4.84 secs, of which rollout took 3.35 secs and gradient updates took 1.48 secs\n",
      "Current adp learning rate: 0.000538868437152976\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #14 --------------------\n",
      "Average adp Loss: 98.19475\n",
      "Iteration took: 4.68 secs, of which rollout took 3.23 secs and gradient updates took 1.45 secs\n",
      "Current adp learning rate: 0.000527477722221287\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #15 --------------------\n",
      "Average adp Loss: 97.90506\n",
      "Iteration took: 4.75 secs, of which rollout took 3.28 secs and gradient updates took 1.47 secs\n",
      "Current adp learning rate: 0.0005154700992764123\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #16 --------------------\n",
      "Average adp Loss: 96.70378\n",
      "Iteration took: 4.61 secs, of which rollout took 3.1 secs and gradient updates took 1.51 secs\n",
      "Current adp learning rate: 0.0005028976578306461\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #17 --------------------\n",
      "Average adp Loss: 94.93117\n",
      "Iteration took: 4.7 secs, of which rollout took 3.21 secs and gradient updates took 1.48 secs\n",
      "Current adp learning rate: 0.0004898141415293609\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #18 --------------------\n",
      "Average adp Loss: 94.43348\n",
      "Iteration took: 4.68 secs, of which rollout took 3.2 secs and gradient updates took 1.48 secs\n",
      "Current adp learning rate: 0.00047627456363342737\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #19 --------------------\n",
      "Average adp Loss: 92.73173\n",
      "Iteration took: 4.6 secs, of which rollout took 3.15 secs and gradient updates took 1.45 secs\n",
      "Current adp learning rate: 0.00046233482030757096\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #20 --------------------\n",
      "Average adp Loss: 92.84915\n",
      "Iteration took: 4.77 secs, of which rollout took 3.26 secs and gradient updates took 1.51 secs\n",
      "Current adp learning rate: 0.0004480513055338411\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #21 --------------------\n",
      "Average adp Loss: 91.2739\n",
      "Iteration took: 4.84 secs, of which rollout took 3.37 secs and gradient updates took 1.46 secs\n",
      "Current adp learning rate: 0.00043348053137013895\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #22 --------------------\n",
      "Average adp Loss: 91.05352\n",
      "Iteration took: 4.8 secs, of which rollout took 3.33 secs and gradient updates took 1.47 secs\n",
      "Current adp learning rate: 0.00041867875712823176\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #23 --------------------\n",
      "Average adp Loss: 90.19816\n",
      "Iteration took: 4.72 secs, of which rollout took 3.23 secs and gradient updates took 1.48 secs\n",
      "Current adp learning rate: 0.00040370163085697795\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #24 --------------------\n",
      "Average adp Loss: 88.96019\n",
      "Iteration took: 4.76 secs, of which rollout took 3.25 secs and gradient updates took 1.51 secs\n",
      "Current adp learning rate: 0.000388603846288343\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #25 --------------------\n",
      "Average adp Loss: 88.36157\n",
      "Iteration took: 4.72 secs, of which rollout took 3.16 secs and gradient updates took 1.55 secs\n",
      "Current adp learning rate: 0.00037343881814050524\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_learning_rate</td><td>███▇▇▇▇▆▆▆▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>adp_learning_rate</td><td>██████▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▂▂▁▁</td></tr><tr><td>adp_maximum_base_validation_reward</td><td>▁</td></tr><tr><td>average_actor_loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>average_adapt_loss</td><td>█▆▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>average_critic_loss</td><td>█▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>average_episode_rewards</td><td>▁▁▂▂▂▄▅▆▇▇▆▇▇▇▇▇▇███████████████████████</td></tr><tr><td>critic_learning_rate</td><td>█████▇▇▇▆▆▅▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iteration_compute</td><td>▂▃▃▃▅▆▆▇▇▆▇▇▆▇▆▇▆▆█▆▇▆▆▆▆▆▆▆▅▆▆▆▆▆▆█▁▁▁▁</td></tr><tr><td>maximum_base_validation_reward</td><td>▁</td></tr><tr><td>mean_validation_reward</td><td>▁</td></tr><tr><td>simulated_iterations</td><td>▁▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇███▁▁▁▁▂▂▂▂▂▃</td></tr><tr><td>val_durs</td><td>▁████▇█▅██</td></tr><tr><td>val_rews</td><td>▁▅▆▇█▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_learning_rate</td><td>0.0</td></tr><tr><td>adp_learning_rate</td><td>0.00037</td></tr><tr><td>adp_maximum_base_validation_reward</td><td>208.41161</td></tr><tr><td>average_actor_loss</td><td>-0.21462</td></tr><tr><td>average_adapt_loss</td><td>88.36157</td></tr><tr><td>average_critic_loss</td><td>47.23237</td></tr><tr><td>average_episode_rewards</td><td>178.06905</td></tr><tr><td>critic_learning_rate</td><td>0.0</td></tr><tr><td>iteration_compute</td><td>4.71582</td></tr><tr><td>maximum_base_validation_reward</td><td>210.87888</td></tr><tr><td>mean_validation_reward</td><td>178.63396</td></tr><tr><td>simulated_iterations</td><td>25</td></tr><tr><td>val_durs</td><td>289</td></tr><tr><td>val_rews</td><td>177.80372</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-sweep-2</strong> at: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8fim0vr6' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/8fim0vr6</a><br/> View project at: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250116_140332-8fim0vr6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wluc7sls with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactor_lr: 0.0020207050083377357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadp_lr: 0.0005221344588285969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadp_num_steps: 523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_discount: 260\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_lr: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.23441601810215737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_sgd_batches: 754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_updates_per_iteration: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_steps: 1300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mohamedrostom62/ADLR/tum-adlr-ws25-16/wandb/run-20250116_142040-wluc7sls</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/wluc7sls' target=\"_blank\">colorful-sweep-3</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/xoxts7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/wluc7sls' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/wluc7sls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -450.87\n",
      "Average Actor Loss: -0.18977\n",
      "Average Critic Loss: 1173.4915296094262\n",
      "Average KL Divergence: 0.0165544277466302\n",
      "Iteration took: 37.12 secs, of which rollout took 14.81 secs and gradient updates took 22.3 secs\n",
      "Current actor learning rate: 0.0020207050083377357\n",
      "Current critic learning rate: 0.0020207050083377357\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -438.23\n",
      "Average Actor Loss: -0.20375\n",
      "Average Critic Loss: 998.6993634643721\n",
      "Average KL Divergence: 0.011855490853965533\n",
      "Iteration took: 37.57 secs, of which rollout took 15.0 secs and gradient updates took 22.57 secs\n",
      "Current actor learning rate: 0.002015091938870131\n",
      "Current critic learning rate: 0.002015091938870131\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -365.37\n",
      "Average Actor Loss: -0.2108\n",
      "Average Critic Loss: 895.8596845815615\n",
      "Average KL Divergence: 0.011133781445190954\n",
      "Iteration took: 38.14 secs, of which rollout took 15.9 secs and gradient updates took 22.23 secs\n",
      "Current actor learning rate: 0.002003896983654186\n",
      "Current critic learning rate: 0.002003896983654186\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -246.28\n",
      "Average Actor Loss: -0.21505\n",
      "Average Critic Loss: 815.5240699978773\n",
      "Average KL Divergence: 0.010990508414655955\n",
      "Iteration took: 40.64 secs, of which rollout took 18.17 secs and gradient updates took 22.46 secs\n",
      "Current actor learning rate: 0.0019871978421237345\n",
      "Current critic learning rate: 0.0019871978421237345\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -163.74\n",
      "Average Actor Loss: -0.21718\n",
      "Average Critic Loss: 724.8079641556556\n",
      "Average KL Divergence: 0.010702845150967682\n",
      "Iteration took: 39.31 secs, of which rollout took 16.74 secs and gradient updates took 22.56 secs\n",
      "Current actor learning rate: 0.0019651178661001375\n",
      "Current critic learning rate: 0.0019651178661001375\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -135.2\n",
      "Average Actor Loss: -0.21821\n",
      "Average Critic Loss: 638.0852242569471\n",
      "Average KL Divergence: 0.010484257796914198\n",
      "Iteration took: 41.47 secs, of which rollout took 18.79 secs and gradient updates took 22.68 secs\n",
      "Current actor learning rate: 0.0019378245624043025\n",
      "Current critic learning rate: 0.0019378245624043025\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -96.07\n",
      "Average Actor Loss: -0.21905\n",
      "Average Critic Loss: 564.9274701807889\n",
      "Average KL Divergence: 0.010465679398147784\n",
      "Iteration took: 40.9 secs, of which rollout took 18.16 secs and gradient updates took 22.73 secs\n",
      "Current actor learning rate: 0.0019055274863642307\n",
      "Current critic learning rate: 0.0019055274863642307\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -59.27\n",
      "Average Actor Loss: -0.2196\n",
      "Average Critic Loss: 508.6031174434143\n",
      "Average KL Divergence: 0.010397724786210552\n",
      "Iteration took: 47.42 secs, of which rollout took 24.86 secs and gradient updates took 22.54 secs\n",
      "Current actor learning rate: 0.0018684755630182595\n",
      "Current critic learning rate: 0.0018684755630182595\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -55.54\n",
      "Average Actor Loss: -0.21962\n",
      "Average Critic Loss: 467.02178329136194\n",
      "Average KL Divergence: 0.01031292498052632\n",
      "Iteration took: 68.56 secs, of which rollout took 45.94 secs and gradient updates took 22.62 secs\n",
      "Current actor learning rate: 0.0018269538838400759\n",
      "Current critic learning rate: 0.0018269538838400759\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #10 --------------------\n",
      "Average Episodic Return: -78.36\n",
      "Average Actor Loss: -0.2194\n",
      "Average Critic Loss: 434.80595371585383\n",
      "Average KL Divergence: 0.010123447678825422\n",
      "Iteration took: 130.1 secs, of which rollout took 91.59 secs and gradient updates took 23.35 secs\n",
      "Current actor learning rate: 0.001781280036744074\n",
      "Current critic learning rate: 0.001781280036744074\n",
      "Average Validation Return: -40.4\n",
      "Average Validation Duration: 417.5 secs\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"lunar\")\n",
    "wandb.agent(sweep_id, function=train_model, count=MAX_RUN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'adp_lr': 0.09632103099393384, 'batches': 570, 'actor_lr': 0.09512195454699128, 'num_envs': 38, 'anneal_lr': True, 'num_steps': 3, 'adp_num_steps': 7, 'max_grad_norm': 0.2936539784109834, 'anneal_discount': 850, 'n_updates_per_iteration': 12}\n",
      "Best Metrics: {'_wandb': {'runtime': 0}}\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "# Fetch the sweep object\n",
    "sweep = api.sweep(f\"pmsaraiva2712-tum/lunar/{sweep_id}\")\n",
    "\n",
    "# Fetch all runs from the sweep\n",
    "runs = sweep.runs\n",
    "\n",
    "# Sort runs by the metric you are optimizing for, e.g., 'val_loss'\n",
    "best_run = sorted(runs, key=lambda run: run.summary.get('val_rewards', float('-inf')), reverse=True)[0]\n",
    "\n",
    "# Extract best hyperparameters and metrics\n",
    "best_params = best_run.config\n",
    "best_metrics = best_run.summary\n",
    "\n",
    "# Print the best hyperparameters and metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Metrics:\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:612\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/backend_bases.py:2175\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2175\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/figure.py:3162\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3162\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3165\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:3101\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3098\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m spine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspines\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   3099\u001b[0m         artists\u001b[38;5;241m.\u001b[39mremove(spine)\n\u001b[0;32m-> 3101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_title_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxison:\n\u001b[1;32m   3104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axis_map\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:3045\u001b[0m, in \u001b[0;36m_AxesBase._update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3043\u001b[0m top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(top, bb\u001b[38;5;241m.\u001b[39mymax)\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title\u001b[38;5;241m.\u001b[39mget_text():\n\u001b[0;32m-> 3045\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update offsetText\u001b[39;00m\n\u001b[1;32m   3046\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39moffsetText\u001b[38;5;241m.\u001b[39mget_text():\n\u001b[1;32m   3047\u001b[0m         bb \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39moffsetText\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axis.py:1370\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1369\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1370\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_label_position(renderer)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axis.py:1302\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1300\u001b[0m major_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_majorticklocs()\n\u001b[1;32m   1301\u001b[0m major_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_ticks(major_locs)\n\u001b[0;32m-> 1302\u001b[0m major_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_major_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmajor_locs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(major_ticks, major_locs, major_labels):\n\u001b[1;32m   1304\u001b[0m     tick\u001b[38;5;241m.\u001b[39mupdate_position(loc)\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axis.py:1670\u001b[0m, in \u001b[0;36mAxis.get_major_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     numticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_majorticklocs())\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks) \u001b[38;5;241m<\u001b[39m numticks:\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;66;03m# Update the new tick label properties from the old.\u001b[39;00m\n\u001b[0;32m-> 1670\u001b[0m     tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_tick_props(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks[\u001b[38;5;241m0\u001b[39m], tick)\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axis.py:1598\u001b[0m, in \u001b[0;36mAxis._get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Axis subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must define \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1596\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tick_class or reimplement _get_tick()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1597\u001b[0m tick_kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_minor_tick_kw\n\u001b[0;32m-> 1598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tick_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmajor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtick_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axis.py:471\u001b[0m, in \u001b[0;36mYTick.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m trans, va, ha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text1_transform()\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m    468\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    469\u001b[0m     verticalalignment\u001b[38;5;241m=\u001b[39mva, horizontalalignment\u001b[38;5;241m=\u001b[39mha, transform\u001b[38;5;241m=\u001b[39mtrans,\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 471\u001b[0m trans, va, ha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text2_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m    473\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    474\u001b[0m     verticalalignment\u001b[38;5;241m=\u001b[39mva, horizontalalignment\u001b[38;5;241m=\u001b[39mha, transform\u001b[38;5;241m=\u001b[39mtrans,\n\u001b[1;32m    475\u001b[0m )\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axis.py:481\u001b[0m, in \u001b[0;36mYTick._get_text2_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text2_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_yaxis_text2_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:1067\u001b[0m, in \u001b[0;36m_AxesBase.get_yaxis_text2_transform\u001b[0;34m(self, pad_points)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03mReturns\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m-------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03mmay need to place axis elements in different locations.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m labels_align \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mytick.alignment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1067\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_yaxis_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtick2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1068\u001b[0m         mtransforms\u001b[38;5;241m.\u001b[39mScaledTranslation(pad_points \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m72\u001b[39m, \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1069\u001b[0m                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi_scale_trans),\n\u001b[1;32m   1070\u001b[0m         labels_align, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:1016\u001b[0m, in \u001b[0;36m_AxesBase.get_yaxis_transform\u001b[0;34m(self, which)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspines\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39mget_spine_transform()\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtick2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# for cartesian projection, this is top spine\u001b[39;00m\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241m.\u001b[39mget_spine_transform()\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown value for which: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhich\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/ADLR/tum-adlr-ws25-16/.venv/lib/python3.11/site-packages/matplotlib/spines.py:558\u001b[0m, in \u001b[0;36mSpines.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example NumPy array\n",
    "data = np.random.randint(0, 100, size=100)  # Random integers between 0 and 100\n",
    "\n",
    "# Calculate the median\n",
    "median_value = np.median(data)\n",
    "\n",
    "# Create histogram\n",
    "num_bins = 10  # Number of buckets\n",
    "plt.hist(data, bins=num_bins, edgecolor='black', alpha=0.7, label='Data')\n",
    "\n",
    "# Add a vertical line for the median\n",
    "plt.axvline(median_value, color='red', linestyle='dashed', linewidth=2, label=f'Median = {median_value}')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Value Range')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram with Buckets and Median')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
