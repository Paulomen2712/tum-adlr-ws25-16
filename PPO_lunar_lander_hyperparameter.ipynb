{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from model.ppo3 import PPO\n",
    "from env.wrappers import LunarContinuous\n",
    "from utils.logger import WandbSummaryWritter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_hyperparameters = {\n",
    "    'save_freq': 0 ,  \n",
    "    'val_freq': 10,\n",
    "    'val_iter': 10,\n",
    "    'env': LunarContinuous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise wandb session in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOTAL_TIMESTEPS_TO_TRAIN = 500\n",
    "VAL_ITER = 20\n",
    "MAX_RUN_COUNT = 30\n",
    "sweep_config = {\n",
    "    'method': 'bayes', \n",
    "    'metric': {\n",
    "        'name': 'validation_rewards',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 0.1\n",
    "        },\n",
    "        'gamma': {\n",
    "            'min': 0.9,\n",
    "            'max': 1.\n",
    "        },\n",
    "        # 'lr_gamma': {\n",
    "        #     'min': 0.999,\n",
    "        #     'max': 1.\n",
    "        # },\n",
    "        'lam': {\n",
    "            'min': 0.9,\n",
    "            'max': 1.\n",
    "        },\n",
    "        'max_grad_norm': {\n",
    "            \"distribution\": \"q_log_uniform\",\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 10, \n",
    "        },\n",
    "        'n_updates_per_iteration': {\n",
    "            'values': list(range(1, 21))\n",
    "        },\n",
    "        'num_envs': {\n",
    "            'values': list(range(1, 100))\n",
    "        },\n",
    "        'anneal_lr': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'num_steps': {\n",
    "            'distribution': 'q_uniform',\n",
    "            'min': 300,\n",
    "            'max': 4000,\n",
    "            'q': 100\n",
    "        },'batches': {\n",
    "            'distribution': 'q_uniform',\n",
    "            \"min\": 1,     # 2^0\n",
    "            \"max\": 1024,  # 2^10\n",
    "            \"q\": 2 \n",
    "        }\n",
    "    },\n",
    "     \"constraints\": [\n",
    "        {\"params\": [\"num_envs\", \"num_steps\"], \"max_product\": MAX_TOTAL_TIMESTEPS_TO_TRAIN}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config = None):\n",
    "    logger = WandbSummaryWritter(project='lunar', config =config)\n",
    "    ppo = ppo = PPO(logger,**misc_hyperparameters) if config is None else PPO(summary_writter=logger, **config, **misc_hyperparameters)\n",
    "    ppo.train()\n",
    "\n",
    "    val_rews, val_dur = ppo.validate(VAL_ITER, False)\n",
    "\n",
    "    wandb.log({\n",
    "        \"validation_rewards\": val_rews,\n",
    "        \"validation_duration\": val_dur\n",
    "        # \"max_reward_video\": wandb.Video(f\"videos\\\\rl-video-episode-{np.argmax(val_rews)}.mp4\", fps=4, format=\"mp4\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model fo the desired timestamps. Alternatively can specify a checkpoint to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('constraints' was unexpected)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. max_grad_norm uses q_log_uniform, where min/max specify base-e exponents. Use q_log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: q4p6pbl8\n",
      "Sweep URL: https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/q4p6pbl8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1dgokfx0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tanneal_lr: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches: 808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9736512887162576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlam: 0.992544563139167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.03559224270177326\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 4847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_updates_per_iteration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_steps: 2200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\pmsar\\git\\tum-adlr-ws25-16\\wandb\\run-20241213_200325-1dgokfx0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/1dgokfx0' target=\"_blank\">polished-sweep-1</a></strong> to <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/q4p6pbl8' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/q4p6pbl8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/q4p6pbl8' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/sweeps/q4p6pbl8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmsaraiva2712-tum/lunar/runs/1dgokfx0' target=\"_blank\">https://wandb.ai/pmsaraiva2712-tum/lunar/runs/1dgokfx0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Return: -348.75\n",
      "Average Loss: -0.00027\n",
      "Average KL Divergence: 0.007431268561720991\n",
      "Iteration took: 12.78 secs, of which rollout took 10.99 secs and gradient updates took 1.79 secs\n",
      "Current learning rate: 0.005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #2 --------------------\n",
      "Average Episodic Return: -128.63\n",
      "Average Loss: -0.00165\n",
      "Average KL Divergence: 0.006857339117428077\n",
      "Iteration took: 15.43 secs, of which rollout took 13.28 secs and gradient updates took 2.13 secs\n",
      "Current learning rate: 0.0049005\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #3 --------------------\n",
      "Average Episodic Return: -113.75\n",
      "Average Loss: -0.00192\n",
      "Average KL Divergence: 0.007583392630168908\n",
      "Iteration took: 13.28 secs, of which rollout took 11.16 secs and gradient updates took 2.1 secs\n",
      "Current learning rate: 0.0047064402\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #4 --------------------\n",
      "Average Episodic Return: -69.12\n",
      "Average Loss: -0.0016\n",
      "Average KL Divergence: 0.007906660994633687\n",
      "Iteration took: 13.74 secs, of which rollout took 11.48 secs and gradient updates took 2.24 secs\n",
      "Current learning rate: 0.00442828958418\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #5 --------------------\n",
      "Average Episodic Return: -69.77\n",
      "Average Loss: -0.0031\n",
      "Average KL Divergence: 0.008516279386219069\n",
      "Iteration took: 13.99 secs, of which rollout took 11.77 secs and gradient updates took 2.19 secs\n",
      "Current learning rate: 0.004081111680780287\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #6 --------------------\n",
      "Average Episodic Return: -13.65\n",
      "Average Loss: -0.00391\n",
      "Average KL Divergence: 0.009112422673123524\n",
      "Iteration took: 13.87 secs, of which rollout took 12.06 secs and gradient updates took 1.79 secs\n",
      "Current learning rate: 0.003683203291904209\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #7 --------------------\n",
      "Average Episodic Return: -8.71\n",
      "Average Loss: -0.00468\n",
      "Average KL Divergence: 0.009455404174155314\n",
      "Iteration took: 15.7 secs, of which rollout took 13.76 secs and gradient updates took 1.93 secs\n",
      "Current learning rate: 0.0032544784287265585\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #8 --------------------\n",
      "Average Episodic Return: -14.57\n",
      "Average Loss: -0.00452\n",
      "Average KL Divergence: 0.00942024302220786\n",
      "Iteration took: 16.62 secs, of which rollout took 14.69 secs and gradient updates took 1.91 secs\n",
      "Current learning rate: 0.0028147983930055997\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Iteration #9 --------------------\n",
      "Average Episodic Return: -9.87\n",
      "Average Loss: -0.00394\n",
      "Average KL Divergence: 0.009243317219717676\n",
      "Iteration took: 29.33 secs, of which rollout took 27.34 secs and gradient updates took 1.97 secs\n",
      "Current learning rate: 0.00238244535983994\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"lunar\")\n",
    "wandb.agent(sweep_id, function=train_model, count=MAX_RUN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'lr': 0.033773968633186116, 'lam': 0.965122224947915, 'gamma': 0.9391731546579618, 'lr_gamma': 0.99964198121568, 'max_grad_norm': 1.0549291822676827, 'n_sgd_batches': 8, 'timesteps_per_batch': 6600, 'n_updates_per_iteration': 17, 'max_timesteps_per_episode': 800}\n",
      "Best Metrics: {'val_rewards': -117.685825451997, '_runtime': 18.8714706, '_step': 2, '_timestamp': 1733497014.9633105, '_wandb': {'runtime': 18}, 'average_episode_lengths': 97.5, 'average_episode_rewards': -214.25900286086735, 'average_loss': 0.001098420703783631, 'learning_rate': 0.004990008995201681, 'max_reward_video': {'_type': 'video-file', 'path': 'media/videos/max_reward_video_2_de1368eb4a9cffe45bc9.mp4', 'sha256': 'de1368eb4a9cffe45bc98fb1781e53c0619e8cb010e012a453b94f289e4f54ad', 'size': 10645}, 'simulated_iterations': 2, 'simulated_timesteps': 9719, 'validation_duration': 76.4}\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "# Fetch the sweep object\n",
    "sweep = api.sweep(f\"pmsaraiva2712-tum/lunar/{sweep_id}\")\n",
    "\n",
    "# Fetch all runs from the sweep\n",
    "runs = sweep.runs\n",
    "\n",
    "# Sort runs by the metric you are optimizing for, e.g., 'val_loss'\n",
    "best_run = sorted(runs, key=lambda run: run.summary.get('val_rewards', float('-inf')), reverse=True)[0]\n",
    "\n",
    "# Extract best hyperparameters and metrics\n",
    "best_params = best_run.config\n",
    "best_metrics = best_run.summary\n",
    "\n",
    "# Print the best hyperparameters and metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Metrics:\", best_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
